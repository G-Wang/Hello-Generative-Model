{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cover](complements/cover.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from dataset import DatasetFromFolder\n",
    "from model import Generator, Discriminator\n",
    "\n",
    "import utils\n",
    "import argparse\n",
    "import os, itertools\n",
    "from logger import Logger\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argument Parsing & Directories Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=1, beta1=0.5, beta2=0.999, crop_size=256, dataset='horse2zebra', decay_epoch=100, fliplr=True, input_size=256, lambdaA=10, lambdaB=10, lrD=0.0002, lrG=0.0002, ndf=64, ngf=32, num_epochs=200, num_resnet=6, resize_scale=286)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "#Data Set Parameter\n",
    "parser.add_argument('--dataset', required=False, default='horse2zebra', help='input dataset')\n",
    "parser.add_argument('--batch_size', type=int, default=1, help='train batch size')\n",
    "parser.add_argument('--input_size', type=int, default=256, help='input size')\n",
    "parser.add_argument('--resize_scale', type=int, default=286, help='resize scale (0 is false)')\n",
    "parser.add_argument('--crop_size', type=int, default=256, help='crop size (0 is false)')\n",
    "parser.add_argument('--fliplr', type=bool, default=True, help='random fliplr True of False')\n",
    "\n",
    "#Model Parameters \n",
    "parser.add_argument('--ngf', type=int, default=32) # number of generator filters\n",
    "parser.add_argument('--ndf', type=int, default=64) # number of discriminator filters\n",
    "parser.add_argument('--num_resnet', type=int, default=6, help='number of resnet blocks in generator')\n",
    "\n",
    "#Learning Parameters\n",
    "parser.add_argument('--num_epochs', type=int, default=200, help='number of train epochs')\n",
    "parser.add_argument('--decay_epoch', type=int, default=100, help='start decaying learning rate after this number')\n",
    "parser.add_argument('--lrG', type=float, default=0.0002, help='learning rate for generator, default=0.0002')\n",
    "parser.add_argument('--lrD', type=float, default=0.0002, help='learning rate for discriminator, default=0.0002')\n",
    "parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for Adam optimizer')\n",
    "parser.add_argument('--beta2', type=float, default=0.999, help='beta2 for Adam optimizer')\n",
    "parser.add_argument('--lambdaA', type=float, default=10, help='lambdaA for cycle loss')\n",
    "parser.add_argument('--lambdaB', type=float, default=10, help='lambdaB for cycle loss')\n",
    "params = parser.parse_args([])\n",
    "print(params)\n",
    "\n",
    "# Directories for loading data and saving results\n",
    "data_dir = 'data/' + params.dataset + '/'\n",
    "save_dir = params.dataset + '_results/'\n",
    "model_dir = params.dataset + '_model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(data_dir) : \n",
    "    os.makedirs(data_dir)\n",
    "    file = 'ZIP_FILE.zip'\n",
    "    url = 'https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/' + params.dataset + '.zip'\n",
    "    print(url)\n",
    "    !wget -N $url -O $file\n",
    "    !unzip $file -d 'data/'\n",
    "    !rm $file\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "# Set the logger\n",
    "D_A_log_dir = save_dir + 'D_A_logs'\n",
    "D_B_log_dir = save_dir + 'D_B_logs'\n",
    "if not os.path.exists(D_A_log_dir):\n",
    "    os.mkdir(D_A_log_dir)\n",
    "D_A_logger = Logger(D_A_log_dir)\n",
    "if not os.path.exists(D_B_log_dir):\n",
    "    os.mkdir(D_B_log_dir)\n",
    "D_B_logger = Logger(D_B_log_dir)\n",
    "\n",
    "G_A_log_dir = save_dir + 'G_A_logs'\n",
    "G_B_log_dir = save_dir + 'G_B_logs'\n",
    "if not os.path.exists(G_A_log_dir):\n",
    "    os.mkdir(G_A_log_dir)\n",
    "G_A_logger = Logger(G_A_log_dir)\n",
    "if not os.path.exists(G_B_log_dir):\n",
    "    os.mkdir(G_B_log_dir)\n",
    "G_B_logger = Logger(G_B_log_dir)\n",
    "\n",
    "cycle_A_log_dir = save_dir + 'cycle_A_logs'\n",
    "cycle_B_log_dir = save_dir + 'cycle_B_logs'\n",
    "if not os.path.exists(cycle_A_log_dir):\n",
    "    os.mkdir(cycle_A_log_dir)\n",
    "cycle_A_logger = Logger(cycle_A_log_dir)\n",
    "if not os.path.exists(cycle_B_log_dir):\n",
    "    os.mkdir(cycle_B_log_dir)\n",
    "cycle_B_logger = Logger(cycle_B_log_dir)\n",
    "\n",
    "img_log_dir = save_dir + 'img_logs'\n",
    "if not os.path.exists(img_log_dir):\n",
    "    os.mkdir(img_log_dir)\n",
    "img_logger = Logger(img_log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data pre-processing\n",
    "transform = transforms.Compose([transforms.Resize(params.input_size),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n",
    "\n",
    "# Train data\n",
    "train_data_A = DatasetFromFolder(data_dir, subfolder='trainA', transform=transform,\n",
    "                                 resize_scale=params.resize_scale, crop_size=params.crop_size, fliplr=params.fliplr)\n",
    "train_data_loader_A = torch.utils.data.DataLoader(dataset=train_data_A,\n",
    "                                                  batch_size=params.batch_size,\n",
    "                                                  shuffle=True)\n",
    "train_data_B = DatasetFromFolder(data_dir, subfolder='trainB', transform=transform,\n",
    "                                 resize_scale=params.resize_scale, crop_size=params.crop_size, fliplr=params.fliplr)\n",
    "train_data_loader_B = torch.utils.data.DataLoader(dataset=train_data_B,\n",
    "                                                  batch_size=params.batch_size,\n",
    "                                                  shuffle=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# Test data\n",
    "test_data_A = DatasetFromFolder(data_dir, subfolder='testA', transform=transform)\n",
    "test_data_loader_A = torch.utils.data.DataLoader(dataset=test_data_A,\n",
    "                                                 batch_size=params.batch_size,\n",
    "                                                 shuffle=False)\n",
    "test_data_B = DatasetFromFolder(data_dir, subfolder='testB', transform=transform)\n",
    "test_data_loader_B = torch.utils.data.DataLoader(dataset=test_data_B,\n",
    "                                                 batch_size=params.batch_size,\n",
    "                                                 shuffle=False)\n",
    "\n",
    "# Get specific test images\n",
    "test_real_A_data = train_data_A.__getitem__(11).unsqueeze(0)  # Convert to 4d tensor (BxNxHxW)\n",
    "test_real_B_data = train_data_B.__getitem__(91).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models & Optimizers & Criterions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Models\n",
    "G_A = Generator(3, params.ngf, 3, params.num_resnet) # arguments : input_dim, num_filter, output_dim, num_resnet\n",
    "G_B = Generator(3, params.ngf, 3, params.num_resnet)\n",
    "D_A = Discriminator(3, params.ndf, 1)               # arguments : input_dim, num_filter, output_dim\n",
    "D_B = Discriminator(3, params.ndf, 1) \n",
    "\n",
    "G_A.normal_weight_init(mean=0.0, std=0.02)\n",
    "G_B.normal_weight_init(mean=0.0, std=0.02)\n",
    "D_A.normal_weight_init(mean=0.0, std=0.02)\n",
    "D_B.normal_weight_init(mean=0.0, std=0.02)\n",
    "\n",
    "G_A.cuda()\n",
    "G_B.cuda()\n",
    "D_A.cuda()\n",
    "D_B.cuda()\n",
    "\n",
    "# optimizers\n",
    "G_optimizer = torch.optim.Adam(itertools.chain(G_A.parameters(), G_B.parameters()), lr=params.lrG, betas=(params.beta1, params.beta2))\n",
    "D_A_optimizer = torch.optim.Adam(D_A.parameters(), lr=params.lrD, betas=(params.beta1, params.beta2))\n",
    "D_B_optimizer = torch.optim.Adam(D_B.parameters(), lr=params.lrD, betas=(params.beta1, params.beta2))\n",
    "\n",
    "# Loss function\n",
    "MSE_loss = torch.nn.MSELoss().cuda()\n",
    "L1_loss = torch.nn.L1Loss().cuda()\n",
    "\n",
    "# # Training GAN\n",
    "D_A_avg_losses = []\n",
    "D_B_avg_losses = []\n",
    "G_A_avg_losses = []\n",
    "G_B_avg_losses = []\n",
    "cycle_A_avg_losses = []\n",
    "cycle_B_avg_losses = []\n",
    "\n",
    "# Generated image pool\n",
    "num_pool = 50\n",
    "fake_A_pool = utils.ImagePool(num_pool)\n",
    "fake_B_pool = utils.ImagePool(num_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "<br><br>\n",
    "## Concepts\n",
    "![concept](complements/concept.jpg)\n",
    "<br>\n",
    "## GAN Losses\n",
    "![gan_loss](complements/gan_loss.JPG)\n",
    "<br>\n",
    "## Cycle Consistency Losses\n",
    "![cycle_consistency_loss](complements/cycle_consistency.JPG)\n",
    "<br>\n",
    "## Full Objectives = Gan Losses + Cycle Consistency Losses \n",
    "![full_objectives](complements/full_objectives.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Step [1/1067], D_A_loss: 0.8223, D_B_loss: 0.6316, G_A_loss: 1.1238, G_B_loss: 1.4754\n",
      "Epoch [1/200], Step [11/1067], D_A_loss: 0.3386, D_B_loss: 0.3924, G_A_loss: 0.4125, G_B_loss: 0.3399\n",
      "Epoch [1/200], Step [21/1067], D_A_loss: 0.2907, D_B_loss: 0.3036, G_A_loss: 0.4047, G_B_loss: 0.3946\n",
      "Epoch [1/200], Step [31/1067], D_A_loss: 0.2476, D_B_loss: 0.2239, G_A_loss: 0.3103, G_B_loss: 0.3212\n",
      "Epoch [1/200], Step [41/1067], D_A_loss: 0.3263, D_B_loss: 0.2529, G_A_loss: 0.4628, G_B_loss: 0.4056\n",
      "Epoch [1/200], Step [51/1067], D_A_loss: 0.2255, D_B_loss: 0.1867, G_A_loss: 0.3775, G_B_loss: 0.2804\n",
      "Epoch [1/200], Step [61/1067], D_A_loss: 0.2878, D_B_loss: 0.2702, G_A_loss: 0.3631, G_B_loss: 0.4655\n",
      "Epoch [1/200], Step [71/1067], D_A_loss: 0.2095, D_B_loss: 0.2790, G_A_loss: 0.2322, G_B_loss: 0.2770\n",
      "Epoch [1/200], Step [81/1067], D_A_loss: 0.2977, D_B_loss: 0.2719, G_A_loss: 0.2823, G_B_loss: 0.3732\n",
      "Epoch [1/200], Step [91/1067], D_A_loss: 0.1551, D_B_loss: 0.2790, G_A_loss: 0.7240, G_B_loss: 0.3952\n",
      "Epoch [1/200], Step [101/1067], D_A_loss: 0.3086, D_B_loss: 0.3569, G_A_loss: 0.4512, G_B_loss: 0.4258\n",
      "Epoch [1/200], Step [111/1067], D_A_loss: 0.2968, D_B_loss: 0.2353, G_A_loss: 0.3607, G_B_loss: 0.1882\n",
      "Epoch [1/200], Step [121/1067], D_A_loss: 0.1501, D_B_loss: 0.2124, G_A_loss: 0.4323, G_B_loss: 0.4138\n",
      "Epoch [1/200], Step [131/1067], D_A_loss: 0.2228, D_B_loss: 0.1775, G_A_loss: 0.1992, G_B_loss: 0.4906\n",
      "Epoch [1/200], Step [141/1067], D_A_loss: 0.3241, D_B_loss: 0.1284, G_A_loss: 0.2052, G_B_loss: 0.3548\n",
      "Epoch [1/200], Step [151/1067], D_A_loss: 0.1671, D_B_loss: 0.1046, G_A_loss: 0.3673, G_B_loss: 0.3285\n",
      "Epoch [1/200], Step [161/1067], D_A_loss: 0.2340, D_B_loss: 0.1821, G_A_loss: 0.4553, G_B_loss: 0.3114\n",
      "Epoch [1/200], Step [171/1067], D_A_loss: 0.1531, D_B_loss: 0.1513, G_A_loss: 0.4050, G_B_loss: 0.3832\n",
      "Epoch [1/200], Step [181/1067], D_A_loss: 0.2064, D_B_loss: 0.2386, G_A_loss: 0.3870, G_B_loss: 0.3882\n",
      "Epoch [1/200], Step [191/1067], D_A_loss: 0.3066, D_B_loss: 0.1926, G_A_loss: 0.2685, G_B_loss: 0.2720\n",
      "Epoch [1/200], Step [201/1067], D_A_loss: 0.1110, D_B_loss: 0.2873, G_A_loss: 0.3945, G_B_loss: 0.4552\n",
      "Epoch [1/200], Step [211/1067], D_A_loss: 0.3156, D_B_loss: 0.2088, G_A_loss: 0.3080, G_B_loss: 0.3020\n",
      "Epoch [1/200], Step [221/1067], D_A_loss: 0.1192, D_B_loss: 0.1452, G_A_loss: 0.3193, G_B_loss: 0.2217\n",
      "Epoch [1/200], Step [231/1067], D_A_loss: 0.1930, D_B_loss: 0.1012, G_A_loss: 0.5296, G_B_loss: 0.3391\n",
      "Epoch [1/200], Step [241/1067], D_A_loss: 0.0718, D_B_loss: 0.2005, G_A_loss: 0.6157, G_B_loss: 0.2027\n",
      "Epoch [1/200], Step [251/1067], D_A_loss: 0.2650, D_B_loss: 0.1066, G_A_loss: 0.2851, G_B_loss: 0.8183\n",
      "Epoch [1/200], Step [261/1067], D_A_loss: 0.1073, D_B_loss: 0.0750, G_A_loss: 0.4119, G_B_loss: 0.3327\n",
      "Epoch [1/200], Step [271/1067], D_A_loss: 0.0952, D_B_loss: 0.2013, G_A_loss: 0.5322, G_B_loss: 0.4106\n",
      "Epoch [1/200], Step [281/1067], D_A_loss: 0.1718, D_B_loss: 0.2160, G_A_loss: 0.2917, G_B_loss: 0.4589\n",
      "Epoch [1/200], Step [291/1067], D_A_loss: 0.2334, D_B_loss: 0.0714, G_A_loss: 0.1860, G_B_loss: 0.1907\n",
      "Epoch [1/200], Step [301/1067], D_A_loss: 0.0284, D_B_loss: 0.1101, G_A_loss: 0.4518, G_B_loss: 0.2936\n",
      "Epoch [1/200], Step [311/1067], D_A_loss: 0.2912, D_B_loss: 0.0703, G_A_loss: 0.7803, G_B_loss: 0.5011\n",
      "Epoch [1/200], Step [321/1067], D_A_loss: 0.3013, D_B_loss: 0.1468, G_A_loss: 0.3179, G_B_loss: 1.0289\n",
      "Epoch [1/200], Step [331/1067], D_A_loss: 0.3598, D_B_loss: 0.0440, G_A_loss: 0.1991, G_B_loss: 0.1198\n",
      "Epoch [1/200], Step [341/1067], D_A_loss: 0.1190, D_B_loss: 0.1306, G_A_loss: 0.7363, G_B_loss: 0.4515\n",
      "Epoch [1/200], Step [351/1067], D_A_loss: 0.2359, D_B_loss: 0.2078, G_A_loss: 0.3885, G_B_loss: 0.1937\n",
      "Epoch [1/200], Step [361/1067], D_A_loss: 0.3100, D_B_loss: 0.0489, G_A_loss: 0.2881, G_B_loss: 0.7911\n",
      "Epoch [1/200], Step [371/1067], D_A_loss: 0.1020, D_B_loss: 0.0951, G_A_loss: 0.4152, G_B_loss: 0.3314\n",
      "Epoch [1/200], Step [381/1067], D_A_loss: 0.0688, D_B_loss: 0.1517, G_A_loss: 0.4152, G_B_loss: 0.2567\n",
      "Epoch [1/200], Step [391/1067], D_A_loss: 0.2412, D_B_loss: 0.0906, G_A_loss: 0.0302, G_B_loss: 0.6300\n",
      "Epoch [1/200], Step [401/1067], D_A_loss: 0.1398, D_B_loss: 0.1509, G_A_loss: 0.2656, G_B_loss: 0.7496\n",
      "Epoch [1/200], Step [411/1067], D_A_loss: 0.1577, D_B_loss: 0.1185, G_A_loss: 0.4493, G_B_loss: 0.3176\n",
      "Epoch [1/200], Step [421/1067], D_A_loss: 0.1879, D_B_loss: 0.1464, G_A_loss: 0.3243, G_B_loss: 0.9612\n",
      "Epoch [1/200], Step [431/1067], D_A_loss: 0.2079, D_B_loss: 0.4548, G_A_loss: 0.8714, G_B_loss: 0.4633\n",
      "Epoch [1/200], Step [441/1067], D_A_loss: 0.0718, D_B_loss: 0.1195, G_A_loss: 0.8459, G_B_loss: 1.0944\n",
      "Epoch [1/200], Step [451/1067], D_A_loss: 0.1459, D_B_loss: 0.1186, G_A_loss: 0.3486, G_B_loss: 0.0730\n",
      "Epoch [1/200], Step [461/1067], D_A_loss: 0.0836, D_B_loss: 0.2970, G_A_loss: 0.1265, G_B_loss: 0.0880\n",
      "Epoch [1/200], Step [471/1067], D_A_loss: 0.1590, D_B_loss: 0.1237, G_A_loss: 0.6621, G_B_loss: 0.1921\n",
      "Epoch [1/200], Step [481/1067], D_A_loss: 0.1902, D_B_loss: 0.0920, G_A_loss: 0.2510, G_B_loss: 0.5092\n",
      "Epoch [1/200], Step [491/1067], D_A_loss: 0.1329, D_B_loss: 0.1557, G_A_loss: 0.4799, G_B_loss: 0.9112\n",
      "Epoch [1/200], Step [501/1067], D_A_loss: 0.2053, D_B_loss: 0.1469, G_A_loss: 0.7398, G_B_loss: 0.2013\n",
      "Epoch [1/200], Step [511/1067], D_A_loss: 0.0772, D_B_loss: 0.1900, G_A_loss: 0.6372, G_B_loss: 0.2931\n",
      "Epoch [1/200], Step [521/1067], D_A_loss: 0.0950, D_B_loss: 0.1039, G_A_loss: 0.7091, G_B_loss: 0.1186\n",
      "Epoch [1/200], Step [531/1067], D_A_loss: 0.1262, D_B_loss: 0.1575, G_A_loss: 0.0382, G_B_loss: 0.5571\n",
      "Epoch [1/200], Step [541/1067], D_A_loss: 0.1588, D_B_loss: 0.3963, G_A_loss: 0.1818, G_B_loss: 0.8969\n",
      "Epoch [1/200], Step [551/1067], D_A_loss: 0.0392, D_B_loss: 0.0767, G_A_loss: 0.4285, G_B_loss: 0.2774\n",
      "Epoch [1/200], Step [561/1067], D_A_loss: 0.1502, D_B_loss: 0.2531, G_A_loss: 0.7033, G_B_loss: 0.2479\n",
      "Epoch [1/200], Step [571/1067], D_A_loss: 0.0728, D_B_loss: 0.0510, G_A_loss: 0.0642, G_B_loss: 0.5366\n",
      "Epoch [1/200], Step [581/1067], D_A_loss: 0.0969, D_B_loss: 0.0966, G_A_loss: 0.6294, G_B_loss: 0.1044\n",
      "Epoch [1/200], Step [591/1067], D_A_loss: 0.1492, D_B_loss: 0.0552, G_A_loss: 0.3211, G_B_loss: 0.7168\n",
      "Epoch [1/200], Step [601/1067], D_A_loss: 0.4349, D_B_loss: 0.2536, G_A_loss: 0.4817, G_B_loss: 0.0461\n",
      "Epoch [1/200], Step [611/1067], D_A_loss: 0.1482, D_B_loss: 0.1727, G_A_loss: 0.4660, G_B_loss: 0.7343\n",
      "Epoch [1/200], Step [621/1067], D_A_loss: 0.3353, D_B_loss: 0.0924, G_A_loss: 0.4342, G_B_loss: 0.0789\n",
      "Epoch [1/200], Step [631/1067], D_A_loss: 0.1438, D_B_loss: 0.1952, G_A_loss: 0.8194, G_B_loss: 0.4562\n",
      "Epoch [1/200], Step [641/1067], D_A_loss: 0.3484, D_B_loss: 0.2684, G_A_loss: 1.0152, G_B_loss: 0.0846\n",
      "Epoch [1/200], Step [651/1067], D_A_loss: 0.1854, D_B_loss: 0.2676, G_A_loss: 0.2601, G_B_loss: 0.9830\n",
      "Epoch [1/200], Step [661/1067], D_A_loss: 0.0754, D_B_loss: 0.2035, G_A_loss: 0.3714, G_B_loss: 0.5499\n",
      "Epoch [1/200], Step [671/1067], D_A_loss: 0.1907, D_B_loss: 0.1042, G_A_loss: 0.8450, G_B_loss: 0.2591\n",
      "Epoch [1/200], Step [681/1067], D_A_loss: 0.1635, D_B_loss: 0.0311, G_A_loss: 0.3044, G_B_loss: 0.2926\n",
      "Epoch [1/200], Step [691/1067], D_A_loss: 0.0364, D_B_loss: 0.3135, G_A_loss: 0.5506, G_B_loss: 0.2284\n",
      "Epoch [1/200], Step [701/1067], D_A_loss: 0.0938, D_B_loss: 0.2420, G_A_loss: 0.2951, G_B_loss: 0.1856\n",
      "Epoch [1/200], Step [711/1067], D_A_loss: 0.2366, D_B_loss: 0.1932, G_A_loss: 0.3692, G_B_loss: 0.1688\n",
      "Epoch [1/200], Step [721/1067], D_A_loss: 0.1872, D_B_loss: 0.4100, G_A_loss: 0.0831, G_B_loss: 0.2851\n",
      "Epoch [1/200], Step [731/1067], D_A_loss: 0.2565, D_B_loss: 0.0683, G_A_loss: 0.2775, G_B_loss: 0.9127\n",
      "Epoch [1/200], Step [741/1067], D_A_loss: 0.2258, D_B_loss: 0.1416, G_A_loss: 0.3204, G_B_loss: 0.4291\n",
      "Epoch [1/200], Step [751/1067], D_A_loss: 0.2736, D_B_loss: 0.1899, G_A_loss: 0.5115, G_B_loss: 0.1294\n",
      "Epoch [1/200], Step [761/1067], D_A_loss: 0.0217, D_B_loss: 0.2309, G_A_loss: 0.2341, G_B_loss: 0.3666\n",
      "Epoch [1/200], Step [771/1067], D_A_loss: 0.1837, D_B_loss: 0.1468, G_A_loss: 0.2205, G_B_loss: 0.2750\n",
      "Epoch [1/200], Step [781/1067], D_A_loss: 0.0948, D_B_loss: 0.1903, G_A_loss: 0.4404, G_B_loss: 0.2811\n",
      "Epoch [1/200], Step [791/1067], D_A_loss: 0.3110, D_B_loss: 0.3553, G_A_loss: 0.7755, G_B_loss: 0.1752\n",
      "Epoch [1/200], Step [801/1067], D_A_loss: 0.3653, D_B_loss: 0.1043, G_A_loss: 0.7746, G_B_loss: 0.0533\n",
      "Epoch [1/200], Step [811/1067], D_A_loss: 0.2781, D_B_loss: 0.1050, G_A_loss: 0.7493, G_B_loss: 0.1246\n",
      "Epoch [1/200], Step [821/1067], D_A_loss: 0.0764, D_B_loss: 0.1096, G_A_loss: 0.5636, G_B_loss: 0.1414\n",
      "Epoch [1/200], Step [831/1067], D_A_loss: 0.0587, D_B_loss: 0.2166, G_A_loss: 0.1791, G_B_loss: 0.3975\n",
      "Epoch [1/200], Step [841/1067], D_A_loss: 0.3772, D_B_loss: 0.0975, G_A_loss: 0.1638, G_B_loss: 0.0628\n",
      "Epoch [1/200], Step [851/1067], D_A_loss: 0.0895, D_B_loss: 0.9157, G_A_loss: 0.0561, G_B_loss: 0.0759\n",
      "Epoch [1/200], Step [861/1067], D_A_loss: 0.1047, D_B_loss: 0.1195, G_A_loss: 0.6771, G_B_loss: 0.8153\n",
      "Epoch [1/200], Step [871/1067], D_A_loss: 0.2049, D_B_loss: 0.4638, G_A_loss: 0.1748, G_B_loss: 0.1702\n",
      "Epoch [1/200], Step [881/1067], D_A_loss: 0.3040, D_B_loss: 0.2636, G_A_loss: 0.3369, G_B_loss: 0.3329\n",
      "Epoch [1/200], Step [891/1067], D_A_loss: 0.2502, D_B_loss: 0.0222, G_A_loss: 0.4453, G_B_loss: 0.1238\n",
      "Epoch [1/200], Step [901/1067], D_A_loss: 0.4036, D_B_loss: 0.1102, G_A_loss: 0.4998, G_B_loss: 0.0320\n",
      "Epoch [1/200], Step [911/1067], D_A_loss: 0.0846, D_B_loss: 0.1099, G_A_loss: 0.7639, G_B_loss: 0.0658\n",
      "Epoch [1/200], Step [921/1067], D_A_loss: 0.0263, D_B_loss: 0.2374, G_A_loss: 0.4270, G_B_loss: 0.2281\n",
      "Epoch [1/200], Step [931/1067], D_A_loss: 0.1840, D_B_loss: 0.0899, G_A_loss: 0.1658, G_B_loss: 0.3041\n",
      "Epoch [1/200], Step [941/1067], D_A_loss: 0.2070, D_B_loss: 0.2686, G_A_loss: 0.1095, G_B_loss: 0.6459\n",
      "Epoch [1/200], Step [951/1067], D_A_loss: 0.2049, D_B_loss: 0.0704, G_A_loss: 0.3288, G_B_loss: 0.5440\n",
      "Epoch [1/200], Step [961/1067], D_A_loss: 0.3251, D_B_loss: 0.1389, G_A_loss: 0.5056, G_B_loss: 0.6495\n",
      "Epoch [1/200], Step [971/1067], D_A_loss: 0.1168, D_B_loss: 0.1764, G_A_loss: 0.6340, G_B_loss: 0.4206\n",
      "Epoch [1/200], Step [981/1067], D_A_loss: 0.0681, D_B_loss: 0.0703, G_A_loss: 0.5783, G_B_loss: 0.3392\n",
      "Epoch [1/200], Step [991/1067], D_A_loss: 0.0938, D_B_loss: 0.1017, G_A_loss: 0.3890, G_B_loss: 0.3689\n",
      "Epoch [1/200], Step [1001/1067], D_A_loss: 0.1818, D_B_loss: 0.2211, G_A_loss: 0.6833, G_B_loss: 0.2164\n",
      "Epoch [1/200], Step [1011/1067], D_A_loss: 0.0750, D_B_loss: 0.1681, G_A_loss: 0.4162, G_B_loss: 0.4991\n",
      "Epoch [1/200], Step [1021/1067], D_A_loss: 0.0212, D_B_loss: 0.3118, G_A_loss: 0.3065, G_B_loss: 0.4408\n",
      "Epoch [1/200], Step [1031/1067], D_A_loss: 0.1192, D_B_loss: 0.1624, G_A_loss: 0.4235, G_B_loss: 0.5238\n",
      "Epoch [1/200], Step [1041/1067], D_A_loss: 0.2931, D_B_loss: 0.1792, G_A_loss: 0.3747, G_B_loss: 0.2550\n",
      "Epoch [1/200], Step [1051/1067], D_A_loss: 0.1686, D_B_loss: 0.2047, G_A_loss: 0.9578, G_B_loss: 0.2412\n",
      "Epoch [1/200], Step [1061/1067], D_A_loss: 0.1896, D_B_loss: 0.1762, G_A_loss: 0.2050, G_B_loss: 0.5441\n",
      "Epoch [2/200], Step [1/1067], D_A_loss: 0.1447, D_B_loss: 0.1130, G_A_loss: 0.6840, G_B_loss: 0.3304\n",
      "Epoch [2/200], Step [11/1067], D_A_loss: 0.3572, D_B_loss: 0.1086, G_A_loss: 0.6567, G_B_loss: 1.0477\n",
      "Epoch [2/200], Step [21/1067], D_A_loss: 0.1351, D_B_loss: 0.0789, G_A_loss: 0.5833, G_B_loss: 0.3409\n",
      "Epoch [2/200], Step [31/1067], D_A_loss: 0.1875, D_B_loss: 0.1270, G_A_loss: 0.4040, G_B_loss: 0.5288\n",
      "Epoch [2/200], Step [41/1067], D_A_loss: 0.2605, D_B_loss: 0.1463, G_A_loss: 0.1889, G_B_loss: 0.2377\n",
      "Epoch [2/200], Step [51/1067], D_A_loss: 0.1290, D_B_loss: 0.1939, G_A_loss: 0.3500, G_B_loss: 0.4750\n",
      "Epoch [2/200], Step [61/1067], D_A_loss: 0.0665, D_B_loss: 0.2395, G_A_loss: 0.7606, G_B_loss: 0.1421\n",
      "Epoch [2/200], Step [71/1067], D_A_loss: 0.3152, D_B_loss: 0.1431, G_A_loss: 0.0479, G_B_loss: 0.0775\n",
      "Epoch [2/200], Step [81/1067], D_A_loss: 0.1492, D_B_loss: 0.2421, G_A_loss: 0.2744, G_B_loss: 0.2471\n",
      "Epoch [2/200], Step [91/1067], D_A_loss: 0.1666, D_B_loss: 0.0271, G_A_loss: 0.6797, G_B_loss: 0.2525\n",
      "Epoch [2/200], Step [101/1067], D_A_loss: 0.0899, D_B_loss: 0.2478, G_A_loss: 0.2897, G_B_loss: 0.2044\n",
      "Epoch [2/200], Step [111/1067], D_A_loss: 0.0944, D_B_loss: 0.1072, G_A_loss: 0.5560, G_B_loss: 0.2072\n",
      "Epoch [2/200], Step [121/1067], D_A_loss: 0.0544, D_B_loss: 0.1096, G_A_loss: 0.3435, G_B_loss: 0.2842\n",
      "Epoch [2/200], Step [131/1067], D_A_loss: 0.2193, D_B_loss: 0.0312, G_A_loss: 0.0898, G_B_loss: 0.5009\n",
      "Epoch [2/200], Step [141/1067], D_A_loss: 0.0775, D_B_loss: 0.0637, G_A_loss: 0.5102, G_B_loss: 0.3940\n",
      "Epoch [2/200], Step [151/1067], D_A_loss: 0.1140, D_B_loss: 0.2514, G_A_loss: 0.1436, G_B_loss: 0.4407\n",
      "Epoch [2/200], Step [161/1067], D_A_loss: 0.0642, D_B_loss: 0.0407, G_A_loss: 0.4963, G_B_loss: 0.6137\n",
      "Epoch [2/200], Step [171/1067], D_A_loss: 0.0313, D_B_loss: 0.1386, G_A_loss: 0.2867, G_B_loss: 0.6303\n",
      "Epoch [2/200], Step [181/1067], D_A_loss: 0.0612, D_B_loss: 0.1047, G_A_loss: 0.7099, G_B_loss: 0.1150\n",
      "Epoch [2/200], Step [191/1067], D_A_loss: 0.1378, D_B_loss: 0.0759, G_A_loss: 0.6179, G_B_loss: 0.2892\n",
      "Epoch [2/200], Step [201/1067], D_A_loss: 0.0734, D_B_loss: 0.1783, G_A_loss: 0.6610, G_B_loss: 0.2077\n",
      "Epoch [2/200], Step [211/1067], D_A_loss: 0.0537, D_B_loss: 0.0593, G_A_loss: 0.1569, G_B_loss: 0.5669\n",
      "Epoch [2/200], Step [221/1067], D_A_loss: 0.1112, D_B_loss: 0.0598, G_A_loss: 0.7457, G_B_loss: 0.5396\n",
      "Epoch [2/200], Step [231/1067], D_A_loss: 0.1969, D_B_loss: 0.2541, G_A_loss: 0.1140, G_B_loss: 0.4098\n",
      "Epoch [2/200], Step [241/1067], D_A_loss: 0.1969, D_B_loss: 0.3014, G_A_loss: 1.2509, G_B_loss: 0.5690\n",
      "Epoch [2/200], Step [251/1067], D_A_loss: 0.0487, D_B_loss: 0.2477, G_A_loss: 0.1767, G_B_loss: 0.5414\n",
      "Epoch [2/200], Step [261/1067], D_A_loss: 0.2118, D_B_loss: 0.1999, G_A_loss: 0.5745, G_B_loss: 0.6658\n",
      "Epoch [2/200], Step [271/1067], D_A_loss: 0.2569, D_B_loss: 0.0603, G_A_loss: 0.1682, G_B_loss: 0.1705\n",
      "Epoch [2/200], Step [281/1067], D_A_loss: 0.1680, D_B_loss: 0.1371, G_A_loss: 0.3094, G_B_loss: 0.2196\n",
      "Epoch [2/200], Step [291/1067], D_A_loss: 0.5946, D_B_loss: 0.1680, G_A_loss: 0.7227, G_B_loss: 0.0337\n",
      "Epoch [2/200], Step [301/1067], D_A_loss: 0.0310, D_B_loss: 0.0654, G_A_loss: 0.3606, G_B_loss: 0.3853\n",
      "Epoch [2/200], Step [311/1067], D_A_loss: 0.3976, D_B_loss: 0.2557, G_A_loss: 0.3895, G_B_loss: 0.3589\n",
      "Epoch [2/200], Step [321/1067], D_A_loss: 0.1612, D_B_loss: 0.1482, G_A_loss: 0.3100, G_B_loss: 0.2567\n",
      "Epoch [2/200], Step [331/1067], D_A_loss: 0.1954, D_B_loss: 0.2815, G_A_loss: 0.8137, G_B_loss: 0.2501\n",
      "Epoch [2/200], Step [341/1067], D_A_loss: 0.1186, D_B_loss: 0.1155, G_A_loss: 0.6736, G_B_loss: 0.6178\n",
      "Epoch [2/200], Step [351/1067], D_A_loss: 0.1613, D_B_loss: 0.0277, G_A_loss: 0.5641, G_B_loss: 0.3740\n",
      "Epoch [2/200], Step [361/1067], D_A_loss: 0.2259, D_B_loss: 0.1326, G_A_loss: 0.2860, G_B_loss: 1.0765\n",
      "Epoch [2/200], Step [371/1067], D_A_loss: 0.1718, D_B_loss: 0.3517, G_A_loss: 0.1159, G_B_loss: 0.1964\n",
      "Epoch [2/200], Step [381/1067], D_A_loss: 0.1866, D_B_loss: 0.2616, G_A_loss: 0.2913, G_B_loss: 0.2532\n",
      "Epoch [2/200], Step [391/1067], D_A_loss: 0.2691, D_B_loss: 0.2017, G_A_loss: 0.5353, G_B_loss: 0.4876\n",
      "Epoch [2/200], Step [401/1067], D_A_loss: 0.2784, D_B_loss: 0.1993, G_A_loss: 0.1756, G_B_loss: 0.4496\n",
      "Epoch [2/200], Step [411/1067], D_A_loss: 0.0844, D_B_loss: 0.0306, G_A_loss: 0.3639, G_B_loss: 0.2162\n",
      "Epoch [2/200], Step [421/1067], D_A_loss: 0.1800, D_B_loss: 0.2840, G_A_loss: 0.1406, G_B_loss: 0.1953\n",
      "Epoch [2/200], Step [431/1067], D_A_loss: 0.2379, D_B_loss: 0.2356, G_A_loss: 0.1973, G_B_loss: 0.6193\n",
      "Epoch [2/200], Step [441/1067], D_A_loss: 0.1548, D_B_loss: 0.1203, G_A_loss: 0.5025, G_B_loss: 0.2692\n",
      "Epoch [2/200], Step [451/1067], D_A_loss: 0.0193, D_B_loss: 0.1677, G_A_loss: 1.2617, G_B_loss: 0.3464\n",
      "Epoch [2/200], Step [461/1067], D_A_loss: 0.2569, D_B_loss: 0.0935, G_A_loss: 0.1327, G_B_loss: 0.1553\n",
      "Epoch [2/200], Step [471/1067], D_A_loss: 0.1885, D_B_loss: 0.2328, G_A_loss: 0.4904, G_B_loss: 0.3645\n",
      "Epoch [2/200], Step [481/1067], D_A_loss: 0.0612, D_B_loss: 0.0860, G_A_loss: 0.5265, G_B_loss: 0.2210\n",
      "Epoch [2/200], Step [491/1067], D_A_loss: 0.1723, D_B_loss: 0.0724, G_A_loss: 0.2676, G_B_loss: 0.6081\n",
      "Epoch [2/200], Step [501/1067], D_A_loss: 0.1434, D_B_loss: 0.1183, G_A_loss: 0.9306, G_B_loss: 0.5026\n",
      "Epoch [2/200], Step [511/1067], D_A_loss: 0.5022, D_B_loss: 0.0344, G_A_loss: 0.3141, G_B_loss: 1.1137\n",
      "Epoch [2/200], Step [521/1067], D_A_loss: 0.3920, D_B_loss: 0.1343, G_A_loss: 0.5514, G_B_loss: 0.6230\n",
      "Epoch [2/200], Step [531/1067], D_A_loss: 0.3498, D_B_loss: 0.2071, G_A_loss: 0.8259, G_B_loss: 0.0760\n",
      "Epoch [2/200], Step [541/1067], D_A_loss: 0.2862, D_B_loss: 0.0486, G_A_loss: 0.2736, G_B_loss: 0.1024\n",
      "Epoch [2/200], Step [551/1067], D_A_loss: 0.1736, D_B_loss: 0.1782, G_A_loss: 0.3090, G_B_loss: 0.2813\n",
      "Epoch [2/200], Step [561/1067], D_A_loss: 0.0957, D_B_loss: 0.0839, G_A_loss: 0.3555, G_B_loss: 0.2355\n",
      "Epoch [2/200], Step [571/1067], D_A_loss: 0.1808, D_B_loss: 0.1139, G_A_loss: 0.4591, G_B_loss: 0.1877\n",
      "Epoch [2/200], Step [581/1067], D_A_loss: 0.1457, D_B_loss: 0.0656, G_A_loss: 0.1958, G_B_loss: 0.6300\n",
      "Epoch [2/200], Step [591/1067], D_A_loss: 0.1524, D_B_loss: 0.0902, G_A_loss: 0.0833, G_B_loss: 0.4412\n",
      "Epoch [2/200], Step [601/1067], D_A_loss: 0.0448, D_B_loss: 0.1403, G_A_loss: 0.3409, G_B_loss: 0.2639\n",
      "Epoch [2/200], Step [611/1067], D_A_loss: 0.2158, D_B_loss: 0.1204, G_A_loss: 0.1547, G_B_loss: 0.2137\n",
      "Epoch [2/200], Step [621/1067], D_A_loss: 0.1531, D_B_loss: 0.0997, G_A_loss: 0.5582, G_B_loss: 0.6650\n",
      "Epoch [2/200], Step [631/1067], D_A_loss: 0.2501, D_B_loss: 0.0725, G_A_loss: 0.3085, G_B_loss: 0.3996\n",
      "Epoch [2/200], Step [641/1067], D_A_loss: 0.2911, D_B_loss: 0.1084, G_A_loss: 0.4547, G_B_loss: 0.0947\n",
      "Epoch [2/200], Step [651/1067], D_A_loss: 0.1138, D_B_loss: 0.1079, G_A_loss: 0.5927, G_B_loss: 0.1633\n",
      "Epoch [2/200], Step [661/1067], D_A_loss: 0.2729, D_B_loss: 0.2656, G_A_loss: 0.6023, G_B_loss: 0.0992\n",
      "Epoch [2/200], Step [671/1067], D_A_loss: 0.3436, D_B_loss: 0.3488, G_A_loss: 0.1147, G_B_loss: 0.4859\n",
      "Epoch [2/200], Step [681/1067], D_A_loss: 0.1197, D_B_loss: 0.1528, G_A_loss: 0.4298, G_B_loss: 0.3577\n",
      "Epoch [2/200], Step [691/1067], D_A_loss: 0.0176, D_B_loss: 0.4012, G_A_loss: 0.9912, G_B_loss: 0.5889\n",
      "Epoch [2/200], Step [701/1067], D_A_loss: 0.3029, D_B_loss: 0.2455, G_A_loss: 0.2966, G_B_loss: 0.4623\n",
      "Epoch [2/200], Step [711/1067], D_A_loss: 0.0879, D_B_loss: 0.0882, G_A_loss: 0.3365, G_B_loss: 0.3614\n",
      "Epoch [2/200], Step [721/1067], D_A_loss: 0.0488, D_B_loss: 0.0627, G_A_loss: 0.6515, G_B_loss: 0.9158\n",
      "Epoch [2/200], Step [731/1067], D_A_loss: 0.1971, D_B_loss: 0.1176, G_A_loss: 0.5143, G_B_loss: 0.3548\n",
      "Epoch [2/200], Step [741/1067], D_A_loss: 0.3153, D_B_loss: 0.1539, G_A_loss: 1.0945, G_B_loss: 0.1181\n",
      "Epoch [2/200], Step [751/1067], D_A_loss: 0.1050, D_B_loss: 0.1642, G_A_loss: 0.0862, G_B_loss: 0.1377\n",
      "Epoch [2/200], Step [761/1067], D_A_loss: 0.2919, D_B_loss: 0.2665, G_A_loss: 0.1326, G_B_loss: 0.0922\n",
      "Epoch [2/200], Step [771/1067], D_A_loss: 0.2760, D_B_loss: 0.1250, G_A_loss: 0.3454, G_B_loss: 0.3153\n",
      "Epoch [2/200], Step [781/1067], D_A_loss: 0.2972, D_B_loss: 0.1519, G_A_loss: 0.2982, G_B_loss: 0.5740\n",
      "Epoch [2/200], Step [791/1067], D_A_loss: 0.2349, D_B_loss: 0.1859, G_A_loss: 0.3187, G_B_loss: 0.1201\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-461dc53cc644>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Train generator G\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# A -> B\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mfake_B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG_A\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mD_B_fake_decision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD_B\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_B\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mG_A_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMSE_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_B_fake_decision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_B_fake_decision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/isjeon/pytorch/tut/fastcam/Hello-Generative-Model/Day06/CycleGAN/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m# Resnet blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;31m# Decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mdec1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mdec2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/isjeon/pytorch/tut/fastcam/Hello-Generative-Model/Day06/CycleGAN/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/instancenorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Apply instance norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0minput_reshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         out = F.batch_norm(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "for epoch in range(params.num_epochs):\n",
    "    D_A_losses = []\n",
    "    D_B_losses = []\n",
    "    G_A_losses = []\n",
    "    G_B_losses = []\n",
    "    cycle_A_losses = []\n",
    "    cycle_B_losses = []\n",
    "\n",
    "    # learning rate decay\n",
    "    if (epoch + 1) > params.decay_epoch:\n",
    "        D_A_optimizer.param_groups[0]['lr'] -= params.lrD / (params.num_epochs - params.decay_epoch)\n",
    "        D_B_optimizer.param_groups[0]['lr'] -= params.lrD / (params.num_epochs - params.decay_epoch)\n",
    "        G_optimizer.param_groups[0]['lr'] -= params.lrG / (params.num_epochs - params.decay_epoch)\n",
    "\n",
    "    # training\n",
    "    for i, (real_A, real_B) in enumerate(zip(train_data_loader_A, train_data_loader_B)):\n",
    "\n",
    "        # input image data\n",
    "        real_A = Variable(real_A.cuda())\n",
    "        real_B = Variable(real_B.cuda())\n",
    "\n",
    "        # ------------------------ Train generator G ----------------------------\n",
    "        # A -> B\n",
    "        fake_B = G_A(real_A)\n",
    "        D_B_fake_decision = D_B(fake_B)\n",
    "        G_A_loss = MSE_loss(D_B_fake_decision, Variable(torch.ones(D_B_fake_decision.size()).cuda()))\n",
    "\n",
    "        # forward cycle loss\n",
    "        recon_A = G_B(fake_B)\n",
    "        cycle_A_loss = L1_loss(recon_A, real_A) * params.lambdaA\n",
    "\n",
    "        # B -> A\n",
    "        fake_A = G_B(real_B)\n",
    "        D_A_fake_decision = D_A(fake_A)\n",
    "        G_B_loss = MSE_loss(D_A_fake_decision, Variable(torch.ones(D_A_fake_decision.size()).cuda()))\n",
    "\n",
    "        # backward cycle loss\n",
    "        recon_B = G_A(fake_A)\n",
    "        cycle_B_loss = L1_loss(recon_B, real_B) * params.lambdaB\n",
    "\n",
    "        # Back propagation\n",
    "        G_loss = G_A_loss + G_B_loss + cycle_A_loss + cycle_B_loss\n",
    "        G_optimizer.zero_grad()\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "        \n",
    "        # ------------------------ Train discriminator D_A ------------------------\n",
    "        D_A_real_decision = D_A(real_A)\n",
    "        D_A_real_loss = MSE_loss(D_A_real_decision, Variable(torch.ones(D_A_real_decision.size()).cuda()))\n",
    "        \n",
    "        fake_A = fake_A_pool.query(fake_A)\n",
    "        \n",
    "        D_A_fake_decision = D_A(fake_A)\n",
    "        D_A_fake_loss = MSE_loss(D_A_fake_decision, Variable(torch.zeros(D_A_fake_decision.size()).cuda()))\n",
    "\n",
    "        # Back propagation\n",
    "        D_A_loss = (D_A_real_loss + D_A_fake_loss) * 0.5\n",
    "        D_A_optimizer.zero_grad()\n",
    "        D_A_loss.backward()\n",
    "        D_A_optimizer.step()\n",
    "\n",
    "        # ------------------------ Train discriminator D_B ------------------------\n",
    "        D_B_real_decision = D_B(real_B)\n",
    "        D_B_real_loss = MSE_loss(D_B_real_decision, Variable(torch.ones(D_B_real_decision.size()).cuda()))\n",
    "        fake_B = fake_B_pool.query(fake_B)\n",
    "        D_B_fake_decision = D_B(fake_B)\n",
    "        D_B_fake_loss = MSE_loss(D_B_fake_decision, Variable(torch.zeros(D_B_fake_decision.size()).cuda()))\n",
    "\n",
    "        # Back propagation\n",
    "        D_B_loss = (D_B_real_loss + D_B_fake_loss) * 0.5\n",
    "        D_B_optimizer.zero_grad()\n",
    "        D_B_loss.backward()\n",
    "        D_B_optimizer.step()\n",
    "\n",
    "        # ------------------------ Print -----------------------------\n",
    "        # loss values\n",
    "        D_A_losses.append(D_A_loss.data[0])\n",
    "        D_B_losses.append(D_B_loss.data[0])\n",
    "        G_A_losses.append(G_A_loss.data[0])\n",
    "        G_B_losses.append(G_B_loss.data[0])\n",
    "        cycle_A_losses.append(cycle_A_loss.data[0])\n",
    "        cycle_B_losses.append(cycle_B_loss.data[0])\n",
    "\n",
    "        if i%10 == 0:\n",
    "            print('Epoch [%d/%d], Step [%d/%d], D_A_loss: %.4f, D_B_loss: %.4f, G_A_loss: %.4f, G_B_loss: %.4f'\n",
    "                  % (epoch+1, params.num_epochs, i+1, len(train_data_loader_A), D_A_loss.data[0], D_B_loss.data[0], G_A_loss.data[0], G_B_loss.data[0]))\n",
    "\n",
    "        # ============ TensorBoard logging ============#\n",
    "        D_A_logger.scalar_summary('losses', D_A_loss.data[0], step + 1)\n",
    "        D_B_logger.scalar_summary('losses', D_B_loss.data[0], step + 1)\n",
    "        G_A_logger.scalar_summary('losses', G_A_loss.data[0], step + 1)\n",
    "        G_B_logger.scalar_summary('losses', G_B_loss.data[0], step + 1)\n",
    "        cycle_A_logger.scalar_summary('losses', cycle_A_loss.data[0], step + 1)\n",
    "        cycle_B_logger.scalar_summary('losses', cycle_B_loss.data[0], step + 1)\n",
    "        step += 1\n",
    "\n",
    "    D_A_avg_loss = torch.mean(torch.FloatTensor(D_A_losses))\n",
    "    D_B_avg_loss = torch.mean(torch.FloatTensor(D_B_losses))\n",
    "    G_A_avg_loss = torch.mean(torch.FloatTensor(G_A_losses))\n",
    "    G_B_avg_loss = torch.mean(torch.FloatTensor(G_B_losses))\n",
    "    cycle_A_avg_loss = torch.mean(torch.FloatTensor(cycle_A_losses))\n",
    "    cycle_B_avg_loss = torch.mean(torch.FloatTensor(cycle_B_losses))\n",
    "\n",
    "    # avg loss values for plot\n",
    "    D_A_avg_losses.append(D_A_avg_loss)\n",
    "    D_B_avg_losses.append(D_B_avg_loss)\n",
    "    G_A_avg_losses.append(G_A_avg_loss)\n",
    "    G_B_avg_losses.append(G_B_avg_loss)\n",
    "    cycle_A_avg_losses.append(cycle_A_avg_loss)\n",
    "    cycle_B_avg_losses.append(cycle_B_avg_loss)\n",
    "\n",
    "    # Show result for test image\n",
    "    test_real_A = Variable(test_real_A_data.cuda())\n",
    "    test_fake_B = G_A(test_real_A)\n",
    "    test_recon_A = G_B(test_fake_B)\n",
    "\n",
    "    test_real_B = Variable(test_real_B_data.cuda())\n",
    "    test_fake_A = G_B(test_real_B)\n",
    "    test_recon_B = G_A(test_fake_A)\n",
    "\n",
    "    utils.plot_train_result([test_real_A, test_real_B], [test_fake_B, test_fake_A], [test_recon_A, test_recon_B],\n",
    "                            epoch, save=True, save_dir=save_dir)\n",
    "\n",
    "    # log the images\n",
    "    result_AtoB = np.concatenate((utils.to_np(test_real_A), utils.to_np(test_fake_B), utils.to_np(test_recon_A)), axis=3)\n",
    "    result_BtoA = np.concatenate((utils.to_np(test_real_B), utils.to_np(test_fake_A), utils.to_np(test_recon_B)), axis=3)\n",
    "\n",
    "    info = { 'result_AtoB': result_AtoB.transpose(0, 2, 3, 1),  # convert to BxHxWxC\n",
    "             'result_BtoA': result_BtoA.transpose(0, 2, 3, 1) }\n",
    "\n",
    "    for tag, images in info.items():\n",
    "        img_logger.image_summary(tag, images, epoch + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average losses\n",
    "avg_losses = []\n",
    "avg_losses.append(D_A_avg_losses)\n",
    "avg_losses.append(D_B_avg_losses)\n",
    "avg_losses.append(G_A_avg_losses)\n",
    "avg_losses.append(G_B_avg_losses)\n",
    "avg_losses.append(cycle_A_avg_losses)\n",
    "avg_losses.append(cycle_B_avg_losses)\n",
    "utils.plot_loss(avg_losses, params.num_epochs, save=True, save_dir=save_dir)\n",
    "\n",
    "# Make gif\n",
    "utils.make_gif(params.dataset, params.num_epochs, save_dir=save_dir)\n",
    "# Save trained parameters of model\n",
    "torch.save(G_A.state_dict(), model_dir + 'generator_A_param.pkl')\n",
    "torch.save(G_B.state_dict(), model_dir + 'generator_B_param.pkl')\n",
    "torch.save(D_A.state_dict(), model_dir + 'discriminator_A_param.pkl')\n",
    "torch.save(D_B.state_dict(), model_dir + 'discriminator_B_param.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
