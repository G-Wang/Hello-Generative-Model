{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cover](complements/cover.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from dataset import DatasetFromFolder\n",
    "from model import Generator, Discriminator\n",
    "import utils\n",
    "import argparse\n",
    "import os, itertools\n",
    "from logger import Logger\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argument Parsing & Directories Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', required=False, default='horse2zebra', help='input dataset')\n",
    "parser.add_argument('--batch_size', type=int, default=1, help='train batch size')\n",
    "parser.add_argument('--ngf', type=int, default=32) # number of generator filters\n",
    "parser.add_argument('--ndf', type=int, default=64) # number of discriminator filters\n",
    "parser.add_argument('--num_resnet', type=int, default=6, help='number of resnet blocks in generator')\n",
    "parser.add_argument('--input_size', type=int, default=256, help='input size')\n",
    "parser.add_argument('--resize_scale', type=int, default=286, help='resize scale (0 is false)')\n",
    "parser.add_argument('--crop_size', type=int, default=256, help='crop size (0 is false)')\n",
    "parser.add_argument('--fliplr', type=bool, default=True, help='random fliplr True of False')\n",
    "parser.add_argument('--num_epochs', type=int, default=200, help='number of train epochs')\n",
    "parser.add_argument('--decay_epoch', type=int, default=100, help='start decaying learning rate after this number')\n",
    "parser.add_argument('--lrG', type=float, default=0.0002, help='learning rate for generator, default=0.0002')\n",
    "parser.add_argument('--lrD', type=float, default=0.0002, help='learning rate for discriminator, default=0.0002')\n",
    "parser.add_argument('--lambdaA', type=float, default=10, help='lambdaA for cycle loss')\n",
    "parser.add_argument('--lambdaB', type=float, default=10, help='lambdaB for cycle loss')\n",
    "parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for Adam optimizer')\n",
    "parser.add_argument('--beta2', type=float, default=0.999, help='beta2 for Adam optimizer')\n",
    "params = parser.parse_args([])\n",
    "print(params)\n",
    "\n",
    "# Directories for loading data and saving results\n",
    "data_dir = 'data/' + params.dataset + '/'\n",
    "save_dir = params.dataset + '_results/'\n",
    "model_dir = params.dataset + '_model/'\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "\n",
    "# Set the logger\n",
    "D_A_log_dir = save_dir + 'D_A_logs'\n",
    "D_B_log_dir = save_dir + 'D_B_logs'\n",
    "if not os.path.exists(D_A_log_dir):\n",
    "    os.mkdir(D_A_log_dir)\n",
    "D_A_logger = Logger(D_A_log_dir)\n",
    "if not os.path.exists(D_B_log_dir):\n",
    "    os.mkdir(D_B_log_dir)\n",
    "D_B_logger = Logger(D_B_log_dir)\n",
    "\n",
    "G_A_log_dir = save_dir + 'G_A_logs'\n",
    "G_B_log_dir = save_dir + 'G_B_logs'\n",
    "if not os.path.exists(G_A_log_dir):\n",
    "    os.mkdir(G_A_log_dir)\n",
    "G_A_logger = Logger(G_A_log_dir)\n",
    "if not os.path.exists(G_B_log_dir):\n",
    "    os.mkdir(G_B_log_dir)\n",
    "G_B_logger = Logger(G_B_log_dir)\n",
    "\n",
    "cycle_A_log_dir = save_dir + 'cycle_A_logs'\n",
    "cycle_B_log_dir = save_dir + 'cycle_B_logs'\n",
    "if not os.path.exists(cycle_A_log_dir):\n",
    "    os.mkdir(cycle_A_log_dir)\n",
    "cycle_A_logger = Logger(cycle_A_log_dir)\n",
    "if not os.path.exists(cycle_B_log_dir):\n",
    "    os.mkdir(cycle_B_log_dir)\n",
    "cycle_B_logger = Logger(cycle_B_log_dir)\n",
    "\n",
    "img_log_dir = save_dir + 'img_logs'\n",
    "if not os.path.exists(img_log_dir):\n",
    "    os.mkdir(img_log_dir)\n",
    "img_logger = Logger(img_log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data pre-processing\n",
    "transform = transforms.Compose([transforms.Scale(params.input_size),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n",
    "\n",
    "# Train data\n",
    "train_data_A = DatasetFromFolder(data_dir, subfolder='trainA', transform=transform,\n",
    "                                 resize_scale=params.resize_scale, crop_size=params.crop_size, fliplr=params.fliplr)\n",
    "train_data_loader_A = torch.utils.data.DataLoader(dataset=train_data_A,\n",
    "                                                  batch_size=params.batch_size,\n",
    "                                                  shuffle=True)\n",
    "train_data_B = DatasetFromFolder(data_dir, subfolder='trainB', transform=transform,\n",
    "                                 resize_scale=params.resize_scale, crop_size=params.crop_size, fliplr=params.fliplr)\n",
    "train_data_loader_B = torch.utils.data.DataLoader(dataset=train_data_B,\n",
    "                                                  batch_size=params.batch_size,\n",
    "                                                  shuffle=True)\n",
    "\n",
    "# Test data\n",
    "test_data_A = DatasetFromFolder(data_dir, subfolder='testA', transform=transform)\n",
    "test_data_loader_A = torch.utils.data.DataLoader(dataset=test_data_A,\n",
    "                                                 batch_size=params.batch_size,\n",
    "                                                 shuffle=False)\n",
    "test_data_B = DatasetFromFolder(data_dir, subfolder='testB', transform=transform)\n",
    "test_data_loader_B = torch.utils.data.DataLoader(dataset=test_data_B,\n",
    "                                                 batch_size=params.batch_size,\n",
    "                                                 shuffle=False)\n",
    "\n",
    "# Get specific test images\n",
    "test_real_A_data = test_data_A.__getitem__(11).unsqueeze(0)  # Convert to 4d tensor (BxNxHxW)\n",
    "test_real_B_data = test_data_B.__getitem__(91).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models & Optimizers & Criterions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Models\n",
    "G_A = Generator(3, params.ngf, 3, params.num_resnet) # arguments : input_dim, num_filter, output_dim, num_resnet\n",
    "G_B = Generator(3, params.ngf, 3, params.num_resnet)\n",
    "D_A = Discriminator(3, params.ndf, 1) # arguments : input_dim, num_filter, output_dim\n",
    "D_B = Discriminator(3, params.ndf, 1) \n",
    "G_A.normal_weight_init(mean=0.0, std=0.02)\n",
    "G_B.normal_weight_init(mean=0.0, std=0.02)\n",
    "D_A.normal_weight_init(mean=0.0, std=0.02)\n",
    "D_B.normal_weight_init(mean=0.0, std=0.02)\n",
    "G_A.cuda()\n",
    "G_B.cuda()\n",
    "D_A.cuda()\n",
    "D_B.cuda()\n",
    "\n",
    "# optimizers\n",
    "G_optimizer = torch.optim.Adam(itertools.chain(G_A.parameters(), G_B.parameters()), lr=params.lrG, betas=(params.beta1, params.beta2))\n",
    "D_A_optimizer = torch.optim.Adam(D_A.parameters(), lr=params.lrD, betas=(params.beta1, params.beta2))\n",
    "D_B_optimizer = torch.optim.Adam(D_B.parameters(), lr=params.lrD, betas=(params.beta1, params.beta2))\n",
    "\n",
    "# Loss function\n",
    "MSE_loss = torch.nn.MSELoss().cuda()\n",
    "L1_loss = torch.nn.L1Loss().cuda()\n",
    "\n",
    "# Training GAN\n",
    "D_A_avg_losses = []\n",
    "D_B_avg_losses = []\n",
    "G_A_avg_losses = []\n",
    "G_B_avg_losses = []\n",
    "cycle_A_avg_losses = []\n",
    "cycle_B_avg_losses = []\n",
    "\n",
    "# Generated image pool\n",
    "num_pool = 50\n",
    "fake_A_pool = utils.ImagePool(num_pool)\n",
    "fake_B_pool = utils.ImagePool(num_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "<br><br>\n",
    "## Concepts\n",
    "![concept](complements/concept.jpg)\n",
    "<br>\n",
    "## GAN Losses\n",
    "![gan_loss](complements/gan_loss.JPG)\n",
    "<br>\n",
    "## Cycle Consistency Losses\n",
    "![cycle_consistency_loss](complements/cycle_consistency.JPG)\n",
    "<br>\n",
    "## Full Objectives = Gan Losses + Cycle Consistency Losses \n",
    "![full_objectives](complements/full_objectives.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "step = 0\n",
    "for epoch in range(params.num_epochs):\n",
    "    D_A_losses = []\n",
    "    D_B_losses = []\n",
    "    G_A_losses = []\n",
    "    G_B_losses = []\n",
    "    cycle_A_losses = []\n",
    "    cycle_B_losses = []\n",
    "\n",
    "    # learning rate decay\n",
    "    if (epoch + 1) > params.decay_epoch:\n",
    "        D_A_optimizer.param_groups[0]['lr'] -= params.lrD / (params.num_epochs - params.decay_epoch)\n",
    "        D_B_optimizer.param_groups[0]['lr'] -= params.lrD / (params.num_epochs - params.decay_epoch)\n",
    "        G_optimizer.param_groups[0]['lr'] -= params.lrG / (params.num_epochs - params.decay_epoch)\n",
    "\n",
    "    # training\n",
    "    for i, (real_A, real_B) in enumerate(zip(train_data_loader_A, train_data_loader_B)):\n",
    "\n",
    "        # input image data\n",
    "        real_A = Variable(real_A.cuda())\n",
    "        real_B = Variable(real_B.cuda())\n",
    "\n",
    "        # Train generator G\n",
    "        # A -> B\n",
    "        fake_B = G_A(real_A)\n",
    "        D_B_fake_decision = D_B(fake_B)\n",
    "        G_A_loss = MSE_loss(D_B_fake_decision, Variable(torch.ones(D_B_fake_decision.size()).cuda()))\n",
    "\n",
    "        # forward cycle loss\n",
    "        recon_A = G_B(fake_B)\n",
    "        cycle_A_loss = L1_loss(recon_A, real_A) * params.lambdaA\n",
    "\n",
    "        # B -> A\n",
    "        fake_A = G_B(real_B)\n",
    "        D_A_fake_decision = D_A(fake_A)\n",
    "        G_B_loss = MSE_loss(D_A_fake_decision, Variable(torch.ones(D_A_fake_decision.size()).cuda()))\n",
    "\n",
    "        # backward cycle loss\n",
    "        recon_B = G_A(fake_A)\n",
    "        cycle_B_loss = L1_loss(recon_B, real_B) * params.lambdaB\n",
    "\n",
    "        # Back propagation\n",
    "        G_loss = G_A_loss + G_B_loss + cycle_A_loss + cycle_B_loss\n",
    "        G_optimizer.zero_grad()\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "\n",
    "        # Train discriminator D_A\n",
    "        D_A_real_decision = D_A(real_A)\n",
    "        D_A_real_loss = MSE_loss(D_A_real_decision, Variable(torch.ones(D_A_real_decision.size()).cuda()))\n",
    "        fake_A = fake_A_pool.query(fake_A)\n",
    "        D_A_fake_decision = D_A(fake_A)\n",
    "        D_A_fake_loss = MSE_loss(D_A_fake_decision, Variable(torch.zeros(D_A_fake_decision.size()).cuda()))\n",
    "\n",
    "        # Back propagation\n",
    "        D_A_loss = (D_A_real_loss + D_A_fake_loss) * 0.5\n",
    "        D_A_optimizer.zero_grad()\n",
    "        D_A_loss.backward()\n",
    "        D_A_optimizer.step()\n",
    "\n",
    "        # Train discriminator D_B\n",
    "        D_B_real_decision = D_B(real_B)\n",
    "        D_B_real_loss = MSE_loss(D_B_real_decision, Variable(torch.ones(D_B_real_decision.size()).cuda()))\n",
    "        fake_B = fake_B_pool.query(fake_B)\n",
    "        D_B_fake_decision = D_B(fake_B)\n",
    "        D_B_fake_loss = MSE_loss(D_B_fake_decision, Variable(torch.zeros(D_B_fake_decision.size()).cuda()))\n",
    "\n",
    "        # Back propagation\n",
    "        D_B_loss = (D_B_real_loss + D_B_fake_loss) * 0.5\n",
    "        D_B_optimizer.zero_grad()\n",
    "        D_B_loss.backward()\n",
    "        D_B_optimizer.step()\n",
    "\n",
    "        # loss values\n",
    "        D_A_losses.append(D_A_loss.data[0])\n",
    "        D_B_losses.append(D_B_loss.data[0])\n",
    "        G_A_losses.append(G_A_loss.data[0])\n",
    "        G_B_losses.append(G_B_loss.data[0])\n",
    "        cycle_A_losses.append(cycle_A_loss.data[0])\n",
    "        cycle_B_losses.append(cycle_B_loss.data[0])\n",
    "\n",
    "        print('Epoch [%d/%d], Step [%d/%d], D_A_loss: %.4f, D_B_loss: %.4f, G_A_loss: %.4f, G_B_loss: %.4f'\n",
    "              % (epoch+1, params.num_epochs, i+1, len(train_data_loader_A), D_A_loss.data[0], D_B_loss.data[0], G_A_loss.data[0], G_B_loss.data[0]))\n",
    "\n",
    "        # ============ TensorBoard logging ============#\n",
    "        D_A_logger.scalar_summary('losses', D_A_loss.data[0], step + 1)\n",
    "        D_B_logger.scalar_summary('losses', D_B_loss.data[0], step + 1)\n",
    "        G_A_logger.scalar_summary('losses', G_A_loss.data[0], step + 1)\n",
    "        G_B_logger.scalar_summary('losses', G_B_loss.data[0], step + 1)\n",
    "        cycle_A_logger.scalar_summary('losses', cycle_A_loss.data[0], step + 1)\n",
    "        cycle_B_logger.scalar_summary('losses', cycle_B_loss.data[0], step + 1)\n",
    "        step += 1\n",
    "\n",
    "    D_A_avg_loss = torch.mean(torch.FloatTensor(D_A_losses))\n",
    "    D_B_avg_loss = torch.mean(torch.FloatTensor(D_B_losses))\n",
    "    G_A_avg_loss = torch.mean(torch.FloatTensor(G_A_losses))\n",
    "    G_B_avg_loss = torch.mean(torch.FloatTensor(G_B_losses))\n",
    "    cycle_A_avg_loss = torch.mean(torch.FloatTensor(cycle_A_losses))\n",
    "    cycle_B_avg_loss = torch.mean(torch.FloatTensor(cycle_B_losses))\n",
    "\n",
    "    # avg loss values for plot\n",
    "    D_A_avg_losses.append(D_A_avg_loss)\n",
    "    D_B_avg_losses.append(D_B_avg_loss)\n",
    "    G_A_avg_losses.append(G_A_avg_loss)\n",
    "    G_B_avg_losses.append(G_B_avg_loss)\n",
    "    cycle_A_avg_losses.append(cycle_A_avg_loss)\n",
    "    cycle_B_avg_losses.append(cycle_B_avg_loss)\n",
    "\n",
    "    # Show result for test image\n",
    "    test_real_A = Variable(test_real_A_data.cuda())\n",
    "    test_fake_B = G_A(test_real_A)\n",
    "    test_recon_A = G_B(test_fake_B)\n",
    "\n",
    "    test_real_B = Variable(test_real_B_data.cuda())\n",
    "    test_fake_A = G_B(test_real_B)\n",
    "    test_recon_B = G_A(test_fake_A)\n",
    "\n",
    "    utils.plot_train_result([test_real_A, test_real_B], [test_fake_B, test_fake_A], [test_recon_A, test_recon_B],\n",
    "                            epoch, save=True, save_dir=save_dir)\n",
    "\n",
    "    # log the images\n",
    "    result_AtoB = np.concatenate((utils.to_np(test_real_A), utils.to_np(test_fake_B), utils.to_np(test_recon_A)), axis=3)\n",
    "    result_BtoA = np.concatenate((utils.to_np(test_real_B), utils.to_np(test_fake_A), utils.to_np(test_recon_B)), axis=3)\n",
    "\n",
    "    info = {\n",
    "        'result_AtoB': result_AtoB.transpose(0, 2, 3, 1),  # convert to BxHxWxC\n",
    "        'result_BtoA': result_BtoA.transpose(0, 2, 3, 1)\n",
    "    }\n",
    "\n",
    "    for tag, images in info.items():\n",
    "        img_logger.image_summary(tag, images, epoch + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=1, beta1=0.5, beta2=0.999, crop_size=256, dataset='horse2zebra', decay_epoch=100, fliplr=True, input_size=256, lambdaA=10, lambdaB=10, lrD=0.0002, lrG=0.0002, ndf=64, ngf=32, num_epochs=200, num_resnet=6, resize_scale=286)\n",
      "Epoch [1/200], Step [1/1067], D_A_loss: 0.6513, D_B_loss: 0.4340, G_A_loss: 0.6177, G_B_loss: 1.2352\n",
      "Epoch [1/200], Step [2/1067], D_A_loss: 1.3525, D_B_loss: 1.4868, G_A_loss: 0.2566, G_B_loss: 0.3821\n",
      "Epoch [1/200], Step [3/1067], D_A_loss: 2.9805, D_B_loss: 1.3504, G_A_loss: 0.1625, G_B_loss: 0.3528\n",
      "Epoch [1/200], Step [4/1067], D_A_loss: 0.5265, D_B_loss: 0.4577, G_A_loss: 0.3338, G_B_loss: 0.3466\n",
      "Epoch [1/200], Step [5/1067], D_A_loss: 0.4774, D_B_loss: 0.4753, G_A_loss: 0.6359, G_B_loss: 0.3336\n",
      "Epoch [1/200], Step [6/1067], D_A_loss: 0.4131, D_B_loss: 1.0043, G_A_loss: 0.5815, G_B_loss: 0.3932\n",
      "Epoch [1/200], Step [7/1067], D_A_loss: 0.3354, D_B_loss: 0.6969, G_A_loss: 0.4373, G_B_loss: 0.4031\n",
      "Epoch [1/200], Step [8/1067], D_A_loss: 0.3436, D_B_loss: 0.3997, G_A_loss: 0.3210, G_B_loss: 0.3849\n",
      "Epoch [1/200], Step [9/1067], D_A_loss: 0.3102, D_B_loss: 0.3836, G_A_loss: 0.3399, G_B_loss: 0.3347\n",
      "Epoch [1/200], Step [10/1067], D_A_loss: 0.2928, D_B_loss: 0.3893, G_A_loss: 0.3361, G_B_loss: 0.3293\n",
      "Epoch [1/200], Step [11/1067], D_A_loss: 0.2893, D_B_loss: 0.4819, G_A_loss: 0.3984, G_B_loss: 0.3633\n",
      "Epoch [1/200], Step [12/1067], D_A_loss: 0.3072, D_B_loss: 0.3976, G_A_loss: 0.5692, G_B_loss: 0.3227\n",
      "Epoch [1/200], Step [13/1067], D_A_loss: 0.2609, D_B_loss: 0.3915, G_A_loss: 0.4268, G_B_loss: 0.3380\n",
      "Epoch [1/200], Step [14/1067], D_A_loss: 0.2366, D_B_loss: 0.3478, G_A_loss: 0.3318, G_B_loss: 0.3805\n",
      "Epoch [1/200], Step [15/1067], D_A_loss: 0.2809, D_B_loss: 0.2959, G_A_loss: 0.3612, G_B_loss: 0.3073\n",
      "Epoch [1/200], Step [16/1067], D_A_loss: 0.2571, D_B_loss: 0.3190, G_A_loss: 0.4387, G_B_loss: 0.3374\n",
      "Epoch [1/200], Step [17/1067], D_A_loss: 0.2264, D_B_loss: 0.3873, G_A_loss: 0.3796, G_B_loss: 0.3201\n",
      "Epoch [1/200], Step [18/1067], D_A_loss: 0.2428, D_B_loss: 0.3136, G_A_loss: 0.3762, G_B_loss: 0.3228\n",
      "Epoch [1/200], Step [19/1067], D_A_loss: 0.2960, D_B_loss: 0.2972, G_A_loss: 0.3962, G_B_loss: 0.3581\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6b5f7c870de9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mG_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG_A_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mG_B_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcycle_A_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcycle_B_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mG_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mG_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0mG_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kongda/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kongda/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Plot average losses\n",
    "avg_losses = []\n",
    "avg_losses.append(D_A_avg_losses)\n",
    "avg_losses.append(D_B_avg_losses)\n",
    "avg_losses.append(G_A_avg_losses)\n",
    "avg_losses.append(G_B_avg_losses)\n",
    "avg_losses.append(cycle_A_avg_losses)\n",
    "avg_losses.append(cycle_B_avg_losses)\n",
    "utils.plot_loss(avg_losses, params.num_epochs, save=True, save_dir=save_dir)\n",
    "\n",
    "# Make gif\n",
    "utils.make_gif(params.dataset, params.num_epochs, save_dir=save_dir)\n",
    "# Save trained parameters of model\n",
    "torch.save(G_A.state_dict(), model_dir + 'generator_A_param.pkl')\n",
    "torch.save(G_B.state_dict(), model_dir + 'generator_B_param.pkl')\n",
    "torch.save(D_A.state_dict(), model_dir + 'discriminator_A_param.pkl')\n",
    "torch.save(D_B.state_dict(), model_dir + 'discriminator_B_param.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
