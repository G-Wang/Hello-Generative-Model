{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch, os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "from utils.visdom_utils import VisFunc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architectures\n",
    "![infogan](http://nooverfit.com/wp/wp-content/uploads/2017/10/QQ%E6%88%AA%E5%9B%BE20171009174341.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Front-end Feature Extractor in Discriminator  \n",
    "<br>\n",
    "InfoGAN의 Architecture에서 볼 수 있는 특이한 점은,  \n",
    "Discriminator D와 Encoder Q가 서로 독립적인 네트워크가 아니라,  \n",
    "네트워크의 일정 부분을 서로 공유하고 있다는 점입니다.  \n",
    "<br>\n",
    "D와 Q가 서로 공유하고 있는 부분을 보통 Front-End라고 부릅니다.  \n",
    "<br>\n",
    "true image와 fake image들은 우선 Front-End를 통과하게 되고,  \n",
    "거기서 나온 feature들은 두 갈래로 나뉘어 D와 Q의 output으로 각각 나오게 됩니다. \n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "또한 DCGAN 이후로 다양한 GAN variants들에서 볼 수 있는 일반적인 테크닉들이 InfoGAN에도 적용되어 있습니다.  \n",
    "\n",
    "* Discriminator에는 Leaky ReLU를, Generator에는 ReLU를 사용하는 것이 가장 일반적인 방식이 되었습니다.  \n",
    "* Batch Normalization도 GAN에서 사용이 되고 있습니다. (그러나 최근에는 BN 말고도 더 다양한 Normalization을 사용하기도 합니다.)   \n",
    "* Generator의 마지막 레이어와 Discriminator의 첫 번째 레이어는 BN을 적용하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FrontEnd(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FrontEnd, self).__init__()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(1,64,4,2,1),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(64,128,4,2,1,bias=False),\n",
    "            nn.BatchNorm2d(128), \n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(128, 1024,7,bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        output = self.main(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator D  \n",
    "<br>\n",
    "이미지의 True or False를 판별하기 위한 Feature들은 Front-End에서 충분히 뽑혔다고 판단한 것 같습니다.  \n",
    "D의 네트워크는 매우 단순합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dmodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Dmodel, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(1024,1,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        output=self.main(x).view(-1,1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Q  \n",
    "<br>\n",
    "D와 마찬가지로 Q도 매우 단순한 구조로 되어 있습니다.  \n",
    "Q는 discrete code와 continuous code를 예측해야하는데,  \n",
    "discrete code는 CrossEntropy를 이용해서 reconstruction loss를 계산하기 때문에 logit을,  \n",
    "continuous code는 Log Gaussian을 이용해서 reconstruction loss를 계산하기 때문에 mu와 var를 output하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Qmodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Qmodel,self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(1024,128,1,bias=False) \n",
    "        self.bn = nn.BatchNorm2d(128)\n",
    "        self.lReLU = nn.LeakyReLU(0.1, inplace=True)\n",
    "        self.conv_disc = nn.Conv2d(128,10,1)\n",
    "        self.conv_mu = nn.Conv2d(128,2,1)\n",
    "        self.conv_var = nn.Conv2d(128,2,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        y = self.conv(x)\n",
    "        disc_logits = self.conv_disc(y).squeeze()\n",
    "        mu = self.conv_mu(y).squeeze()\n",
    "        var = self.conv_var(y).squeeze().exp()\n",
    "        return disc_logits, mu, var\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Gmodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Gmodel, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(74, 1024,1,1, bias=False), # noise 62 + discrete code 10 + continuous code 2 = 74\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(1024, 128,7,1,bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128,64,4,2,1,bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64,1,4,2,1,bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.main(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Models and Optimizer and Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    \n",
    "# Models\n",
    "FE=FrontEnd()\n",
    "D=Dmodel()\n",
    "Q=Qmodel()\n",
    "G=Gmodel()\n",
    "\n",
    "for i in [FE, D, Q, G]:\n",
    "    i.cuda()\n",
    "    i.apply(weights_init)\n",
    "\n",
    "# Optimizers\n",
    "optimD = optim.Adam([{'params':FE.parameters()},\n",
    "                     {'params':D.parameters()}],\n",
    "                    lr=0.0001, betas=(0.5, 0.99) )\n",
    "\n",
    "optimG = optim.Adam([{'params':G.parameters()},\n",
    "                     {'params':Q.parameters()}],\n",
    "                    lr=0.0002, betas=(0.5, 0.99) )\n",
    "\n",
    "# Datasets\n",
    "batch_size = 100\n",
    "\n",
    "# Train using 10K Test Images\n",
    "train_data = dset.MNIST('./dataset', train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "\n",
    "# Test using 60K Train Images\n",
    "test_data = dset.MNIST('./dataset', train=False, transform=transforms.ToTensor(), download=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=4, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs(datasets, noises) and Visualization Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fixed random variables for test\n",
    "c0 = torch.linspace(-1,1,10).view(-1,1).repeat(10,0)\n",
    "c1 = torch.stack((c0, torch.zeros(1).expand_as(c0)),1).cuda()\n",
    "c2 = torch.stack((torch.zeros(1).expand_as(c0), c0),1).cuda()\n",
    "one_hot = torch.eye(10).repeat(1,1,10).view(100,10).cuda()\n",
    "fix_noise = torch.Tensor(100, 62).uniform_(-1, 1).cuda()\n",
    "\n",
    "# random noises sampling function\n",
    "def _noise_sample(dis_c, con_c, noise, bs):\n",
    "    idx = np.random.randint(10, size=bs)\n",
    "    c = np.zeros((bs, 10))\n",
    "    c[range(bs),idx] = 1.0\n",
    "    dis_c.data.copy_(torch.Tensor(c))\n",
    "    con_c.data.uniform_(-1.0, 1.0)\n",
    "    noise.data.uniform_(-1.0, 1.0)\n",
    "    z = torch.cat([noise, dis_c, con_c], 1).view(-1, 74, 1, 1)\n",
    "    return z, idx\n",
    "\n",
    "# Visdom\n",
    "env_name = 'infoGAN'\n",
    "vf = VisFunc(enval=env_name)\n",
    "\n",
    "# Generated Image Folder\n",
    "if not os.path.exists('tmp') : os.makedirs('tmp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![log_gaussian](https://user-images.githubusercontent.com/613623/30778123-7328abc0-a0cd-11e7-8998-7e4ef07cc25f.png)\n",
    "https://user-images.githubusercontent.com/613623/30778123-7328abc0-a0cd-11e7-8998-7e4ef07cc25f.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Losses\n",
    "class log_gaussian:\n",
    "    def __call__(self, x, mu, var):\n",
    "        logli = -0.5*(var.mul(2*np.pi)+1e-6).log() - (x-mu).pow(2).div(var.mul(2.0)+1e-6)\n",
    "        return logli.sum(1).mean().mul(-1)\n",
    "\n",
    "criterionD = nn.BCELoss()\n",
    "criterionQ_dis = nn.CrossEntropyLoss()\n",
    "criterionQ_con = log_gaussian()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Classification Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_mode(mode='train'):\n",
    "    if mode == 'train' : \n",
    "        G.train()\n",
    "        FE.train()\n",
    "        D.train()\n",
    "        Q.train()\n",
    "    elif mode == 'eval' :\n",
    "        G.eval()\n",
    "        FE.eval()\n",
    "        D.eval()\n",
    "        Q.eval()\n",
    "    else : raise BaseException('wrong mode') \n",
    "    \n",
    "def test(num_class=10):\n",
    "        '''\n",
    "        1. 클러스터링 결과를 2D 테이블로 만든다\n",
    "        2. 테이블을 이용해서 클러스터가 얼마나 잘 뭉쳤는지 계산한다(hungarian algorithm 이용)\n",
    "        \n",
    "        Table Example : \n",
    "        \n",
    "             true\n",
    "             label 0     1     2     3     4     5     6     7     8     9\n",
    "     cluster  | - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "        0     |    0     1     0     1     0     3     5     1     0   969\n",
    "        1     |    0     3     1     2     7   613     4     0   505     0\n",
    "        2     |    5   547     3     3   461     5     1     0     5     2\n",
    "        3     |    8     5     0   950    17     1     0    25     2     2\n",
    "        4     |    2   184   774     0     0     6     6     0    10     0\n",
    "        5     |    1     1     0     3     3     1     5   872     4     2\n",
    "        6     |    0     4     2     0     0     4   927    13     0     8\n",
    "        7     |  684    13    88     1    11     2     0     2   226     1\n",
    "        8     |    2    13     3     7   592   317     7    15    12     6\n",
    "        9     |  302   158   273    14     5     5     1     7   241     3\n",
    "        \n",
    "        위 테이블을 보면, 클러스터 0은 9라는 숫자가 제일 많이 차지하고 있는 것을 알 수 있다.( 첫 번째 행 )\n",
    "        또한 클러스터 3은 3이라는 숫자가 제일 많이 차지하고 있는 것을 알 수 있다. ( 네 번째 행 )\n",
    "        \n",
    "        클러스터 0, 3, 5, 6 처럼 클러스터링이 잘 된 곳이 있는 반면,\n",
    "        클러스터 1, 2, 4, 7, 8, 9는 여러 종류의 숫자가 섞여 있는 것을 알 수 있다.\n",
    "        \n",
    "        이러한 테이블을 만들어 놓고 hungarian algorithm을 이용하면 clustering performance를 계산할 수 있다.\n",
    "        '''\n",
    "        \n",
    "        import torch.nn.functional as F\n",
    "        from scipy.optimize import linear_sum_assignment\n",
    "        \n",
    "        set_mode('eval')\n",
    "        \n",
    "        LABEL = [] # 각 트레이닝 샘플의 label을 저장\n",
    "        PREDICT = [] # 각 트레이닝 샘플의 encoder prediction 결과를 저장\n",
    "        TABLE = [[0 for i in range(num_class)] for k in range(num_class)] # LABEL과 PREDICT를 종합해서 위에서 처럼 2D 테이블을 만들어야 한다.\n",
    "        total = 0\n",
    "        for batch_idx, (images,labels) in enumerate(test_loader):\n",
    "            real_x = Variable(images.cuda()) # test image 뽑아서\n",
    "            fe_out = FE(real_x) # FE에 넣고\n",
    "            q_logits, _, _ = Q(fe_out) # discrete code의 logit을 뽑은 뒤\n",
    "            q_softmax = F.softmax(q_logits) # softmax를 통과시켜\n",
    "            q_index = q_softmax.max(1)[1] # 가장 likely한 클러스터를 찾는다.\n",
    "            \n",
    "            PREDICT.append(q_index.data) # 각 트레이닝 샘플의 클러스터와 실제 레이블을 차곡차곡 쌓는다.\n",
    "            LABEL.append(labels)\n",
    "            total += labels.size(0)\n",
    "\n",
    "        # 위에서 쌓은 결과를 이용해서 테이블을 채운다.\n",
    "        LABEL = torch.cat(LABEL,0)\n",
    "        PREDICT = torch.cat(PREDICT,0).cpu()\n",
    "        for idx, label in enumerate(LABEL):\n",
    "            TABLE[label][PREDICT[idx]] += 1\n",
    "\n",
    "        # 테이블을 이용해서 hungarian algorithm을 수행한다.\n",
    "        TABLE = torch.FloatTensor(TABLE)\n",
    "        row, col = linear_sum_assignment(-TABLE.numpy())\n",
    "        acc = TABLE.numpy()[row,col].sum()/total\n",
    "\n",
    "        print(TABLE)\n",
    "        print('[TEST RESULT] : ACC : {:.4f}%'.format(acc*100))\n",
    "\n",
    "        set_mode('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    0     0   957     0     0     0    23     0     0     0\n",
      "    0     0  1104     0     0     0    31     0     0     0\n",
      "    0     0   934     0     0     0    98     0     0     0\n",
      "    0     0   888     0     0     0   122     0     0     0\n",
      "    0     0   526     0     0     0   456     0     0     0\n",
      "    0     0   706     0     0     0   186     0     0     0\n",
      "    0     0   730     0     0     0   228     0     0     0\n",
      "    0     0  1008     0     0     0    20     0     0     0\n",
      "    0     0   750     0     0     0   224     0     0     0\n",
      "    0     0   731     0     0     0   278     0     0     0\n",
      "[torch.FloatTensor of size 10x10]\n",
      "\n",
      "[TEST RESULT] : ACC : 15.6000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wonkonge/anaconda3/envs/kongda/lib/python3.6/site-packages/torch/nn/functional.py:767: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Iter:0, Dloss: 1.431, Gloss: 3.204, Preal: 0.549, Pfake: 0.522\n",
      "Epoch:0, Iter:50, Dloss: 1.145, Gloss: 1.060, Preal: 0.571, Pfake: 0.421\n",
      "Epoch:0, Iter:100, Dloss: 1.002, Gloss: 0.982, Preal: 0.612, Pfake: 0.375\n",
      "Epoch:0, Iter:150, Dloss: 0.882, Gloss: 1.062, Preal: 0.638, Pfake: 0.321\n",
      "Epoch:0, Iter:200, Dloss: 0.698, Gloss: 1.296, Preal: 0.712, Pfake: 0.273\n",
      "Epoch:0, Iter:250, Dloss: 0.599, Gloss: 1.491, Preal: 0.743, Pfake: 0.236\n",
      "Epoch:0, Iter:300, Dloss: 0.579, Gloss: 1.431, Preal: 0.738, Pfake: 0.216\n",
      "Epoch:0, Iter:350, Dloss: 0.476, Gloss: 1.591, Preal: 0.787, Pfake: 0.186\n",
      "Epoch:0, Iter:400, Dloss: 0.396, Gloss: 1.677, Preal: 0.833, Pfake: 0.168\n",
      "Epoch:0, Iter:450, Dloss: 0.368, Gloss: 1.829, Preal: 0.831, Pfake: 0.148\n",
      "Epoch:0, Iter:500, Dloss: 0.350, Gloss: 1.909, Preal: 0.845, Pfake: 0.145\n",
      "Epoch:0, Iter:550, Dloss: 0.333, Gloss: 1.986, Preal: 0.851, Pfake: 0.136\n",
      "Epoch:1, Iter:0, Dloss: 0.253, Gloss: 1.976, Preal: 0.911, Pfake: 0.129\n",
      "Epoch:1, Iter:50, Dloss: 0.332, Gloss: 2.028, Preal: 0.860, Pfake: 0.150\n",
      "Epoch:1, Iter:100, Dloss: 0.262, Gloss: 2.115, Preal: 0.898, Pfake: 0.125\n",
      "Epoch:1, Iter:150, Dloss: 0.283, Gloss: 2.269, Preal: 0.872, Pfake: 0.113\n",
      "Epoch:1, Iter:200, Dloss: 0.365, Gloss: 2.189, Preal: 0.852, Pfake: 0.170\n",
      "Epoch:1, Iter:250, Dloss: 0.443, Gloss: 2.165, Preal: 0.798, Pfake: 0.220\n",
      "Epoch:1, Iter:300, Dloss: 0.599, Gloss: 1.708, Preal: 0.774, Pfake: 0.213\n",
      "Epoch:1, Iter:350, Dloss: 0.680, Gloss: 1.443, Preal: 0.716, Pfake: 0.251\n",
      "Epoch:1, Iter:400, Dloss: 0.845, Gloss: 1.320, Preal: 0.685, Pfake: 0.280\n",
      "Epoch:1, Iter:450, Dloss: 0.828, Gloss: 1.370, Preal: 0.660, Pfake: 0.302\n",
      "Epoch:1, Iter:500, Dloss: 0.786, Gloss: 1.350, Preal: 0.702, Pfake: 0.293\n",
      "Epoch:1, Iter:550, Dloss: 0.971, Gloss: 1.307, Preal: 0.622, Pfake: 0.296\n",
      "Epoch:2, Iter:0, Dloss: 0.989, Gloss: 1.169, Preal: 0.650, Pfake: 0.329\n",
      "Epoch:2, Iter:50, Dloss: 0.939, Gloss: 1.216, Preal: 0.661, Pfake: 0.312\n",
      "Epoch:2, Iter:100, Dloss: 1.022, Gloss: 0.986, Preal: 0.633, Pfake: 0.391\n",
      "Epoch:2, Iter:150, Dloss: 0.997, Gloss: 1.148, Preal: 0.614, Pfake: 0.340\n",
      "Epoch:2, Iter:200, Dloss: 0.991, Gloss: 1.133, Preal: 0.608, Pfake: 0.334\n",
      "Epoch:2, Iter:250, Dloss: 1.006, Gloss: 1.091, Preal: 0.623, Pfake: 0.361\n",
      "Epoch:2, Iter:300, Dloss: 0.965, Gloss: 1.087, Preal: 0.632, Pfake: 0.351\n",
      "Epoch:2, Iter:350, Dloss: 0.994, Gloss: 1.021, Preal: 0.628, Pfake: 0.358\n",
      "Epoch:2, Iter:400, Dloss: 1.028, Gloss: 0.982, Preal: 0.604, Pfake: 0.373\n",
      "Epoch:2, Iter:450, Dloss: 1.070, Gloss: 1.009, Preal: 0.580, Pfake: 0.356\n",
      "Epoch:2, Iter:500, Dloss: 1.088, Gloss: 0.995, Preal: 0.608, Pfake: 0.373\n",
      "Epoch:2, Iter:550, Dloss: 1.011, Gloss: 1.062, Preal: 0.605, Pfake: 0.356\n",
      "Epoch:3, Iter:0, Dloss: 1.114, Gloss: 1.022, Preal: 0.607, Pfake: 0.385\n",
      "Epoch:3, Iter:50, Dloss: 1.095, Gloss: 1.196, Preal: 0.569, Pfake: 0.327\n",
      "Epoch:3, Iter:100, Dloss: 1.092, Gloss: 1.007, Preal: 0.612, Pfake: 0.370\n",
      "Epoch:3, Iter:150, Dloss: 1.119, Gloss: 0.987, Preal: 0.604, Pfake: 0.382\n",
      "Epoch:3, Iter:200, Dloss: 1.123, Gloss: 1.039, Preal: 0.583, Pfake: 0.371\n",
      "Epoch:3, Iter:250, Dloss: 1.150, Gloss: 0.967, Preal: 0.555, Pfake: 0.370\n",
      "Epoch:3, Iter:300, Dloss: 1.156, Gloss: 0.955, Preal: 0.585, Pfake: 0.381\n",
      "Epoch:3, Iter:350, Dloss: 1.147, Gloss: 0.974, Preal: 0.606, Pfake: 0.389\n",
      "Epoch:3, Iter:400, Dloss: 1.139, Gloss: 0.996, Preal: 0.583, Pfake: 0.384\n",
      "Epoch:3, Iter:450, Dloss: 1.114, Gloss: 1.043, Preal: 0.603, Pfake: 0.372\n",
      "Epoch:3, Iter:500, Dloss: 1.115, Gloss: 0.917, Preal: 0.596, Pfake: 0.386\n",
      "Epoch:3, Iter:550, Dloss: 1.241, Gloss: 0.946, Preal: 0.548, Pfake: 0.397\n",
      "Epoch:4, Iter:0, Dloss: 1.083, Gloss: 0.914, Preal: 0.610, Pfake: 0.383\n",
      "Epoch:4, Iter:50, Dloss: 1.075, Gloss: 1.029, Preal: 0.599, Pfake: 0.364\n",
      "Epoch:4, Iter:100, Dloss: 1.145, Gloss: 0.922, Preal: 0.602, Pfake: 0.392\n",
      "Epoch:4, Iter:150, Dloss: 1.115, Gloss: 1.006, Preal: 0.583, Pfake: 0.375\n",
      "Epoch:4, Iter:200, Dloss: 1.120, Gloss: 0.976, Preal: 0.580, Pfake: 0.377\n",
      "Epoch:4, Iter:250, Dloss: 1.166, Gloss: 0.952, Preal: 0.591, Pfake: 0.389\n",
      "Epoch:4, Iter:300, Dloss: 1.111, Gloss: 0.997, Preal: 0.591, Pfake: 0.368\n",
      "Epoch:4, Iter:350, Dloss: 1.195, Gloss: 0.952, Preal: 0.561, Pfake: 0.379\n",
      "Epoch:4, Iter:400, Dloss: 1.161, Gloss: 0.966, Preal: 0.563, Pfake: 0.373\n",
      "Epoch:4, Iter:450, Dloss: 1.157, Gloss: 0.944, Preal: 0.585, Pfake: 0.379\n",
      "Epoch:4, Iter:500, Dloss: 1.157, Gloss: 0.964, Preal: 0.592, Pfake: 0.383\n",
      "Epoch:4, Iter:550, Dloss: 1.167, Gloss: 0.941, Preal: 0.597, Pfake: 0.399\n",
      "\n",
      "    0     4     0    39     4     0   922     0    10     1\n",
      "  231     2   890     2     2     5     0     0     3     0\n",
      "   10    15    19     7     9   920    24     5     7    16\n",
      "  167    19     1   129   453    20    25     2   181    13\n",
      "    5    83     2   212     1     6     1   661     2     9\n",
      "   47    14     1    89   575     2    13     7   134    10\n",
      "   91     0     2   207     8     4    32     3   611     0\n",
      "    7     9    35    54     3    23     3    27     2   865\n",
      "  255   542     6    39    30     5    25    11    58     3\n",
      "    8   275     4   324     5     2    14   338     2    37\n",
      "[torch.FloatTensor of size 10x10]\n",
      "\n",
      "[TEST RESULT] : ACC : 64.7700%\n",
      "Epoch:5, Iter:0, Dloss: 1.130, Gloss: 0.954, Preal: 0.608, Pfake: 0.385\n",
      "Epoch:5, Iter:50, Dloss: 1.153, Gloss: 0.907, Preal: 0.621, Pfake: 0.405\n",
      "Epoch:5, Iter:100, Dloss: 1.093, Gloss: 0.964, Preal: 0.601, Pfake: 0.364\n",
      "Epoch:5, Iter:150, Dloss: 1.183, Gloss: 0.900, Preal: 0.602, Pfake: 0.404\n",
      "Epoch:5, Iter:200, Dloss: 1.151, Gloss: 1.010, Preal: 0.569, Pfake: 0.371\n",
      "Epoch:5, Iter:250, Dloss: 1.100, Gloss: 0.921, Preal: 0.629, Pfake: 0.388\n",
      "Epoch:5, Iter:300, Dloss: 1.166, Gloss: 0.934, Preal: 0.583, Pfake: 0.396\n",
      "Epoch:5, Iter:350, Dloss: 1.106, Gloss: 0.956, Preal: 0.608, Pfake: 0.380\n",
      "Epoch:5, Iter:400, Dloss: 1.174, Gloss: 0.957, Preal: 0.573, Pfake: 0.381\n",
      "Epoch:5, Iter:450, Dloss: 1.190, Gloss: 0.972, Preal: 0.555, Pfake: 0.376\n",
      "Epoch:5, Iter:500, Dloss: 1.219, Gloss: 0.929, Preal: 0.559, Pfake: 0.391\n",
      "Epoch:5, Iter:550, Dloss: 1.168, Gloss: 0.983, Preal: 0.571, Pfake: 0.372\n",
      "Epoch:6, Iter:0, Dloss: 1.172, Gloss: 0.950, Preal: 0.560, Pfake: 0.388\n",
      "Epoch:6, Iter:50, Dloss: 1.126, Gloss: 0.935, Preal: 0.613, Pfake: 0.400\n",
      "Epoch:6, Iter:100, Dloss: 1.118, Gloss: 0.916, Preal: 0.609, Pfake: 0.394\n",
      "Epoch:6, Iter:150, Dloss: 1.178, Gloss: 0.896, Preal: 0.591, Pfake: 0.401\n",
      "Epoch:6, Iter:200, Dloss: 1.122, Gloss: 0.977, Preal: 0.609, Pfake: 0.382\n",
      "Epoch:6, Iter:250, Dloss: 1.128, Gloss: 0.941, Preal: 0.583, Pfake: 0.388\n",
      "Epoch:6, Iter:300, Dloss: 1.132, Gloss: 1.017, Preal: 0.592, Pfake: 0.373\n",
      "Epoch:6, Iter:350, Dloss: 1.174, Gloss: 0.957, Preal: 0.555, Pfake: 0.381\n",
      "Epoch:6, Iter:400, Dloss: 1.112, Gloss: 0.974, Preal: 0.581, Pfake: 0.368\n",
      "Epoch:6, Iter:450, Dloss: 1.072, Gloss: 1.026, Preal: 0.602, Pfake: 0.351\n",
      "Epoch:6, Iter:500, Dloss: 1.194, Gloss: 0.993, Preal: 0.569, Pfake: 0.375\n",
      "Epoch:6, Iter:550, Dloss: 1.212, Gloss: 0.915, Preal: 0.593, Pfake: 0.392\n",
      "Epoch:7, Iter:0, Dloss: 1.116, Gloss: 0.990, Preal: 0.603, Pfake: 0.376\n",
      "Epoch:7, Iter:50, Dloss: 1.115, Gloss: 1.016, Preal: 0.564, Pfake: 0.356\n",
      "Epoch:7, Iter:100, Dloss: 1.141, Gloss: 0.927, Preal: 0.590, Pfake: 0.386\n",
      "Epoch:7, Iter:150, Dloss: 1.111, Gloss: 0.961, Preal: 0.601, Pfake: 0.389\n",
      "Epoch:7, Iter:200, Dloss: 1.162, Gloss: 0.988, Preal: 0.590, Pfake: 0.376\n",
      "Epoch:7, Iter:250, Dloss: 1.119, Gloss: 1.068, Preal: 0.583, Pfake: 0.354\n",
      "Epoch:7, Iter:300, Dloss: 1.148, Gloss: 0.998, Preal: 0.573, Pfake: 0.374\n",
      "Epoch:7, Iter:350, Dloss: 1.119, Gloss: 0.935, Preal: 0.586, Pfake: 0.384\n",
      "Epoch:7, Iter:400, Dloss: 1.152, Gloss: 0.941, Preal: 0.575, Pfake: 0.380\n",
      "Epoch:7, Iter:450, Dloss: 1.090, Gloss: 0.976, Preal: 0.584, Pfake: 0.368\n",
      "Epoch:7, Iter:500, Dloss: 1.156, Gloss: 0.975, Preal: 0.564, Pfake: 0.370\n",
      "Epoch:7, Iter:550, Dloss: 1.187, Gloss: 0.961, Preal: 0.578, Pfake: 0.388\n",
      "Epoch:8, Iter:0, Dloss: 1.093, Gloss: 0.976, Preal: 0.636, Pfake: 0.370\n",
      "Epoch:8, Iter:50, Dloss: 1.125, Gloss: 0.946, Preal: 0.565, Pfake: 0.373\n",
      "Epoch:8, Iter:100, Dloss: 1.125, Gloss: 1.079, Preal: 0.562, Pfake: 0.345\n",
      "Epoch:8, Iter:150, Dloss: 1.077, Gloss: 0.965, Preal: 0.600, Pfake: 0.362\n",
      "Epoch:8, Iter:200, Dloss: 1.159, Gloss: 0.994, Preal: 0.542, Pfake: 0.358\n",
      "Epoch:8, Iter:250, Dloss: 1.109, Gloss: 0.938, Preal: 0.594, Pfake: 0.380\n",
      "Epoch:8, Iter:300, Dloss: 1.093, Gloss: 0.951, Preal: 0.593, Pfake: 0.369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:8, Iter:350, Dloss: 1.043, Gloss: 0.967, Preal: 0.623, Pfake: 0.369\n",
      "Epoch:8, Iter:400, Dloss: 1.106, Gloss: 0.971, Preal: 0.595, Pfake: 0.369\n",
      "Epoch:8, Iter:450, Dloss: 1.133, Gloss: 1.021, Preal: 0.606, Pfake: 0.372\n",
      "Epoch:8, Iter:500, Dloss: 1.115, Gloss: 1.014, Preal: 0.574, Pfake: 0.354\n",
      "Epoch:8, Iter:550, Dloss: 1.008, Gloss: 0.962, Preal: 0.657, Pfake: 0.365\n",
      "Epoch:9, Iter:0, Dloss: 1.050, Gloss: 0.991, Preal: 0.625, Pfake: 0.361\n",
      "Epoch:9, Iter:50, Dloss: 1.089, Gloss: 1.001, Preal: 0.595, Pfake: 0.362\n",
      "Epoch:9, Iter:100, Dloss: 1.096, Gloss: 0.986, Preal: 0.584, Pfake: 0.360\n",
      "Epoch:9, Iter:150, Dloss: 0.998, Gloss: 0.991, Preal: 0.628, Pfake: 0.363\n",
      "Epoch:9, Iter:200, Dloss: 1.048, Gloss: 0.944, Preal: 0.646, Pfake: 0.388\n",
      "Epoch:9, Iter:250, Dloss: 1.037, Gloss: 1.124, Preal: 0.619, Pfake: 0.351\n",
      "Epoch:9, Iter:300, Dloss: 1.055, Gloss: 1.113, Preal: 0.597, Pfake: 0.331\n",
      "Epoch:9, Iter:350, Dloss: 1.096, Gloss: 1.015, Preal: 0.594, Pfake: 0.360\n",
      "Epoch:9, Iter:400, Dloss: 1.133, Gloss: 0.990, Preal: 0.549, Pfake: 0.356\n",
      "Epoch:9, Iter:450, Dloss: 1.067, Gloss: 1.022, Preal: 0.630, Pfake: 0.368\n",
      "Epoch:9, Iter:500, Dloss: 1.170, Gloss: 1.007, Preal: 0.563, Pfake: 0.362\n",
      "Epoch:9, Iter:550, Dloss: 1.139, Gloss: 0.966, Preal: 0.567, Pfake: 0.376\n",
      "\n",
      "    1     1     0     2     1     1   966     0     8     0\n",
      "   27     6  1092     0     0     8     0     0     1     1\n",
      "    6    14     1     1     4   965    18     5     6    12\n",
      "  248     7     0    97   411    17     5     1   215     9\n",
      "   15   156     3    23     0     7     1   776     1     0\n",
      "   70     2     0    92   575     1     5     1   145     1\n",
      "  245     1     2     7     4     6     8     3   682     0\n",
      "    1     9     8    15     2    51     5    14     4   919\n",
      "  155   722     0    24    11     8     3     4    39     8\n",
      "    2    44     3   631     4     2    12   285     4    22\n",
      "[torch.FloatTensor of size 10x10]\n",
      "\n",
      "[TEST RESULT] : ACC : 75.7600%\n",
      "Epoch:10, Iter:0, Dloss: 1.159, Gloss: 1.020, Preal: 0.585, Pfake: 0.356\n",
      "Epoch:10, Iter:50, Dloss: 0.961, Gloss: 1.076, Preal: 0.613, Pfake: 0.322\n",
      "Epoch:10, Iter:100, Dloss: 1.067, Gloss: 1.031, Preal: 0.607, Pfake: 0.348\n",
      "Epoch:10, Iter:150, Dloss: 1.075, Gloss: 1.046, Preal: 0.583, Pfake: 0.340\n",
      "Epoch:10, Iter:200, Dloss: 1.120, Gloss: 0.906, Preal: 0.630, Pfake: 0.391\n",
      "Epoch:10, Iter:250, Dloss: 1.133, Gloss: 1.058, Preal: 0.642, Pfake: 0.370\n",
      "Epoch:10, Iter:300, Dloss: 1.089, Gloss: 1.151, Preal: 0.630, Pfake: 0.365\n",
      "Epoch:10, Iter:350, Dloss: 1.050, Gloss: 1.024, Preal: 0.647, Pfake: 0.355\n",
      "Epoch:10, Iter:400, Dloss: 1.119, Gloss: 0.965, Preal: 0.593, Pfake: 0.369\n",
      "Epoch:10, Iter:450, Dloss: 1.045, Gloss: 1.151, Preal: 0.619, Pfake: 0.361\n",
      "Epoch:10, Iter:500, Dloss: 1.168, Gloss: 0.899, Preal: 0.621, Pfake: 0.392\n",
      "Epoch:10, Iter:550, Dloss: 1.050, Gloss: 0.964, Preal: 0.619, Pfake: 0.366\n",
      "Epoch:11, Iter:0, Dloss: 1.020, Gloss: 1.002, Preal: 0.646, Pfake: 0.354\n",
      "Epoch:11, Iter:50, Dloss: 1.126, Gloss: 0.989, Preal: 0.574, Pfake: 0.358\n",
      "Epoch:11, Iter:100, Dloss: 1.086, Gloss: 1.056, Preal: 0.618, Pfake: 0.343\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    if (epoch % 5) == 0 :  test(num_class=10)\n",
    "    for num_iters, batch_data in enumerate(train_loader,0):\n",
    "\n",
    "        # real part\n",
    "        optimD.zero_grad() # D를 학습시켜야 하므로 우선 gradient를 초기화 시켜주고\n",
    "\n",
    "        x, _ = batch_data # 이미지를 불러온 뒤\n",
    "        real_x = Variable(x.cuda()) # Variable로 만들어 준다.\n",
    "        label = Variable(torch.ones(batch_size).float().cuda(), requires_grad=False) # real 이미지에 대해서는 BCE Loss 정답을 1로 만들어 준다.\n",
    "\n",
    "        fe_out1 = FE(real_x) # 이미지를 FE에 넣어서 feature를 뽑고\n",
    "        probs_real = D(fe_out1) # 이 feature를 다시 D에 넣어서 true/false 값을 얻는다.\n",
    "        label.data.fill_(1) # real 이미지에 대해서는 BCE Loss 정답을 1로 만들어 준다.\n",
    "        loss_real = criterionD(probs_real, label) # Loss 계산한 뒤\n",
    "        loss_real.backward() # D를 학습시킨다.\n",
    "\n",
    "        # fake part\n",
    "        # fake 이미지에 대해서도 D를 트레이닝 시켜준다.\n",
    "        dis_c = Variable(torch.FloatTensor(batch_size,10).cuda()) # 랜덤하게 discrete code와\n",
    "        con_c = Variable(torch.FloatTensor(batch_size,2).cuda()) # continuous code,\n",
    "        noise = Variable(torch.FloatTensor(batch_size,62).cuda()) # 그리고 noise vector를 먼저 만든 뒤에\n",
    "        z, idx = _noise_sample(dis_c,con_c,noise,batch_size) # 각각 categorical distribution, uniform distribution, uniform distribution에서 초기화해준다.\n",
    "\n",
    "        fake_x = G(z) # z는 dis_c, con_c, noise가 합쳐진 값이다.\n",
    "        fe_out2 = FE(fake_x.detach()) # real 이미지와 동일한 과정을 거친다.\n",
    "        probs_fake = D(fe_out2)\n",
    "        label.data.fill_(0)\n",
    "        loss_fake = criterionD(probs_fake, label)\n",
    "        loss_fake.backward()\n",
    "\n",
    "        D_loss = loss_real + loss_fake\n",
    "        optimD.step()\n",
    "\n",
    "        # G and Q part\n",
    "        # G 학습은 fake 이미지에 대해서 D와 반대로 학습시켜주면 된다.\n",
    "        optimG.zero_grad()\n",
    "\n",
    "        fe_out = FE(fake_x)\n",
    "        probs_fake = D(fe_out)\n",
    "        label.data.fill_(1.0)\n",
    "        reconstruct_loss = criterionD(probs_fake, label)\n",
    "\n",
    "        # InfoGAN의 Mutual Information Maximization을 optimization 하는 부분\n",
    "        q_logits, q_mu, q_var = Q(fe_out)\n",
    "        class_ = torch.LongTensor(idx).cuda()\n",
    "        target = Variable(class_)\n",
    "        dis_loss = criterionQ_dis(q_logits, target) # discrete code에 대해서는 Cross Entropy를 계산하고\n",
    "        con_loss = criterionQ_con(con_c, q_mu, q_var)*0.1 # continuous code에 대해서는 Log Gaussian을 계산해준다.\n",
    "\n",
    "        G_loss = reconstruct_loss + dis_loss + con_loss\n",
    "        G_loss.backward()\n",
    "        optimG.step()\n",
    "\n",
    "        if num_iters % 50 == 0:\n",
    "            print('Epoch:{}, Iter:{}, Dloss: {:.3f}, Gloss: {:.3f}, Preal: {:.3f}, Pfake: {:.3f}'.format(\n",
    "                epoch, num_iters, D_loss.data[0],\n",
    "                G_loss.data[0], probs_real.data.mean(), probs_fake.data.mean())\n",
    "            )\n",
    "\n",
    "            z = Variable(torch.cat([fix_noise, one_hot, c1], 1).view(-1, 74, 1, 1))\n",
    "            x_save = G(z)\n",
    "            title1 = '(C1)'+str(epoch)+'_'+str(num_iters)\n",
    "            save_image(x_save.data, 'tmp/'+title1+'.png', nrow=10)\n",
    "            vf.imshow_multi(x_save.data.cpu(), nrow=10, title=title1,factor=1)\n",
    "\n",
    "            z = Variable(torch.cat([fix_noise, one_hot, c2], 1).view(-1, 74, 1, 1))\n",
    "            x_save = G(z)\n",
    "            title2 = '(C2)'+str(epoch)+'_'+str(num_iters)\n",
    "            save_image(x_save.data, 'tmp/'+title2+'.png', nrow=10)\n",
    "            vf.imshow_multi(x_save.data.cpu(), nrow=10, title=title2,factor=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
