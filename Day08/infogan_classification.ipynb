{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch, os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "from utils.visdom_utils import VisFunc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architectures\n",
    "![infogan](http://nooverfit.com/wp/wp-content/uploads/2017/10/QQ%E6%88%AA%E5%9B%BE20171009174341.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Front-end Feature Extractor in Discriminator  \n",
    "<br>\n",
    "InfoGAN의 Architecture에서 볼 수 있는 특이한 점은,  \n",
    "Discriminator D와 Encoder Q가 서로 독립적인 네트워크가 아니라,  \n",
    "네트워크의 일정 부분을 서로 공유하고 있다는 점입니다.  \n",
    "<br>\n",
    "D와 Q가 서로 공유하고 있는 부분을 보통 Front-End라고 부릅니다.  \n",
    "<br>\n",
    "true image와 fake image들은 우선 Front-End를 통과하게 되고,  \n",
    "거기서 나온 feature들은 두 갈래로 나뉘어 D와 Q의 output으로 각각 나오게 됩니다. \n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "또한 DCGAN 이후로 다양한 GAN variants들에서 볼 수 있는 일반적인 테크닉들이 InfoGAN에도 적용되어 있습니다.  \n",
    "\n",
    "* Discriminator에는 Leaky ReLU를, Generator에는 ReLU를 사용하는 것이 가장 일반적인 방식이 되었습니다.  \n",
    "* Batch Normalization도 GAN에서 사용이 되고 있습니다. (그러나 최근에는 BN 말고도 더 다양한 Normalization을 사용하기도 합니다.)   \n",
    "* Generator의 마지막 레이어와 Discriminator의 첫 번째 레이어는 BN을 적용하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FrontEnd(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FrontEnd, self).__init__()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(1,64,4,2,1),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(64,128,4,2,1,bias=False),\n",
    "            nn.BatchNorm2d(128), \n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(128, 1024,7,bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        output = self.main(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator D  \n",
    "<br>\n",
    "이미지의 True or False를 판별하기 위한 Feature들은 Front-End에서 충분히 뽑혔다고 판단한 것 같습니다.  \n",
    "D의 네트워크는 매우 단순합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dmodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Dmodel, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(1024,1,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        output=self.main(x).view(-1,1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Q  \n",
    "<br>\n",
    "D와 마찬가지로 Q도 매우 단순한 구조로 되어 있습니다.  \n",
    "Q는 discrete code와 continuous code를 예측해야하는데,  \n",
    "discrete code는 CrossEntropy를 이용해서 reconstruction loss를 계산하기 때문에 logit을,  \n",
    "continuous code는 Log Gaussian을 이용해서 reconstruction loss를 계산하기 때문에 mu와 var를 output하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Qmodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Qmodel,self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(1024,128,1,bias=False) \n",
    "        self.bn = nn.BatchNorm2d(128)\n",
    "        self.lReLU = nn.LeakyReLU(0.1, inplace=True)\n",
    "        self.conv_disc = nn.Conv2d(128,10,1)\n",
    "        self.conv_mu = nn.Conv2d(128,2,1)\n",
    "        self.conv_var = nn.Conv2d(128,2,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        y = self.conv(x)\n",
    "        disc_logits = self.conv_disc(y).squeeze()\n",
    "        mu = self.conv_mu(y).squeeze()\n",
    "        var = self.conv_var(y).squeeze().exp()\n",
    "        return disc_logits, mu, var\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Gmodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Gmodel, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(74, 1024,1,1, bias=False), # noise 62 + discrete code 10 + continuous code 2 = 74\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(1024, 128,7,1,bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128,64,4,2,1,bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64,1,4,2,1,bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.main(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Models and Optimizer and Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    \n",
    "# Models\n",
    "FE=FrontEnd()\n",
    "D=Dmodel()\n",
    "Q=Qmodel()\n",
    "G=Gmodel()\n",
    "\n",
    "for i in [FE, D, Q, G]:\n",
    "    i.cuda()\n",
    "    i.apply(weights_init)\n",
    "\n",
    "# Optimizers\n",
    "optimD = optim.Adam([{'params':FE.parameters()},\n",
    "                     {'params':D.parameters()}],\n",
    "                    lr=0.0001, betas=(0.5, 0.99) )\n",
    "\n",
    "optimG = optim.Adam([{'params':G.parameters()},\n",
    "                     {'params':Q.parameters()}],\n",
    "                    lr=0.0002, betas=(0.5, 0.99) )\n",
    "\n",
    "# Datasets\n",
    "batch_size = 100\n",
    "\n",
    "# Train using 10K Test Images\n",
    "train_data = dset.MNIST('./dataset', train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "\n",
    "# Test using 60K Train Images\n",
    "test_data = dset.MNIST('./dataset', train=False, transform=transforms.ToTensor(), download=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=4, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs(datasets, noises) and Visualization Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fixed random variables for test\n",
    "c0 = torch.linspace(-1,1,10).view(-1,1).repeat(10,0)\n",
    "c1 = torch.stack((c0, torch.zeros(1).expand_as(c0)),1).cuda()\n",
    "c2 = torch.stack((torch.zeros(1).expand_as(c0), c0),1).cuda()\n",
    "one_hot = torch.eye(10).repeat(1,1,10).view(100,10).cuda()\n",
    "fix_noise = torch.Tensor(100, 62).uniform_(-1, 1).cuda()\n",
    "\n",
    "# random noises sampling function\n",
    "def _noise_sample(dis_c, con_c, noise, bs):\n",
    "    idx = np.random.randint(10, size=bs)\n",
    "    c = np.zeros((bs, 10))\n",
    "    c[range(bs),idx] = 1.0\n",
    "    dis_c.data.copy_(torch.Tensor(c))\n",
    "    con_c.data.uniform_(-1.0, 1.0)\n",
    "    noise.data.uniform_(-1.0, 1.0)\n",
    "    z = torch.cat([noise, dis_c, con_c], 1).view(-1, 74, 1, 1)\n",
    "    return z, idx\n",
    "\n",
    "# Visdom\n",
    "env_name = 'infoGAN'\n",
    "vf = VisFunc(enval=env_name)\n",
    "\n",
    "# Generated Image Folder\n",
    "if not os.path.exists('tmp') : os.makedirs('tmp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![log_gaussian](https://user-images.githubusercontent.com/613623/30778123-7328abc0-a0cd-11e7-8998-7e4ef07cc25f.png)\n",
    "https://user-images.githubusercontent.com/613623/30778123-7328abc0-a0cd-11e7-8998-7e4ef07cc25f.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Losses\n",
    "class log_gaussian:\n",
    "    def __call__(self, x, mu, var):\n",
    "        logli = -0.5*(var.mul(2*np.pi)+1e-6).log() - (x-mu).pow(2).div(var.mul(2.0)+1e-6)\n",
    "        return logli.sum(1).mean().mul(-1)\n",
    "\n",
    "criterionD = nn.BCELoss()\n",
    "criterionQ_dis = nn.CrossEntropyLoss()\n",
    "criterionQ_con = log_gaussian()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Classification Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_mode(mode='train'):\n",
    "    if mode == 'train' : \n",
    "        G.train()\n",
    "        FE.train()\n",
    "        D.train()\n",
    "        Q.train()\n",
    "    elif mode == 'eval' :\n",
    "        G.eval()\n",
    "        FE.eval()\n",
    "        D.eval()\n",
    "        Q.eval()\n",
    "    else : raise BaseException('wrong mode') \n",
    "    \n",
    "def test(num_class=10):\n",
    "        '''\n",
    "        1. 클러스터링 결과를 2D 테이블로 만든다\n",
    "        2. 테이블을 이용해서 클러스터가 얼마나 잘 뭉쳤는지 계산한다(hungarian algorithm 이용)\n",
    "        \n",
    "        Table Example : \n",
    "        \n",
    "             true\n",
    "             label 0     1     2     3     4     5     6     7     8     9\n",
    "     cluster  | - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "        0     |    0     1     0     1     0     3     5     1     0   969\n",
    "        1     |    0     3     1     2     7   613     4     0   505     0\n",
    "        2     |    5   547     3     3   461     5     1     0     5     2\n",
    "        3     |    8     5     0   950    17     1     0    25     2     2\n",
    "        4     |    2   184   774     0     0     6     6     0    10     0\n",
    "        5     |    1     1     0     3     3     1     5   872     4     2\n",
    "        6     |    0     4     2     0     0     4   927    13     0     8\n",
    "        7     |  684    13    88     1    11     2     0     2   226     1\n",
    "        8     |    2    13     3     7   592   317     7    15    12     6\n",
    "        9     |  302   158   273    14     5     5     1     7   241     3\n",
    "        \n",
    "        위 테이블을 보면, 클러스터 0은 9라는 숫자가 제일 많이 차지하고 있는 것을 알 수 있다.( 첫 번째 행 )\n",
    "        또한 클러스터 3은 3이라는 숫자가 제일 많이 차지하고 있는 것을 알 수 있다. ( 네 번째 행 )\n",
    "        \n",
    "        클러스터 0, 3, 5, 6 처럼 클러스터링이 잘 된 곳이 있는 반면,\n",
    "        클러스터 1, 2, 4, 7, 8, 9는 여러 종류의 숫자가 섞여 있는 것을 알 수 있다.\n",
    "        \n",
    "        이러한 테이블을 만들어 놓고 hungarian algorithm을 이용하면 clustering performance를 계산할 수 있다.\n",
    "        '''\n",
    "        \n",
    "        import torch.nn.functional as F\n",
    "        from scipy.optimize import linear_sum_assignment\n",
    "        \n",
    "        set_mode('eval')\n",
    "        \n",
    "        LABEL = [] # 각 트레이닝 샘플의 label을 저장\n",
    "        PREDICT = [] # 각 트레이닝 샘플의 encoder prediction 결과를 저장\n",
    "        TABLE = [[0 for i in range(num_class)] for k in range(num_class)] # LABEL과 PREDICT를 종합해서 위에서 처럼 2D 테이블을 만들어야 한다.\n",
    "        total = 0\n",
    "        for batch_idx, (images,labels) in enumerate(test_loader):\n",
    "            real_x = Variable(images.cuda()) # test image 뽑아서\n",
    "            fe_out = FE(real_x) # FE에 넣고\n",
    "            q_logits, _, _ = Q(fe_out) # discrete code의 logit을 뽑은 뒤\n",
    "            q_softmax = F.softmax(q_logits) # softmax를 통과시켜\n",
    "            q_index = q_softmax.max(1)[1] # 가장 likely한 클러스터를 찾는다.\n",
    "            \n",
    "            PREDICT.append(q_index.data) # 각 트레이닝 샘플의 클러스터와 실제 레이블을 차곡차곡 쌓는다.\n",
    "            LABEL.append(labels)\n",
    "            total += labels.size(0)\n",
    "\n",
    "        # 위에서 쌓은 결과를 이용해서 테이블을 채운다.\n",
    "        LABEL = torch.cat(LABEL,0)\n",
    "        PREDICT = torch.cat(PREDICT,0).cpu()\n",
    "        for idx, label in enumerate(LABEL):\n",
    "            TABLE[label][PREDICT[idx]] += 1\n",
    "\n",
    "        # 테이블을 이용해서 hungarian algorithm을 수행한다.\n",
    "        TABLE = torch.FloatTensor(TABLE)\n",
    "        row, col = linear_sum_assignment(-TABLE.numpy())\n",
    "        acc = TABLE.numpy()[row,col].sum()/total\n",
    "\n",
    "        print(TABLE)\n",
    "        print('[TEST RESULT] : ACC : {:.4f}%'.format(acc*100))\n",
    "\n",
    "        set_mode('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    0     0   957     0     0     0    23     0     0     0\n",
      "    0     0  1104     0     0     0    31     0     0     0\n",
      "    0     0   934     0     0     0    98     0     0     0\n",
      "    0     0   888     0     0     0   122     0     0     0\n",
      "    0     0   526     0     0     0   456     0     0     0\n",
      "    0     0   706     0     0     0   186     0     0     0\n",
      "    0     0   730     0     0     0   228     0     0     0\n",
      "    0     0  1008     0     0     0    20     0     0     0\n",
      "    0     0   750     0     0     0   224     0     0     0\n",
      "    0     0   731     0     0     0   278     0     0     0\n",
      "[torch.FloatTensor of size 10x10]\n",
      "\n",
      "[TEST RESULT] : ACC : 15.6000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wonkonge/anaconda3/envs/kongda/lib/python3.6/site-packages/torch/nn/functional.py:767: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Iter:0, Dloss: 1.431, Gloss: 3.204, Preal: 0.549, Pfake: 0.522\n",
      "Epoch:0, Iter:50, Dloss: 1.145, Gloss: 1.060, Preal: 0.571, Pfake: 0.421\n",
      "Epoch:0, Iter:100, Dloss: 1.002, Gloss: 0.982, Preal: 0.612, Pfake: 0.375\n",
      "Epoch:0, Iter:150, Dloss: 0.882, Gloss: 1.062, Preal: 0.638, Pfake: 0.321\n",
      "Epoch:0, Iter:200, Dloss: 0.698, Gloss: 1.296, Preal: 0.712, Pfake: 0.273\n",
      "Epoch:0, Iter:250, Dloss: 0.599, Gloss: 1.491, Preal: 0.743, Pfake: 0.236\n",
      "Epoch:0, Iter:300, Dloss: 0.579, Gloss: 1.431, Preal: 0.738, Pfake: 0.216\n",
      "Epoch:0, Iter:350, Dloss: 0.476, Gloss: 1.591, Preal: 0.787, Pfake: 0.186\n",
      "Epoch:0, Iter:400, Dloss: 0.396, Gloss: 1.677, Preal: 0.833, Pfake: 0.168\n",
      "Epoch:0, Iter:450, Dloss: 0.368, Gloss: 1.829, Preal: 0.831, Pfake: 0.148\n",
      "Epoch:0, Iter:500, Dloss: 0.350, Gloss: 1.909, Preal: 0.845, Pfake: 0.145\n",
      "Epoch:0, Iter:550, Dloss: 0.333, Gloss: 1.986, Preal: 0.851, Pfake: 0.136\n",
      "Epoch:1, Iter:0, Dloss: 0.253, Gloss: 1.976, Preal: 0.911, Pfake: 0.129\n",
      "Epoch:1, Iter:50, Dloss: 0.332, Gloss: 2.028, Preal: 0.860, Pfake: 0.150\n",
      "Epoch:1, Iter:100, Dloss: 0.262, Gloss: 2.115, Preal: 0.898, Pfake: 0.125\n",
      "Epoch:1, Iter:150, Dloss: 0.283, Gloss: 2.269, Preal: 0.872, Pfake: 0.113\n",
      "Epoch:1, Iter:200, Dloss: 0.365, Gloss: 2.189, Preal: 0.852, Pfake: 0.170\n",
      "Epoch:1, Iter:250, Dloss: 0.443, Gloss: 2.165, Preal: 0.798, Pfake: 0.220\n",
      "Epoch:1, Iter:300, Dloss: 0.599, Gloss: 1.708, Preal: 0.774, Pfake: 0.213\n",
      "Epoch:1, Iter:350, Dloss: 0.680, Gloss: 1.443, Preal: 0.716, Pfake: 0.251\n",
      "Epoch:1, Iter:400, Dloss: 0.845, Gloss: 1.320, Preal: 0.685, Pfake: 0.280\n",
      "Epoch:1, Iter:450, Dloss: 0.828, Gloss: 1.370, Preal: 0.660, Pfake: 0.302\n",
      "Epoch:1, Iter:500, Dloss: 0.786, Gloss: 1.350, Preal: 0.702, Pfake: 0.293\n",
      "Epoch:1, Iter:550, Dloss: 0.971, Gloss: 1.307, Preal: 0.622, Pfake: 0.296\n",
      "Epoch:2, Iter:0, Dloss: 0.989, Gloss: 1.169, Preal: 0.650, Pfake: 0.329\n",
      "Epoch:2, Iter:50, Dloss: 0.939, Gloss: 1.216, Preal: 0.661, Pfake: 0.312\n",
      "Epoch:2, Iter:100, Dloss: 1.022, Gloss: 0.986, Preal: 0.633, Pfake: 0.391\n",
      "Epoch:2, Iter:150, Dloss: 0.997, Gloss: 1.148, Preal: 0.614, Pfake: 0.340\n",
      "Epoch:2, Iter:200, Dloss: 0.991, Gloss: 1.133, Preal: 0.608, Pfake: 0.334\n",
      "Epoch:2, Iter:250, Dloss: 1.006, Gloss: 1.091, Preal: 0.623, Pfake: 0.361\n",
      "Epoch:2, Iter:300, Dloss: 0.965, Gloss: 1.087, Preal: 0.632, Pfake: 0.351\n",
      "Epoch:2, Iter:350, Dloss: 0.994, Gloss: 1.021, Preal: 0.628, Pfake: 0.358\n",
      "Epoch:2, Iter:400, Dloss: 1.028, Gloss: 0.982, Preal: 0.604, Pfake: 0.373\n",
      "Epoch:2, Iter:450, Dloss: 1.070, Gloss: 1.009, Preal: 0.580, Pfake: 0.356\n",
      "Epoch:2, Iter:500, Dloss: 1.088, Gloss: 0.995, Preal: 0.608, Pfake: 0.373\n",
      "Epoch:2, Iter:550, Dloss: 1.011, Gloss: 1.062, Preal: 0.605, Pfake: 0.356\n",
      "Epoch:3, Iter:0, Dloss: 1.114, Gloss: 1.022, Preal: 0.607, Pfake: 0.385\n",
      "Epoch:3, Iter:50, Dloss: 1.095, Gloss: 1.196, Preal: 0.569, Pfake: 0.327\n",
      "Epoch:3, Iter:100, Dloss: 1.092, Gloss: 1.007, Preal: 0.612, Pfake: 0.370\n",
      "Epoch:3, Iter:150, Dloss: 1.119, Gloss: 0.987, Preal: 0.604, Pfake: 0.382\n",
      "Epoch:3, Iter:200, Dloss: 1.123, Gloss: 1.039, Preal: 0.583, Pfake: 0.371\n",
      "Epoch:3, Iter:250, Dloss: 1.150, Gloss: 0.967, Preal: 0.555, Pfake: 0.370\n",
      "Epoch:3, Iter:300, Dloss: 1.156, Gloss: 0.955, Preal: 0.585, Pfake: 0.381\n",
      "Epoch:3, Iter:350, Dloss: 1.147, Gloss: 0.974, Preal: 0.606, Pfake: 0.389\n",
      "Epoch:3, Iter:400, Dloss: 1.139, Gloss: 0.996, Preal: 0.583, Pfake: 0.384\n",
      "Epoch:3, Iter:450, Dloss: 1.114, Gloss: 1.043, Preal: 0.603, Pfake: 0.372\n",
      "Epoch:3, Iter:500, Dloss: 1.115, Gloss: 0.917, Preal: 0.596, Pfake: 0.386\n",
      "Epoch:3, Iter:550, Dloss: 1.241, Gloss: 0.946, Preal: 0.548, Pfake: 0.397\n",
      "Epoch:4, Iter:0, Dloss: 1.083, Gloss: 0.914, Preal: 0.610, Pfake: 0.383\n",
      "Epoch:4, Iter:50, Dloss: 1.075, Gloss: 1.029, Preal: 0.599, Pfake: 0.364\n",
      "Epoch:4, Iter:100, Dloss: 1.145, Gloss: 0.922, Preal: 0.602, Pfake: 0.392\n",
      "Epoch:4, Iter:150, Dloss: 1.115, Gloss: 1.006, Preal: 0.583, Pfake: 0.375\n",
      "Epoch:4, Iter:200, Dloss: 1.120, Gloss: 0.976, Preal: 0.580, Pfake: 0.377\n",
      "Epoch:4, Iter:250, Dloss: 1.166, Gloss: 0.952, Preal: 0.591, Pfake: 0.389\n",
      "Epoch:4, Iter:300, Dloss: 1.111, Gloss: 0.997, Preal: 0.591, Pfake: 0.368\n",
      "Epoch:4, Iter:350, Dloss: 1.195, Gloss: 0.952, Preal: 0.561, Pfake: 0.379\n",
      "Epoch:4, Iter:400, Dloss: 1.161, Gloss: 0.966, Preal: 0.563, Pfake: 0.373\n",
      "Epoch:4, Iter:450, Dloss: 1.157, Gloss: 0.944, Preal: 0.585, Pfake: 0.379\n",
      "Epoch:4, Iter:500, Dloss: 1.157, Gloss: 0.964, Preal: 0.592, Pfake: 0.383\n",
      "Epoch:4, Iter:550, Dloss: 1.167, Gloss: 0.941, Preal: 0.597, Pfake: 0.399\n",
      "\n",
      "    0     4     0    39     4     0   922     0    10     1\n",
      "  231     2   890     2     2     5     0     0     3     0\n",
      "   10    15    19     7     9   920    24     5     7    16\n",
      "  167    19     1   129   453    20    25     2   181    13\n",
      "    5    83     2   212     1     6     1   661     2     9\n",
      "   47    14     1    89   575     2    13     7   134    10\n",
      "   91     0     2   207     8     4    32     3   611     0\n",
      "    7     9    35    54     3    23     3    27     2   865\n",
      "  255   542     6    39    30     5    25    11    58     3\n",
      "    8   275     4   324     5     2    14   338     2    37\n",
      "[torch.FloatTensor of size 10x10]\n",
      "\n",
      "[TEST RESULT] : ACC : 64.7700%\n",
      "Epoch:5, Iter:0, Dloss: 1.130, Gloss: 0.954, Preal: 0.608, Pfake: 0.385\n",
      "Epoch:5, Iter:50, Dloss: 1.153, Gloss: 0.907, Preal: 0.621, Pfake: 0.405\n",
      "Epoch:5, Iter:100, Dloss: 1.093, Gloss: 0.964, Preal: 0.601, Pfake: 0.364\n",
      "Epoch:5, Iter:150, Dloss: 1.183, Gloss: 0.900, Preal: 0.602, Pfake: 0.404\n",
      "Epoch:5, Iter:200, Dloss: 1.151, Gloss: 1.010, Preal: 0.569, Pfake: 0.371\n",
      "Epoch:5, Iter:250, Dloss: 1.100, Gloss: 0.921, Preal: 0.629, Pfake: 0.388\n",
      "Epoch:5, Iter:300, Dloss: 1.166, Gloss: 0.934, Preal: 0.583, Pfake: 0.396\n",
      "Epoch:5, Iter:350, Dloss: 1.106, Gloss: 0.956, Preal: 0.608, Pfake: 0.380\n",
      "Epoch:5, Iter:400, Dloss: 1.174, Gloss: 0.957, Preal: 0.573, Pfake: 0.381\n",
      "Epoch:5, Iter:450, Dloss: 1.190, Gloss: 0.972, Preal: 0.555, Pfake: 0.376\n",
      "Epoch:5, Iter:500, Dloss: 1.219, Gloss: 0.929, Preal: 0.559, Pfake: 0.391\n",
      "Epoch:5, Iter:550, Dloss: 1.168, Gloss: 0.983, Preal: 0.571, Pfake: 0.372\n",
      "Epoch:6, Iter:0, Dloss: 1.172, Gloss: 0.950, Preal: 0.560, Pfake: 0.388\n",
      "Epoch:6, Iter:50, Dloss: 1.126, Gloss: 0.935, Preal: 0.613, Pfake: 0.400\n",
      "Epoch:6, Iter:100, Dloss: 1.118, Gloss: 0.916, Preal: 0.609, Pfake: 0.394\n",
      "Epoch:6, Iter:150, Dloss: 1.178, Gloss: 0.896, Preal: 0.591, Pfake: 0.401\n",
      "Epoch:6, Iter:200, Dloss: 1.122, Gloss: 0.977, Preal: 0.609, Pfake: 0.382\n",
      "Epoch:6, Iter:250, Dloss: 1.128, Gloss: 0.941, Preal: 0.583, Pfake: 0.388\n",
      "Epoch:6, Iter:300, Dloss: 1.132, Gloss: 1.017, Preal: 0.592, Pfake: 0.373\n",
      "Epoch:6, Iter:350, Dloss: 1.174, Gloss: 0.957, Preal: 0.555, Pfake: 0.381\n",
      "Epoch:6, Iter:400, Dloss: 1.112, Gloss: 0.974, Preal: 0.581, Pfake: 0.368\n",
      "Epoch:6, Iter:450, Dloss: 1.072, Gloss: 1.026, Preal: 0.602, Pfake: 0.351\n",
      "Epoch:6, Iter:500, Dloss: 1.194, Gloss: 0.993, Preal: 0.569, Pfake: 0.375\n",
      "Epoch:6, Iter:550, Dloss: 1.212, Gloss: 0.915, Preal: 0.593, Pfake: 0.392\n",
      "Epoch:7, Iter:0, Dloss: 1.116, Gloss: 0.990, Preal: 0.603, Pfake: 0.376\n",
      "Epoch:7, Iter:50, Dloss: 1.115, Gloss: 1.016, Preal: 0.564, Pfake: 0.356\n",
      "Epoch:7, Iter:100, Dloss: 1.141, Gloss: 0.927, Preal: 0.590, Pfake: 0.386\n",
      "Epoch:7, Iter:150, Dloss: 1.111, Gloss: 0.961, Preal: 0.601, Pfake: 0.389\n",
      "Epoch:7, Iter:200, Dloss: 1.162, Gloss: 0.988, Preal: 0.590, Pfake: 0.376\n",
      "Epoch:7, Iter:250, Dloss: 1.119, Gloss: 1.068, Preal: 0.583, Pfake: 0.354\n",
      "Epoch:7, Iter:300, Dloss: 1.148, Gloss: 0.998, Preal: 0.573, Pfake: 0.374\n",
      "Epoch:7, Iter:350, Dloss: 1.119, Gloss: 0.935, Preal: 0.586, Pfake: 0.384\n",
      "Epoch:7, Iter:400, Dloss: 1.152, Gloss: 0.941, Preal: 0.575, Pfake: 0.380\n",
      "Epoch:7, Iter:450, Dloss: 1.090, Gloss: 0.976, Preal: 0.584, Pfake: 0.368\n",
      "Epoch:7, Iter:500, Dloss: 1.156, Gloss: 0.975, Preal: 0.564, Pfake: 0.370\n",
      "Epoch:7, Iter:550, Dloss: 1.187, Gloss: 0.961, Preal: 0.578, Pfake: 0.388\n",
      "Epoch:8, Iter:0, Dloss: 1.093, Gloss: 0.976, Preal: 0.636, Pfake: 0.370\n",
      "Epoch:8, Iter:50, Dloss: 1.125, Gloss: 0.946, Preal: 0.565, Pfake: 0.373\n",
      "Epoch:8, Iter:100, Dloss: 1.125, Gloss: 1.079, Preal: 0.562, Pfake: 0.345\n",
      "Epoch:8, Iter:150, Dloss: 1.077, Gloss: 0.965, Preal: 0.600, Pfake: 0.362\n",
      "Epoch:8, Iter:200, Dloss: 1.159, Gloss: 0.994, Preal: 0.542, Pfake: 0.358\n",
      "Epoch:8, Iter:250, Dloss: 1.109, Gloss: 0.938, Preal: 0.594, Pfake: 0.380\n",
      "Epoch:8, Iter:300, Dloss: 1.093, Gloss: 0.951, Preal: 0.593, Pfake: 0.369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:8, Iter:350, Dloss: 1.043, Gloss: 0.967, Preal: 0.623, Pfake: 0.369\n",
      "Epoch:8, Iter:400, Dloss: 1.106, Gloss: 0.971, Preal: 0.595, Pfake: 0.369\n",
      "Epoch:8, Iter:450, Dloss: 1.133, Gloss: 1.021, Preal: 0.606, Pfake: 0.372\n",
      "Epoch:8, Iter:500, Dloss: 1.115, Gloss: 1.014, Preal: 0.574, Pfake: 0.354\n",
      "Epoch:8, Iter:550, Dloss: 1.008, Gloss: 0.962, Preal: 0.657, Pfake: 0.365\n",
      "Epoch:9, Iter:0, Dloss: 1.050, Gloss: 0.991, Preal: 0.625, Pfake: 0.361\n",
      "Epoch:9, Iter:50, Dloss: 1.089, Gloss: 1.001, Preal: 0.595, Pfake: 0.362\n",
      "Epoch:9, Iter:100, Dloss: 1.096, Gloss: 0.986, Preal: 0.584, Pfake: 0.360\n",
      "Epoch:9, Iter:150, Dloss: 0.998, Gloss: 0.991, Preal: 0.628, Pfake: 0.363\n",
      "Epoch:9, Iter:200, Dloss: 1.048, Gloss: 0.944, Preal: 0.646, Pfake: 0.388\n",
      "Epoch:9, Iter:250, Dloss: 1.037, Gloss: 1.124, Preal: 0.619, Pfake: 0.351\n",
      "Epoch:9, Iter:300, Dloss: 1.055, Gloss: 1.113, Preal: 0.597, Pfake: 0.331\n",
      "Epoch:9, Iter:350, Dloss: 1.096, Gloss: 1.015, Preal: 0.594, Pfake: 0.360\n",
      "Epoch:9, Iter:400, Dloss: 1.133, Gloss: 0.990, Preal: 0.549, Pfake: 0.356\n",
      "Epoch:9, Iter:450, Dloss: 1.067, Gloss: 1.022, Preal: 0.630, Pfake: 0.368\n",
      "Epoch:9, Iter:500, Dloss: 1.170, Gloss: 1.007, Preal: 0.563, Pfake: 0.362\n",
      "Epoch:9, Iter:550, Dloss: 1.139, Gloss: 0.966, Preal: 0.567, Pfake: 0.376\n",
      "\n",
      "    1     1     0     2     1     1   966     0     8     0\n",
      "   27     6  1092     0     0     8     0     0     1     1\n",
      "    6    14     1     1     4   965    18     5     6    12\n",
      "  248     7     0    97   411    17     5     1   215     9\n",
      "   15   156     3    23     0     7     1   776     1     0\n",
      "   70     2     0    92   575     1     5     1   145     1\n",
      "  245     1     2     7     4     6     8     3   682     0\n",
      "    1     9     8    15     2    51     5    14     4   919\n",
      "  155   722     0    24    11     8     3     4    39     8\n",
      "    2    44     3   631     4     2    12   285     4    22\n",
      "[torch.FloatTensor of size 10x10]\n",
      "\n",
      "[TEST RESULT] : ACC : 75.7600%\n",
      "Epoch:10, Iter:0, Dloss: 1.159, Gloss: 1.020, Preal: 0.585, Pfake: 0.356\n",
      "Epoch:10, Iter:50, Dloss: 0.961, Gloss: 1.076, Preal: 0.613, Pfake: 0.322\n",
      "Epoch:10, Iter:100, Dloss: 1.067, Gloss: 1.031, Preal: 0.607, Pfake: 0.348\n",
      "Epoch:10, Iter:150, Dloss: 1.075, Gloss: 1.046, Preal: 0.583, Pfake: 0.340\n",
      "Epoch:10, Iter:200, Dloss: 1.120, Gloss: 0.906, Preal: 0.630, Pfake: 0.391\n",
      "Epoch:10, Iter:250, Dloss: 1.133, Gloss: 1.058, Preal: 0.642, Pfake: 0.370\n",
      "Epoch:10, Iter:300, Dloss: 1.089, Gloss: 1.151, Preal: 0.630, Pfake: 0.365\n",
      "Epoch:10, Iter:350, Dloss: 1.050, Gloss: 1.024, Preal: 0.647, Pfake: 0.355\n",
      "Epoch:10, Iter:400, Dloss: 1.119, Gloss: 0.965, Preal: 0.593, Pfake: 0.369\n",
      "Epoch:10, Iter:450, Dloss: 1.045, Gloss: 1.151, Preal: 0.619, Pfake: 0.361\n",
      "Epoch:10, Iter:500, Dloss: 1.168, Gloss: 0.899, Preal: 0.621, Pfake: 0.392\n",
      "Epoch:10, Iter:550, Dloss: 1.050, Gloss: 0.964, Preal: 0.619, Pfake: 0.366\n",
      "Epoch:11, Iter:0, Dloss: 1.020, Gloss: 1.002, Preal: 0.646, Pfake: 0.354\n",
      "Epoch:11, Iter:50, Dloss: 1.126, Gloss: 0.989, Preal: 0.574, Pfake: 0.358\n",
      "Epoch:11, Iter:100, Dloss: 1.086, Gloss: 1.056, Preal: 0.618, Pfake: 0.343\n",
      "Epoch:11, Iter:150, Dloss: 1.166, Gloss: 0.988, Preal: 0.609, Pfake: 0.374\n",
      "Epoch:11, Iter:200, Dloss: 1.029, Gloss: 1.061, Preal: 0.597, Pfake: 0.340\n",
      "Epoch:11, Iter:250, Dloss: 1.048, Gloss: 1.129, Preal: 0.595, Pfake: 0.334\n",
      "Epoch:11, Iter:300, Dloss: 1.017, Gloss: 1.110, Preal: 0.618, Pfake: 0.343\n",
      "Epoch:11, Iter:350, Dloss: 1.038, Gloss: 1.155, Preal: 0.632, Pfake: 0.349\n",
      "Epoch:11, Iter:400, Dloss: 1.116, Gloss: 1.056, Preal: 0.613, Pfake: 0.352\n",
      "Epoch:11, Iter:450, Dloss: 1.160, Gloss: 1.035, Preal: 0.573, Pfake: 0.339\n",
      "Epoch:11, Iter:500, Dloss: 1.193, Gloss: 0.966, Preal: 0.611, Pfake: 0.381\n",
      "Epoch:11, Iter:550, Dloss: 1.114, Gloss: 1.056, Preal: 0.559, Pfake: 0.342\n",
      "Epoch:12, Iter:0, Dloss: 1.013, Gloss: 1.062, Preal: 0.609, Pfake: 0.356\n",
      "Epoch:12, Iter:50, Dloss: 1.087, Gloss: 1.052, Preal: 0.629, Pfake: 0.339\n",
      "Epoch:12, Iter:100, Dloss: 1.002, Gloss: 1.095, Preal: 0.618, Pfake: 0.329\n",
      "Epoch:12, Iter:150, Dloss: 1.099, Gloss: 1.085, Preal: 0.609, Pfake: 0.342\n",
      "Epoch:12, Iter:200, Dloss: 1.121, Gloss: 1.018, Preal: 0.609, Pfake: 0.358\n",
      "Epoch:12, Iter:250, Dloss: 1.045, Gloss: 1.123, Preal: 0.572, Pfake: 0.320\n",
      "Epoch:12, Iter:300, Dloss: 1.023, Gloss: 0.971, Preal: 0.641, Pfake: 0.370\n",
      "Epoch:12, Iter:350, Dloss: 1.029, Gloss: 1.085, Preal: 0.608, Pfake: 0.339\n",
      "Epoch:12, Iter:400, Dloss: 1.117, Gloss: 1.061, Preal: 0.628, Pfake: 0.344\n",
      "Epoch:12, Iter:450, Dloss: 1.089, Gloss: 1.126, Preal: 0.584, Pfake: 0.333\n",
      "Epoch:12, Iter:500, Dloss: 1.012, Gloss: 1.063, Preal: 0.589, Pfake: 0.340\n",
      "Epoch:12, Iter:550, Dloss: 1.161, Gloss: 1.065, Preal: 0.605, Pfake: 0.347\n",
      "Epoch:13, Iter:0, Dloss: 1.067, Gloss: 1.043, Preal: 0.612, Pfake: 0.351\n",
      "Epoch:13, Iter:50, Dloss: 1.149, Gloss: 1.107, Preal: 0.581, Pfake: 0.336\n",
      "Epoch:13, Iter:100, Dloss: 0.977, Gloss: 1.205, Preal: 0.641, Pfake: 0.306\n",
      "Epoch:13, Iter:150, Dloss: 1.058, Gloss: 1.057, Preal: 0.595, Pfake: 0.344\n",
      "Epoch:13, Iter:200, Dloss: 1.064, Gloss: 1.175, Preal: 0.615, Pfake: 0.349\n",
      "Epoch:13, Iter:250, Dloss: 1.085, Gloss: 0.976, Preal: 0.630, Pfake: 0.362\n",
      "Epoch:13, Iter:300, Dloss: 1.056, Gloss: 1.070, Preal: 0.638, Pfake: 0.352\n",
      "Epoch:13, Iter:350, Dloss: 1.013, Gloss: 1.103, Preal: 0.581, Pfake: 0.326\n",
      "Epoch:13, Iter:400, Dloss: 0.961, Gloss: 1.143, Preal: 0.658, Pfake: 0.321\n",
      "Epoch:13, Iter:450, Dloss: 1.064, Gloss: 1.186, Preal: 0.611, Pfake: 0.329\n",
      "Epoch:13, Iter:500, Dloss: 1.022, Gloss: 1.090, Preal: 0.630, Pfake: 0.331\n",
      "Epoch:13, Iter:550, Dloss: 1.121, Gloss: 1.088, Preal: 0.594, Pfake: 0.348\n",
      "Epoch:14, Iter:0, Dloss: 0.949, Gloss: 1.079, Preal: 0.639, Pfake: 0.331\n",
      "Epoch:14, Iter:50, Dloss: 1.053, Gloss: 1.109, Preal: 0.601, Pfake: 0.325\n",
      "Epoch:14, Iter:100, Dloss: 1.035, Gloss: 1.082, Preal: 0.649, Pfake: 0.346\n",
      "Epoch:14, Iter:150, Dloss: 1.010, Gloss: 1.144, Preal: 0.596, Pfake: 0.311\n",
      "Epoch:14, Iter:200, Dloss: 1.008, Gloss: 1.071, Preal: 0.632, Pfake: 0.340\n",
      "Epoch:14, Iter:250, Dloss: 1.029, Gloss: 1.164, Preal: 0.662, Pfake: 0.312\n",
      "Epoch:14, Iter:300, Dloss: 1.080, Gloss: 1.165, Preal: 0.652, Pfake: 0.318\n",
      "Epoch:14, Iter:350, Dloss: 1.026, Gloss: 1.141, Preal: 0.646, Pfake: 0.310\n",
      "Epoch:14, Iter:400, Dloss: 1.139, Gloss: 1.116, Preal: 0.569, Pfake: 0.328\n",
      "Epoch:14, Iter:450, Dloss: 1.071, Gloss: 1.013, Preal: 0.610, Pfake: 0.369\n",
      "Epoch:14, Iter:500, Dloss: 1.037, Gloss: 1.171, Preal: 0.627, Pfake: 0.307\n",
      "Epoch:14, Iter:550, Dloss: 1.073, Gloss: 1.038, Preal: 0.579, Pfake: 0.345\n",
      "\n",
      "    2     1     0     0     1     0   958     0    16     2\n",
      "   12     9  1100     0     0    10     0     2     1     1\n",
      "   10    14     0     4     2   981     7     5     3     6\n",
      "  452     4     0     9   395    17     2     0   123     8\n",
      "   12    13     0    27     0     2     0   915    10     3\n",
      "  154     1     0     4   537     1     1     2   189     3\n",
      "  236     3     2     0     2     5     4     2   704     0\n",
      "    1     7     4    22     2    37     5     7     4   939\n",
      "   40   876     0     3     9     7     1     3    32     3\n",
      "    6    10     3   876     7     4     7    69    16    11\n",
      "[torch.FloatTensor of size 10x10]\n",
      "\n",
      "[TEST RESULT] : ACC : 83.3800%\n",
      "Epoch:15, Iter:0, Dloss: 0.990, Gloss: 1.200, Preal: 0.632, Pfake: 0.311\n",
      "Epoch:15, Iter:50, Dloss: 0.965, Gloss: 1.055, Preal: 0.636, Pfake: 0.340\n",
      "Epoch:15, Iter:100, Dloss: 0.969, Gloss: 1.055, Preal: 0.653, Pfake: 0.351\n",
      "Epoch:15, Iter:150, Dloss: 1.062, Gloss: 1.188, Preal: 0.598, Pfake: 0.325\n",
      "Epoch:15, Iter:200, Dloss: 1.041, Gloss: 1.048, Preal: 0.606, Pfake: 0.360\n",
      "Epoch:15, Iter:250, Dloss: 0.987, Gloss: 1.120, Preal: 0.674, Pfake: 0.330\n",
      "Epoch:15, Iter:300, Dloss: 0.910, Gloss: 1.160, Preal: 0.689, Pfake: 0.310\n",
      "Epoch:15, Iter:350, Dloss: 0.874, Gloss: 1.205, Preal: 0.662, Pfake: 0.303\n",
      "Epoch:15, Iter:400, Dloss: 0.925, Gloss: 1.245, Preal: 0.693, Pfake: 0.297\n",
      "Epoch:15, Iter:450, Dloss: 1.055, Gloss: 1.085, Preal: 0.580, Pfake: 0.336\n",
      "Epoch:15, Iter:500, Dloss: 1.045, Gloss: 1.211, Preal: 0.650, Pfake: 0.297\n",
      "Epoch:15, Iter:550, Dloss: 1.005, Gloss: 1.066, Preal: 0.624, Pfake: 0.349\n",
      "Epoch:16, Iter:0, Dloss: 0.966, Gloss: 1.166, Preal: 0.631, Pfake: 0.320\n",
      "Epoch:16, Iter:50, Dloss: 0.992, Gloss: 1.009, Preal: 0.613, Pfake: 0.362\n",
      "Epoch:16, Iter:100, Dloss: 1.031, Gloss: 1.114, Preal: 0.628, Pfake: 0.350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:16, Iter:150, Dloss: 0.935, Gloss: 1.151, Preal: 0.618, Pfake: 0.319\n",
      "Epoch:16, Iter:200, Dloss: 0.952, Gloss: 1.066, Preal: 0.624, Pfake: 0.337\n",
      "Epoch:16, Iter:250, Dloss: 1.103, Gloss: 1.101, Preal: 0.552, Pfake: 0.335\n",
      "Epoch:16, Iter:300, Dloss: 1.003, Gloss: 1.135, Preal: 0.648, Pfake: 0.325\n",
      "Epoch:16, Iter:350, Dloss: 0.977, Gloss: 1.260, Preal: 0.703, Pfake: 0.301\n",
      "Epoch:16, Iter:400, Dloss: 0.812, Gloss: 1.278, Preal: 0.688, Pfake: 0.277\n",
      "Epoch:16, Iter:450, Dloss: 1.163, Gloss: 1.095, Preal: 0.596, Pfake: 0.326\n",
      "Epoch:16, Iter:500, Dloss: 1.033, Gloss: 1.087, Preal: 0.616, Pfake: 0.330\n",
      "Epoch:16, Iter:550, Dloss: 0.968, Gloss: 1.070, Preal: 0.682, Pfake: 0.348\n",
      "Epoch:17, Iter:0, Dloss: 0.969, Gloss: 1.281, Preal: 0.617, Pfake: 0.293\n",
      "Epoch:17, Iter:50, Dloss: 0.985, Gloss: 1.183, Preal: 0.623, Pfake: 0.313\n",
      "Epoch:17, Iter:100, Dloss: 0.927, Gloss: 1.161, Preal: 0.590, Pfake: 0.313\n",
      "Epoch:17, Iter:150, Dloss: 0.900, Gloss: 1.165, Preal: 0.671, Pfake: 0.313\n",
      "Epoch:17, Iter:200, Dloss: 0.918, Gloss: 1.252, Preal: 0.678, Pfake: 0.306\n",
      "Epoch:17, Iter:250, Dloss: 0.913, Gloss: 1.128, Preal: 0.663, Pfake: 0.326\n",
      "Epoch:17, Iter:300, Dloss: 0.885, Gloss: 1.297, Preal: 0.659, Pfake: 0.291\n",
      "Epoch:17, Iter:350, Dloss: 0.962, Gloss: 1.257, Preal: 0.634, Pfake: 0.307\n",
      "Epoch:17, Iter:400, Dloss: 0.890, Gloss: 1.224, Preal: 0.642, Pfake: 0.295\n",
      "Epoch:17, Iter:450, Dloss: 0.956, Gloss: 1.184, Preal: 0.655, Pfake: 0.298\n",
      "Epoch:17, Iter:500, Dloss: 0.942, Gloss: 1.129, Preal: 0.636, Pfake: 0.324\n",
      "Epoch:17, Iter:550, Dloss: 0.913, Gloss: 1.288, Preal: 0.677, Pfake: 0.289\n",
      "Epoch:18, Iter:0, Dloss: 1.006, Gloss: 1.278, Preal: 0.669, Pfake: 0.288\n",
      "Epoch:18, Iter:50, Dloss: 0.833, Gloss: 1.279, Preal: 0.688, Pfake: 0.287\n",
      "Epoch:18, Iter:100, Dloss: 0.862, Gloss: 1.252, Preal: 0.690, Pfake: 0.301\n",
      "Epoch:18, Iter:150, Dloss: 0.956, Gloss: 1.255, Preal: 0.636, Pfake: 0.287\n",
      "Epoch:18, Iter:200, Dloss: 0.884, Gloss: 1.213, Preal: 0.666, Pfake: 0.296\n",
      "Epoch:18, Iter:250, Dloss: 1.012, Gloss: 1.309, Preal: 0.595, Pfake: 0.298\n",
      "Epoch:18, Iter:300, Dloss: 1.051, Gloss: 1.358, Preal: 0.567, Pfake: 0.267\n",
      "Epoch:18, Iter:350, Dloss: 0.906, Gloss: 1.452, Preal: 0.610, Pfake: 0.267\n",
      "Epoch:18, Iter:400, Dloss: 0.820, Gloss: 1.377, Preal: 0.730, Pfake: 0.263\n",
      "Epoch:18, Iter:450, Dloss: 0.993, Gloss: 1.216, Preal: 0.700, Pfake: 0.308\n",
      "Epoch:18, Iter:500, Dloss: 0.960, Gloss: 1.352, Preal: 0.675, Pfake: 0.299\n",
      "Epoch:18, Iter:550, Dloss: 1.033, Gloss: 1.085, Preal: 0.637, Pfake: 0.351\n",
      "Epoch:19, Iter:0, Dloss: 0.941, Gloss: 1.266, Preal: 0.695, Pfake: 0.289\n",
      "Epoch:19, Iter:50, Dloss: 0.914, Gloss: 1.425, Preal: 0.650, Pfake: 0.256\n",
      "Epoch:19, Iter:100, Dloss: 0.955, Gloss: 1.175, Preal: 0.664, Pfake: 0.312\n",
      "Epoch:19, Iter:150, Dloss: 0.898, Gloss: 1.253, Preal: 0.682, Pfake: 0.304\n",
      "Epoch:19, Iter:200, Dloss: 0.973, Gloss: 1.433, Preal: 0.707, Pfake: 0.269\n",
      "Epoch:19, Iter:250, Dloss: 0.979, Gloss: 1.182, Preal: 0.617, Pfake: 0.305\n",
      "Epoch:19, Iter:300, Dloss: 0.832, Gloss: 1.386, Preal: 0.702, Pfake: 0.259\n",
      "Epoch:19, Iter:350, Dloss: 0.840, Gloss: 1.348, Preal: 0.735, Pfake: 0.266\n",
      "Epoch:19, Iter:400, Dloss: 1.024, Gloss: 1.257, Preal: 0.599, Pfake: 0.321\n",
      "Epoch:19, Iter:450, Dloss: 0.902, Gloss: 1.454, Preal: 0.688, Pfake: 0.257\n",
      "Epoch:19, Iter:500, Dloss: 0.928, Gloss: 1.097, Preal: 0.642, Pfake: 0.333\n",
      "Epoch:19, Iter:550, Dloss: 0.942, Gloss: 1.259, Preal: 0.651, Pfake: 0.296\n",
      "\n",
      "    1     6     0     0     1     2   958     0    10     2\n",
      "   10    28  1082     0     1     6     0     3     0     5\n",
      "    5    23     0     1     2   984     6     3     1     7\n",
      "  434    20     0    10   476    14     0     0    46    10\n",
      "    4     4     0    15     0     3     0   954     1     1\n",
      "  133     9     0     5   560     0     2     2   178     3\n",
      "  243     8     2     0     2     7     6     2   688     0\n",
      "    0     7     3    17     2    35     2     7     4   951\n",
      "   14   923     0     4     4     2     1     3    22     1\n",
      "    3    14     0   917     5     2     5    48     7     8\n",
      "[torch.FloatTensor of size 10x10]\n",
      "\n",
      "[TEST RESULT] : ACC : 84.5100%\n",
      "Epoch:20, Iter:0, Dloss: 0.881, Gloss: 1.373, Preal: 0.715, Pfake: 0.265\n",
      "Epoch:20, Iter:50, Dloss: 0.873, Gloss: 1.290, Preal: 0.672, Pfake: 0.294\n",
      "Epoch:20, Iter:100, Dloss: 0.846, Gloss: 1.304, Preal: 0.664, Pfake: 0.284\n",
      "Epoch:20, Iter:150, Dloss: 0.946, Gloss: 1.216, Preal: 0.594, Pfake: 0.302\n",
      "Epoch:20, Iter:200, Dloss: 0.760, Gloss: 1.290, Preal: 0.692, Pfake: 0.276\n",
      "Epoch:20, Iter:250, Dloss: 0.763, Gloss: 1.517, Preal: 0.677, Pfake: 0.234\n",
      "Epoch:20, Iter:300, Dloss: 0.958, Gloss: 1.422, Preal: 0.683, Pfake: 0.267\n",
      "Epoch:20, Iter:350, Dloss: 0.870, Gloss: 1.246, Preal: 0.621, Pfake: 0.301\n",
      "Epoch:20, Iter:400, Dloss: 0.828, Gloss: 1.305, Preal: 0.681, Pfake: 0.282\n",
      "Epoch:20, Iter:450, Dloss: 0.779, Gloss: 1.312, Preal: 0.738, Pfake: 0.283\n",
      "Epoch:20, Iter:500, Dloss: 0.868, Gloss: 1.160, Preal: 0.670, Pfake: 0.314\n",
      "Epoch:20, Iter:550, Dloss: 0.869, Gloss: 1.560, Preal: 0.663, Pfake: 0.231\n",
      "Epoch:21, Iter:0, Dloss: 0.971, Gloss: 1.385, Preal: 0.608, Pfake: 0.267\n",
      "Epoch:21, Iter:50, Dloss: 0.863, Gloss: 1.418, Preal: 0.639, Pfake: 0.259\n",
      "Epoch:21, Iter:100, Dloss: 0.837, Gloss: 1.313, Preal: 0.646, Pfake: 0.284\n",
      "Epoch:21, Iter:150, Dloss: 0.849, Gloss: 1.373, Preal: 0.723, Pfake: 0.261\n",
      "Epoch:21, Iter:200, Dloss: 0.828, Gloss: 1.284, Preal: 0.715, Pfake: 0.300\n",
      "Epoch:21, Iter:250, Dloss: 0.849, Gloss: 1.430, Preal: 0.661, Pfake: 0.261\n",
      "Epoch:21, Iter:300, Dloss: 0.875, Gloss: 1.395, Preal: 0.681, Pfake: 0.270\n",
      "Epoch:21, Iter:350, Dloss: 0.756, Gloss: 1.376, Preal: 0.701, Pfake: 0.269\n",
      "Epoch:21, Iter:400, Dloss: 0.853, Gloss: 1.349, Preal: 0.668, Pfake: 0.269\n",
      "Epoch:21, Iter:450, Dloss: 0.735, Gloss: 1.515, Preal: 0.685, Pfake: 0.235\n",
      "Epoch:21, Iter:500, Dloss: 0.801, Gloss: 1.319, Preal: 0.697, Pfake: 0.270\n",
      "Epoch:21, Iter:550, Dloss: 0.774, Gloss: 1.414, Preal: 0.736, Pfake: 0.244\n",
      "Epoch:22, Iter:0, Dloss: 0.808, Gloss: 1.665, Preal: 0.677, Pfake: 0.201\n",
      "Epoch:22, Iter:50, Dloss: 0.832, Gloss: 1.398, Preal: 0.708, Pfake: 0.266\n",
      "Epoch:22, Iter:100, Dloss: 0.915, Gloss: 1.500, Preal: 0.708, Pfake: 0.243\n",
      "Epoch:22, Iter:150, Dloss: 0.894, Gloss: 1.366, Preal: 0.654, Pfake: 0.271\n",
      "Epoch:22, Iter:200, Dloss: 0.803, Gloss: 1.600, Preal: 0.700, Pfake: 0.225\n",
      "Epoch:22, Iter:250, Dloss: 0.849, Gloss: 1.195, Preal: 0.651, Pfake: 0.311\n",
      "Epoch:22, Iter:300, Dloss: 0.763, Gloss: 1.570, Preal: 0.724, Pfake: 0.226\n",
      "Epoch:22, Iter:350, Dloss: 0.837, Gloss: 1.295, Preal: 0.689, Pfake: 0.290\n",
      "Epoch:22, Iter:400, Dloss: 0.884, Gloss: 1.565, Preal: 0.639, Pfake: 0.241\n",
      "Epoch:22, Iter:450, Dloss: 0.902, Gloss: 1.580, Preal: 0.727, Pfake: 0.226\n",
      "Epoch:22, Iter:500, Dloss: 0.844, Gloss: 1.551, Preal: 0.697, Pfake: 0.228\n",
      "Epoch:22, Iter:550, Dloss: 1.031, Gloss: 1.455, Preal: 0.707, Pfake: 0.252\n",
      "Epoch:23, Iter:0, Dloss: 0.817, Gloss: 1.208, Preal: 0.719, Pfake: 0.307\n",
      "Epoch:23, Iter:50, Dloss: 0.814, Gloss: 1.728, Preal: 0.752, Pfake: 0.191\n",
      "Epoch:23, Iter:100, Dloss: 0.853, Gloss: 1.419, Preal: 0.727, Pfake: 0.254\n",
      "Epoch:23, Iter:150, Dloss: 0.737, Gloss: 1.713, Preal: 0.745, Pfake: 0.197\n",
      "Epoch:23, Iter:200, Dloss: 0.733, Gloss: 1.588, Preal: 0.699, Pfake: 0.222\n",
      "Epoch:23, Iter:250, Dloss: 0.890, Gloss: 1.427, Preal: 0.694, Pfake: 0.252\n",
      "Epoch:23, Iter:300, Dloss: 0.861, Gloss: 1.723, Preal: 0.785, Pfake: 0.197\n",
      "Epoch:23, Iter:350, Dloss: 0.862, Gloss: 1.450, Preal: 0.699, Pfake: 0.248\n",
      "Epoch:23, Iter:400, Dloss: 0.867, Gloss: 1.409, Preal: 0.642, Pfake: 0.278\n",
      "Epoch:23, Iter:450, Dloss: 0.846, Gloss: 1.707, Preal: 0.755, Pfake: 0.203\n",
      "Epoch:23, Iter:500, Dloss: 0.883, Gloss: 1.635, Preal: 0.698, Pfake: 0.207\n",
      "Epoch:23, Iter:550, Dloss: 0.760, Gloss: 1.316, Preal: 0.743, Pfake: 0.270\n",
      "Epoch:24, Iter:0, Dloss: 0.665, Gloss: 1.516, Preal: 0.745, Pfake: 0.227\n",
      "Epoch:24, Iter:50, Dloss: 0.770, Gloss: 1.843, Preal: 0.659, Pfake: 0.175\n",
      "Epoch:24, Iter:100, Dloss: 0.776, Gloss: 1.687, Preal: 0.779, Pfake: 0.206\n",
      "Epoch:24, Iter:150, Dloss: 0.871, Gloss: 1.353, Preal: 0.742, Pfake: 0.269\n",
      "Epoch:24, Iter:200, Dloss: 0.770, Gloss: 1.706, Preal: 0.797, Pfake: 0.199\n",
      "Epoch:24, Iter:250, Dloss: 0.795, Gloss: 1.236, Preal: 0.660, Pfake: 0.316\n",
      "Epoch:24, Iter:300, Dloss: 0.732, Gloss: 1.659, Preal: 0.708, Pfake: 0.214\n",
      "Epoch:24, Iter:350, Dloss: 0.649, Gloss: 1.532, Preal: 0.756, Pfake: 0.245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:24, Iter:400, Dloss: 0.855, Gloss: 1.604, Preal: 0.701, Pfake: 0.220\n",
      "Epoch:24, Iter:450, Dloss: 0.803, Gloss: 1.470, Preal: 0.639, Pfake: 0.247\n",
      "Epoch:24, Iter:500, Dloss: 0.820, Gloss: 1.532, Preal: 0.751, Pfake: 0.227\n",
      "Epoch:24, Iter:550, Dloss: 0.861, Gloss: 1.501, Preal: 0.660, Pfake: 0.257\n",
      "\n",
      "    1     2     0     0     1     1   956     0    16     3\n",
      "   16     1  1107     0     0     9     0     0     0     2\n",
      "    8    10     0     0     0   998     3     3     4     6\n",
      "  547     6     0     2   404    15     1     0    23    12\n",
      "   10     3     0    18     0     4     0   941     0     6\n",
      "  138     1     0     5   570     1     2     1   170     4\n",
      "  223     2     1     0     2     6     2     1   721     0\n",
      "    2     1     8     8     2    32     1     2     3   969\n",
      "   34   885     0     9     9     5     1     4    25     2\n",
      "    5     3     2   923     9     1    10    28    12    16\n",
      "[torch.FloatTensor of size 10x10]\n",
      "\n",
      "[TEST RESULT] : ACC : 86.1700%\n",
      "Epoch:25, Iter:0, Dloss: 0.772, Gloss: 1.411, Preal: 0.674, Pfake: 0.293\n",
      "Epoch:25, Iter:50, Dloss: 0.708, Gloss: 1.397, Preal: 0.696, Pfake: 0.268\n",
      "Epoch:25, Iter:100, Dloss: 0.833, Gloss: 1.480, Preal: 0.639, Pfake: 0.246\n",
      "Epoch:25, Iter:150, Dloss: 0.793, Gloss: 2.010, Preal: 0.760, Pfake: 0.157\n",
      "Epoch:25, Iter:200, Dloss: 0.754, Gloss: 1.731, Preal: 0.758, Pfake: 0.200\n",
      "Epoch:25, Iter:250, Dloss: 0.801, Gloss: 1.447, Preal: 0.666, Pfake: 0.248\n",
      "Epoch:25, Iter:300, Dloss: 0.696, Gloss: 1.437, Preal: 0.729, Pfake: 0.257\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    if (epoch % 5) == 0 :  test(num_class=10)\n",
    "    for num_iters, batch_data in enumerate(train_loader,0):\n",
    "\n",
    "        # real part\n",
    "        optimD.zero_grad() # D를 학습시켜야 하므로 우선 gradient를 초기화 시켜주고\n",
    "\n",
    "        x, _ = batch_data # 이미지를 불러온 뒤\n",
    "        real_x = Variable(x.cuda()) # Variable로 만들어 준다.\n",
    "        label = Variable(torch.ones(batch_size).float().cuda(), requires_grad=False) # real 이미지에 대해서는 BCE Loss 정답을 1로 만들어 준다.\n",
    "\n",
    "        fe_out1 = FE(real_x) # 이미지를 FE에 넣어서 feature를 뽑고\n",
    "        probs_real = D(fe_out1) # 이 feature를 다시 D에 넣어서 true/false 값을 얻는다.\n",
    "        label.data.fill_(1) # real 이미지에 대해서는 BCE Loss 정답을 1로 만들어 준다.\n",
    "        loss_real = criterionD(probs_real, label) # Loss 계산한 뒤\n",
    "        loss_real.backward() # D를 학습시킨다.\n",
    "\n",
    "        # fake part\n",
    "        # fake 이미지에 대해서도 D를 트레이닝 시켜준다.\n",
    "        dis_c = Variable(torch.FloatTensor(batch_size,10).cuda()) # 랜덤하게 discrete code와\n",
    "        con_c = Variable(torch.FloatTensor(batch_size,2).cuda()) # continuous code,\n",
    "        noise = Variable(torch.FloatTensor(batch_size,62).cuda()) # 그리고 noise vector를 먼저 만든 뒤에\n",
    "        z, idx = _noise_sample(dis_c,con_c,noise,batch_size) # 각각 categorical distribution, uniform distribution, uniform distribution에서 초기화해준다.\n",
    "\n",
    "        fake_x = G(z) # z는 dis_c, con_c, noise가 합쳐진 값이다.\n",
    "        fe_out2 = FE(fake_x.detach()) # real 이미지와 동일한 과정을 거친다.\n",
    "        probs_fake = D(fe_out2)\n",
    "        label.data.fill_(0)\n",
    "        loss_fake = criterionD(probs_fake, label)\n",
    "        loss_fake.backward()\n",
    "\n",
    "        D_loss = loss_real + loss_fake\n",
    "        optimD.step()\n",
    "\n",
    "        # G and Q part\n",
    "        # G 학습은 fake 이미지에 대해서 D와 반대로 학습시켜주면 된다.\n",
    "        optimG.zero_grad()\n",
    "\n",
    "        fe_out = FE(fake_x)\n",
    "        probs_fake = D(fe_out)\n",
    "        label.data.fill_(1.0)\n",
    "        reconstruct_loss = criterionD(probs_fake, label)\n",
    "\n",
    "        # InfoGAN의 Mutual Information Maximization을 optimization 하는 부분\n",
    "        q_logits, q_mu, q_var = Q(fe_out)\n",
    "        class_ = torch.LongTensor(idx).cuda()\n",
    "        target = Variable(class_)\n",
    "        dis_loss = criterionQ_dis(q_logits, target) # discrete code에 대해서는 Cross Entropy를 계산하고\n",
    "        con_loss = criterionQ_con(con_c, q_mu, q_var)*0.1 # continuous code에 대해서는 Log Gaussian을 계산해준다.\n",
    "\n",
    "        G_loss = reconstruct_loss + dis_loss + con_loss\n",
    "        G_loss.backward()\n",
    "        optimG.step()\n",
    "\n",
    "        if num_iters % 50 == 0:\n",
    "            print('Epoch:{}, Iter:{}, Dloss: {:.3f}, Gloss: {:.3f}, Preal: {:.3f}, Pfake: {:.3f}'.format(\n",
    "                epoch, num_iters, D_loss.data[0],\n",
    "                G_loss.data[0], probs_real.data.mean(), probs_fake.data.mean())\n",
    "            )\n",
    "\n",
    "            z = Variable(torch.cat([fix_noise, one_hot, c1], 1).view(-1, 74, 1, 1))\n",
    "            x_save = G(z)\n",
    "            title1 = '(C1)'+str(epoch)+'_'+str(num_iters)\n",
    "            save_image(x_save.data, 'tmp/'+title1+'.png', nrow=10)\n",
    "            vf.imshow_multi(x_save.data.cpu(), nrow=10, title=title1,factor=1)\n",
    "\n",
    "            z = Variable(torch.cat([fix_noise, one_hot, c2], 1).view(-1, 74, 1, 1))\n",
    "            x_save = G(z)\n",
    "            title2 = '(C2)'+str(epoch)+'_'+str(num_iters)\n",
    "            save_image(x_save.data, 'tmp/'+title2+'.png', nrow=10)\n",
    "            vf.imshow_multi(x_save.data.cpu(), nrow=10, title=title2,factor=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
