{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T16:51:05.645121Z",
     "start_time": "2018-01-12T16:51:03.725793Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isjeon/anaconda3/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T16:51:05.650763Z",
     "start_time": "2018-01-12T16:51:05.646964Z"
    }
   },
   "outputs": [],
   "source": [
    "mb_size = 32\n",
    "z_dim = 5\n",
    "h_dim = 128\n",
    "cnt = 0\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T16:51:06.203790Z",
     "start_time": "2018-01-12T16:51:05.652506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('../../MNIST_data', one_hot=True)\n",
    "X_dim = mnist.train.images.shape[1]\n",
    "y_dim = mnist.train.labels.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-12T16:52:16.999Z"
    }
   },
   "outputs": [],
   "source": [
    "# Encoder\n",
    "Q = torch.nn.Sequential(\n",
    "    torch.nn.Linear(X_dim, h_dim),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h_dim, z_dim)\n",
    ").cuda()\n",
    "\n",
    "# Decoder\n",
    "P = torch.nn.Sequential(\n",
    "    torch.nn.Linear(z_dim, h_dim),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h_dim, X_dim),\n",
    "    torch.nn.Sigmoid()\n",
    ").cuda()\n",
    "\n",
    "# Discriminator\n",
    "D = torch.nn.Sequential(\n",
    "    torch.nn.Linear(z_dim, h_dim),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h_dim, 1),\n",
    "    torch.nn.Sigmoid()\n",
    ").cuda()\n",
    "\n",
    "\n",
    "def reset_grad():\n",
    "    Q.zero_grad()\n",
    "    P.zero_grad()\n",
    "    D.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-12T16:52:17.253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-0; D_loss: 1.377; G_loss: 0.6489; recon_loss: 0.6978\n",
      "Iter-1000; D_loss: 1.402; G_loss: 0.6571; recon_loss: 0.2582\n",
      "Iter-2000; D_loss: 1.481; G_loss: 0.6294; recon_loss: 0.274\n",
      "Iter-3000; D_loss: 1.59; G_loss: 0.7361; recon_loss: 0.2617\n",
      "Iter-4000; D_loss: 1.187; G_loss: 0.7721; recon_loss: 0.2523\n",
      "Iter-5000; D_loss: 1.149; G_loss: 0.873; recon_loss: 0.2692\n",
      "Iter-6000; D_loss: 1.214; G_loss: 0.7575; recon_loss: 0.2401\n",
      "Iter-7000; D_loss: 1.439; G_loss: 0.6357; recon_loss: 0.2758\n",
      "Iter-8000; D_loss: 0.9492; G_loss: 1.102; recon_loss: 0.2601\n",
      "Iter-9000; D_loss: 0.7126; G_loss: 1.554; recon_loss: 0.2479\n",
      "Iter-10000; D_loss: 1.044; G_loss: 0.9682; recon_loss: 0.2577\n",
      "Iter-11000; D_loss: 1.516; G_loss: 1.162; recon_loss: 0.2495\n",
      "Iter-12000; D_loss: 0.6631; G_loss: 2.372; recon_loss: 0.2643\n",
      "Iter-13000; D_loss: 0.2539; G_loss: 2.658; recon_loss: 0.2733\n",
      "Iter-14000; D_loss: 0.547; G_loss: 2.133; recon_loss: 0.2539\n",
      "Iter-15000; D_loss: 0.9916; G_loss: 1.482; recon_loss: 0.2661\n",
      "Iter-16000; D_loss: 0.5827; G_loss: 2.308; recon_loss: 0.2613\n",
      "Iter-17000; D_loss: 0.6977; G_loss: 1.235; recon_loss: 0.2589\n",
      "Iter-18000; D_loss: 1.096; G_loss: 1.238; recon_loss: 0.2583\n",
      "Iter-19000; D_loss: 0.6276; G_loss: 1.364; recon_loss: 0.2485\n",
      "Iter-20000; D_loss: 0.09536; G_loss: 3.079; recon_loss: 0.2773\n",
      "Iter-21000; D_loss: 0.7134; G_loss: 1.7; recon_loss: 0.2608\n",
      "Iter-22000; D_loss: 0.4259; G_loss: 2.358; recon_loss: 0.2679\n",
      "Iter-23000; D_loss: 0.0825; G_loss: 3.396; recon_loss: 0.2569\n",
      "Iter-24000; D_loss: nan; G_loss: nan; recon_loss: 0.3266\n",
      "Iter-25000; D_loss: nan; G_loss: nan; recon_loss: 0.2955\n",
      "Iter-26000; D_loss: nan; G_loss: nan; recon_loss: 0.2882\n",
      "Iter-27000; D_loss: nan; G_loss: nan; recon_loss: 0.2594\n",
      "Iter-28000; D_loss: nan; G_loss: nan; recon_loss: 0.2642\n",
      "Iter-29000; D_loss: nan; G_loss: nan; recon_loss: 0.2702\n",
      "Iter-30000; D_loss: nan; G_loss: nan; recon_loss: 0.2634\n",
      "Iter-31000; D_loss: nan; G_loss: nan; recon_loss: 0.2786\n",
      "Iter-32000; D_loss: nan; G_loss: nan; recon_loss: 0.265\n",
      "Iter-33000; D_loss: nan; G_loss: nan; recon_loss: 0.2687\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def sample_X(size, include_y=False):\n",
    "    X, y = mnist.train.next_batch(size)\n",
    "    X = Variable(torch.from_numpy(X).cuda())\n",
    "\n",
    "    if include_y:\n",
    "        y = np.argmax(y, axis=1).astype(np.int)\n",
    "        y = Variable(torch.from_numpy(y).cuda())\n",
    "        return X, y\n",
    "    return X\n",
    "\n",
    "\n",
    "Q_solver = optim.Adam(Q.parameters(), lr=lr)\n",
    "P_solver = optim.Adam(P.parameters(), lr=lr)\n",
    "D_solver = optim.Adam(D.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "for it in range(1000000):\n",
    "    X = sample_X(mb_size)\n",
    "\n",
    "    \"\"\" Reconstruction phase \"\"\"\n",
    "    z_sample = Q(X)\n",
    "    X_sample = P(z_sample)\n",
    "\n",
    "    recon_loss = nn.binary_cross_entropy(X_sample, X)\n",
    "\n",
    "    recon_loss.backward()\n",
    "    P_solver.step()\n",
    "    Q_solver.step()\n",
    "    reset_grad()\n",
    "\n",
    "    \"\"\" Regularization phase \"\"\"\n",
    "    # Discriminator\n",
    "    z_real = Variable(torch.randn(mb_size, z_dim).cuda())\n",
    "    z_fake = Q(X)\n",
    "\n",
    "    D_real = D(z_real)\n",
    "    D_fake = D(z_fake)\n",
    "\n",
    "    D_loss = -torch.mean(torch.log(D_real) + torch.log(1 - D_fake))\n",
    "\n",
    "    D_loss.backward()\n",
    "    D_solver.step()\n",
    "    reset_grad()\n",
    "\n",
    "    # Generator\n",
    "    z_fake = Q(X)\n",
    "    D_fake = D(z_fake)\n",
    "\n",
    "    G_loss = -torch.mean(torch.log(D_fake))\n",
    "\n",
    "    G_loss.backward()\n",
    "    Q_solver.step()\n",
    "    reset_grad()\n",
    "\n",
    "    # Print and plot every now and then\n",
    "    if it % 1000 == 0:\n",
    "        print('Iter-{}; D_loss: {:.4}; G_loss: {:.4}; recon_loss: {:.4}'\n",
    "              .format(it, D_loss.data[0], G_loss.data[0], recon_loss.data[0]))\n",
    "\n",
    "        samples = P(z_real).data.cpu().numpy()[:16]\n",
    "\n",
    "        fig = plt.figure(figsize=(4, 4))\n",
    "        gs = gridspec.GridSpec(4, 4)\n",
    "        gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "        for i, sample in enumerate(samples):\n",
    "            ax = plt.subplot(gs[i])\n",
    "            plt.axis('off')\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_aspect('equal')\n",
    "            plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "        if not os.path.exists('out/'):\n",
    "            os.makedirs('out/')\n",
    "\n",
    "        plt.savefig('out/{}.png'\n",
    "                    .format(str(cnt).zfill(3)), bbox_inches='tight')\n",
    "        cnt += 1\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
