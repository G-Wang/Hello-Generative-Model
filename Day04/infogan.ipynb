{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "from utils.visdom_utils import VisFunc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models & Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFOGAN\n",
    "<br>\n",
    "![image.png](https://pbs.twimg.com/media/ClBVpr0UoAIdJV5.png)\n",
    "https://pbs.twimg.com/media/ClBVpr0UoAIdJV5.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FrontEnd(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FrontEnd, self).__init__()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(1,64,4,2,1),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(64,128,4,2,1,bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(128, 1024,7,bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        output = self.main(x)\n",
    "        return output\n",
    "\n",
    "class Dmodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Dmodel, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(1024,1,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        output=self.main(x).view(-1,1)\n",
    "        return output\n",
    "\n",
    "class Qmodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Qmodel,self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(1024,128,1,bias=False)\n",
    "        self.bn = nn.BatchNorm2d(128)\n",
    "        self.lReLU = nn.LeakyReLU(0.1, inplace=True)\n",
    "        self.conv_disc = nn.Conv2d(128,10,1)\n",
    "        self.conv_mu = nn.Conv2d(128,2,1)\n",
    "        self.conv_var = nn.Conv2d(128,2,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        y = self.conv(x)\n",
    "        disc_logits = self.conv_disc(y).squeeze()\n",
    "        mu = self.conv_mu(y).squeeze()\n",
    "        var = self.conv_var(y).squeeze().exp()\n",
    "        return disc_logits, mu, var\n",
    "\n",
    "class Gmodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Gmodel, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(74, 1024,1,1, bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(1024, 128,7,1,bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128,64,4,2,1,bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64,1,4,2,1,bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.main(x)\n",
    "        return output\n",
    "    \n",
    "def initialize_weights(net):\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            m.weight.data.normal_(0, 0.02)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.ConvTranspose2d):\n",
    "            m.weight.data.normal_(0, 0.02)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            m.weight.data.normal_(0, 0.02)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            m.weight.data.normal_(1.0, 0.02)\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "        \n",
    "FE=FrontEnd()\n",
    "D=Dmodel()\n",
    "Q=Qmodel()\n",
    "G=Gmodel()\n",
    "\n",
    "for i in [FE, D, Q, G]:\n",
    "    i.cuda()\n",
    "    #initialize_weights(i)\n",
    "    i.apply(weights_init)\n",
    "\n",
    "# Optimizers\n",
    "optimD = optim.Adam([{'params':FE.parameters()},\n",
    "                     {'params':D.parameters()}],\n",
    "                    lr=0.0002, betas=(0.5, 0.99) )\n",
    "\n",
    "optimG = optim.Adam([{'params':G.parameters()},\n",
    "                     {'params':Q.parameters()}],\n",
    "                    lr=0.001, betas=(0.5, 0.99) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs(datasets, noises) and Visualization Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "batch_size = 100\n",
    "dataset = dset.MNIST('./dataset', transform=transforms.ToTensor(), download=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "\n",
    "# fixed random variables for test\n",
    "c0 = torch.linspace(-1,1,10).view(-1,1).repeat(10,0)\n",
    "c1 = torch.stack((c0, torch.zeros(1).expand_as(c0)),1).cuda()\n",
    "c2 = torch.stack((torch.zeros(1).expand_as(c0), c0),1).cuda()\n",
    "one_hot = torch.eye(10).repeat(1,1,10).view(100,10).cuda()\n",
    "fix_noise = torch.Tensor(100, 62).uniform_(-1, 1).cuda()\n",
    "\n",
    "\n",
    "# random noises sampling function\n",
    "def _noise_sample(dis_c, con_c, noise, bs):\n",
    "    idx = np.random.randint(10, size=bs)\n",
    "    c = np.zeros((bs, 10))\n",
    "    c[range(bs),idx] = 1.0\n",
    "    dis_c.data.copy_(torch.Tensor(c))\n",
    "    con_c.data.uniform_(-1.0, 1.0)\n",
    "    noise.data.uniform_(-1.0, 1.0)\n",
    "    z = torch.cat([noise, dis_c, con_c], 1).view(-1, 74, 1, 1)\n",
    "    return z, idx\n",
    "\n",
    "# Visdom\n",
    "env_name = 'infoGAN'\n",
    "vf = VisFunc(enval=env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Loss\n",
    "class log_gaussian:\n",
    "    def __call__(self, x, mu, var):\n",
    "        logli = -0.5*(var.mul(2*np.pi)+1e-6).log() - \\\n",
    "                (x-mu).pow(2).div(var.mul(2.0)+1e-6)\n",
    "        return logli.sum(1).mean().mul(-1)\n",
    "\n",
    "criterionD = nn.BCELoss()\n",
    "criterionQ_dis = nn.CrossEntropyLoss()\n",
    "criterionQ_con = log_gaussian()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wonkonge/anaconda3/envs/kongda/lib/python3.6/site-packages/torch/nn/functional.py:767: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Iter:0, Dloss: [ 1.1796782], Gloss: [ 0.92889738], Preal: 0.5960977077484131, Pfake: 0.40685713291168213\n",
      "Epoch:0, Iter:100, Dloss: [ 1.17863703], Gloss: [ 0.87865114], Preal: 0.58326256275177, Pfake: 0.4000353217124939\n",
      "Epoch:0, Iter:200, Dloss: [ 1.19109201], Gloss: [ 0.87808222], Preal: 0.5777857899665833, Pfake: 0.40564242005348206\n",
      "Epoch:0, Iter:300, Dloss: [ 1.26898575], Gloss: [ 0.88158739], Preal: 0.5365154147148132, Pfake: 0.40621185302734375\n",
      "Epoch:0, Iter:400, Dloss: [ 1.20199955], Gloss: [ 0.96253318], Preal: 0.543311357498169, Pfake: 0.37923240661621094\n",
      "Epoch:0, Iter:500, Dloss: [ 1.18906379], Gloss: [ 0.86854374], Preal: 0.5859373807907104, Pfake: 0.4039718508720398\n",
      "Epoch:1, Iter:0, Dloss: [ 1.22675431], Gloss: [ 0.8864426], Preal: 0.5656239986419678, Pfake: 0.4140525162220001\n",
      "Epoch:1, Iter:100, Dloss: [ 1.2708019], Gloss: [ 0.89321136], Preal: 0.5366061925888062, Pfake: 0.40569552779197693\n",
      "Epoch:1, Iter:200, Dloss: [ 1.18451214], Gloss: [ 0.89279097], Preal: 0.58872389793396, Pfake: 0.3974332809448242\n",
      "Epoch:1, Iter:300, Dloss: [ 1.15726364], Gloss: [ 0.89953762], Preal: 0.5798704028129578, Pfake: 0.39172226190567017\n",
      "Epoch:1, Iter:400, Dloss: [ 1.23701668], Gloss: [ 0.88095802], Preal: 0.5638673305511475, Pfake: 0.4070555567741394\n",
      "Epoch:1, Iter:500, Dloss: [ 1.26584756], Gloss: [ 0.92175204], Preal: 0.5510372519493103, Pfake: 0.3947129547595978\n",
      "Epoch:2, Iter:0, Dloss: [ 1.18343663], Gloss: [ 0.95468545], Preal: 0.5796089768409729, Pfake: 0.3781982362270355\n",
      "Epoch:2, Iter:100, Dloss: [ 1.18982649], Gloss: [ 0.98721391], Preal: 0.5587799549102783, Pfake: 0.38730326294898987\n",
      "Epoch:2, Iter:200, Dloss: [ 1.29102361], Gloss: [ 0.99553472], Preal: 0.5224443674087524, Pfake: 0.39672747254371643\n",
      "Epoch:2, Iter:300, Dloss: [ 1.22282743], Gloss: [ 0.91834337], Preal: 0.5444389581680298, Pfake: 0.3967474400997162\n",
      "Epoch:2, Iter:400, Dloss: [ 1.24605179], Gloss: [ 0.87435752], Preal: 0.5666981339454651, Pfake: 0.41220760345458984\n",
      "Epoch:2, Iter:500, Dloss: [ 1.22761595], Gloss: [ 0.9225387], Preal: 0.5659865140914917, Pfake: 0.4027835726737976\n",
      "Epoch:3, Iter:0, Dloss: [ 1.14980698], Gloss: [ 0.92423391], Preal: 0.5896310210227966, Pfake: 0.3813098669052124\n",
      "Epoch:3, Iter:100, Dloss: [ 1.27131009], Gloss: [ 0.86230135], Preal: 0.530885636806488, Pfake: 0.4032871723175049\n",
      "Epoch:3, Iter:200, Dloss: [ 1.18992186], Gloss: [ 0.8659066], Preal: 0.6024062037467957, Pfake: 0.40919724106788635\n",
      "Epoch:3, Iter:300, Dloss: [ 1.10207725], Gloss: [ 1.00287175], Preal: 0.5724953413009644, Pfake: 0.35691556334495544\n",
      "Epoch:3, Iter:400, Dloss: [ 1.16126013], Gloss: [ 0.951419], Preal: 0.5811514854431152, Pfake: 0.3838775157928467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-37:\n",
      "Process Process-40:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wonkonge/anaconda3/envs/kongda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wonkonge/anaconda3/envs/kongda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process Process-38:\n",
      "  File \"/home/wonkonge/anaconda3/envs/kongda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/wonkonge/anaconda3/envs/kongda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/wonkonge/anaconda3/envs/kongda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/wonkonge/anaconda3/envs/kongda/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "Process Process-39:\n",
      "  File \"/home/wonkonge/anaconda3/envs/kongda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/wonkonge/anaconda3/envs/kongda/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/wonkonge/anaconda3/envs/kongda/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/wonkonge/anaconda3/envs/kongda/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wonkonge/anaconda3/envs/kongda/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/wonkonge/anaconda3/envs/kongda/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/wonkonge/anaconda3/envs/kongda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wonkonge/anaconda3/envs/kongda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/wonkonge/anaconda3/envs/kongda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/wonkonge/anaconda3/envs/kongda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/wonkonge/anaconda3/envs/kongda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/wonkonge/anaconda3/envs/kongda/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/wonkonge/anaconda3/envs/kongda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/wonkonge/anaconda3/envs/kongda/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/wonkonge/anaconda3/envs/kongda/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/wonkonge/anaconda3/envs/kongda/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b9f327e2d93e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mreal_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kongda/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    for num_iters, batch_data in enumerate(dataloader,0):\n",
    "\n",
    "        # real part\n",
    "        optimD.zero_grad()\n",
    "\n",
    "        x, _ = batch_data\n",
    "        real_x = Variable(x.cuda())\n",
    "        label = Variable(torch.ones(batch_size).float().cuda(), requires_grad=False)\n",
    "\n",
    "        fe_out1 = FE(real_x)\n",
    "        probs_real = D(fe_out1)\n",
    "        label.data.fill_(1)\n",
    "        loss_real = criterionD(probs_real, label)\n",
    "        loss_real.backward()\n",
    "\n",
    "        # fake part\n",
    "        dis_c = Variable(torch.FloatTensor(batch_size,10).cuda())\n",
    "        con_c = Variable(torch.FloatTensor(batch_size,2).cuda())\n",
    "        noise = Variable(torch.FloatTensor(batch_size,62).cuda())\n",
    "        z, idx = _noise_sample(dis_c,con_c,noise,batch_size)\n",
    "\n",
    "        fake_x = G(z)\n",
    "        fe_out2 = FE(fake_x.detach())\n",
    "        probs_fake = D(fe_out2)\n",
    "        label.data.fill_(0)\n",
    "        loss_fake = criterionD(probs_fake, label)\n",
    "        loss_fake.backward()\n",
    "\n",
    "        D_loss = loss_real + loss_fake\n",
    "        optimD.step()\n",
    "\n",
    "        # G and Q part\n",
    "        optimG.zero_grad()\n",
    "\n",
    "        fe_out = FE(fake_x)\n",
    "        probs_fake = D(fe_out)\n",
    "        label.data.fill_(1.0)\n",
    "        reconstruct_loss = criterionD(probs_fake, label)\n",
    "\n",
    "        q_logits, q_mu, q_var = Q(fe_out)\n",
    "        class_ = torch.LongTensor(idx).cuda()\n",
    "        target = Variable(class_)\n",
    "        dis_loss = criterionQ_dis(q_logits, target)\n",
    "        con_loss = criterionQ_con(con_c, q_mu, q_var)*0.1\n",
    "\n",
    "        G_loss = reconstruct_loss + dis_loss + con_loss\n",
    "        G_loss.backward()\n",
    "        optimG.step()\n",
    "\n",
    "        if num_iters % 100 == 0:\n",
    "            print('Epoch:{0}, Iter:{1}, Dloss: {2}, Gloss: {3}, Preal: {4}, Pfake: {5}'.format(\n",
    "                epoch, num_iters, D_loss.data.cpu().numpy(),\n",
    "                G_loss.data.cpu().numpy(), probs_real.data.mean(), probs_fake.data.mean())\n",
    "            )\n",
    "\n",
    "            z = Variable(torch.cat([fix_noise, one_hot, c1], 1).view(-1, 74, 1, 1))\n",
    "            x_save = G(z)\n",
    "            title1 = '(C1)'+str(epoch)+'_'+str(num_iters)\n",
    "            save_image(x_save.data, 'tmp/'+title1+'.png', nrow=10)\n",
    "#             title1 = '(C1) ' + str(epoch)+' eopch / '+str(num_iters) + ' iters'\n",
    "            vf.imshow_multi(x_save.data.cpu(), nrow=10, title=title1,factor=1)\n",
    "\n",
    "            z = Variable(torch.cat([fix_noise, one_hot, c2], 1).view(-1, 74, 1, 1))\n",
    "            x_save = G(z)\n",
    "            title2 = '(C2)'+str(epoch)+'_'+str(num_iters)\n",
    "            save_image(x_save.data, 'tmp/'+title2+'.png', nrow=10)\n",
    "#             title2 = '(C2) ' + str(epoch)+' eopch / '+str(num_iters) + ' iters'\n",
    "            vf.imshow_multi(x_save.data.cpu(), nrow=10, title=title2,factor=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
