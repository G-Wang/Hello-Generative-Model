{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "from utils.visdom_utils import VisFunc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models & Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "![image.png](https://pbs.twimg.com/media/ClBVpr0UoAIdJV5.png)\n",
    "https://pbs.twimg.com/media/ClBVpr0UoAIdJV5.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FrontEnd(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FrontEnd, self).__init__()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(1,64,4,2,1),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(64,128,4,2,1,bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(128, 1024,7,bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        output = self.main(x)\n",
    "        return output\n",
    "\n",
    "class Dmodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Dmodel, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(1024,1,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        output=self.main(x).view(-1,1)\n",
    "        return output\n",
    "\n",
    "class Qmodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Qmodel,self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(1024,128,1,bias=False)\n",
    "        self.bn = nn.BatchNorm2d(128)\n",
    "        self.lReLU = nn.LeakyReLU(0.1, inplace=True)\n",
    "        self.conv_disc = nn.Conv2d(128,10,1)\n",
    "        self.conv_mu = nn.Conv2d(128,2,1)\n",
    "        self.conv_var = nn.Conv2d(128,2,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        y = self.conv(x)\n",
    "        disc_logits = self.conv_disc(y).squeeze()\n",
    "        mu = self.conv_mu(y).squeeze()\n",
    "        var = self.conv_var(y).squeeze().exp()\n",
    "        return disc_logits, mu, var\n",
    "\n",
    "class Gmodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Gmodel, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(74, 1024,1,1, bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(1024, 128,7,1,bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128,64,4,2,1,bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64,1,4,2,1,bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.main(x)\n",
    "        return output\n",
    "    \n",
    "def initialize_weights(net):\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            m.weight.data.normal_(0, 0.02)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.ConvTranspose2d):\n",
    "            m.weight.data.normal_(0, 0.02)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            m.weight.data.normal_(0, 0.02)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            m.weight.data.normal_(1.0, 0.02)\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "        \n",
    "FE=FrontEnd()\n",
    "D=Dmodel()\n",
    "Q=Qmodel()\n",
    "G=Gmodel()\n",
    "\n",
    "for i in [FE, D, Q, G]:\n",
    "    i.cuda()\n",
    "    #initialize_weights(i)\n",
    "    i.apply(weights_init)\n",
    "\n",
    "# Optimizers\n",
    "optimD = optim.Adam([{'params':FE.parameters()},\n",
    "                     {'params':D.parameters()}],\n",
    "                    lr=0.0002, betas=(0.5, 0.99) )\n",
    "\n",
    "optimG = optim.Adam([{'params':G.parameters()},\n",
    "                     {'params':Q.parameters()}],\n",
    "                    lr=0.001, betas=(0.5, 0.99) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs(datasets, noises) and Visualization Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "batch_size = 100\n",
    "dataset = dset.MNIST('./dataset', transform=transforms.ToTensor(), download=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "\n",
    "# fixed random variables for test\n",
    "c0 = torch.linspace(-1,1,10).view(-1,1).repeat(10,0)\n",
    "c1 = torch.stack((c0, torch.zeros(1).expand_as(c0)),1).cuda()\n",
    "c2 = torch.stack((torch.zeros(1).expand_as(c0), c0),1).cuda()\n",
    "one_hot = torch.eye(10).repeat(1,1,10).view(100,10).cuda()\n",
    "fix_noise = torch.Tensor(100, 62).uniform_(-1, 1).cuda()\n",
    "\n",
    "\n",
    "# random noises sampling function\n",
    "def _noise_sample(dis_c, con_c, noise, bs):\n",
    "    idx = np.random.randint(10, size=bs)\n",
    "    c = np.zeros((bs, 10))\n",
    "    c[range(bs),idx] = 1.0\n",
    "    dis_c.data.copy_(torch.Tensor(c))\n",
    "    con_c.data.uniform_(-1.0, 1.0)\n",
    "    noise.data.uniform_(-1.0, 1.0)\n",
    "    z = torch.cat([noise, dis_c, con_c], 1).view(-1, 74, 1, 1)\n",
    "    return z, idx\n",
    "\n",
    "# Visdom\n",
    "env_name = 'infoGAN'\n",
    "vf = VisFunc(enval=env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![log_gaussian](https://user-images.githubusercontent.com/613623/30778123-7328abc0-a0cd-11e7-8998-7e4ef07cc25f.png)\n",
    "https://user-images.githubusercontent.com/613623/30778123-7328abc0-a0cd-11e7-8998-7e4ef07cc25f.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Losses\n",
    "class log_gaussian:\n",
    "    def __call__(self, x, mu, var):\n",
    "        logli = -0.5*(var.mul(2*np.pi)+1e-6).log() - (x-mu).pow(2).div(var.mul(2.0)+1e-6)\n",
    "        return logli.sum(1).mean().mul(-1)\n",
    "\n",
    "criterionD = nn.BCELoss()\n",
    "criterionQ_dis = nn.CrossEntropyLoss()\n",
    "criterionQ_con = log_gaussian()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    for num_iters, batch_data in enumerate(dataloader,0):\n",
    "\n",
    "        # real part\n",
    "        optimD.zero_grad()\n",
    "\n",
    "        x, _ = batch_data\n",
    "        real_x = Variable(x.cuda())\n",
    "        label = Variable(torch.ones(batch_size).float().cuda(), requires_grad=False)\n",
    "\n",
    "        fe_out1 = FE(real_x)\n",
    "        probs_real = D(fe_out1)\n",
    "        label.data.fill_(1)\n",
    "        loss_real = criterionD(probs_real, label)\n",
    "        loss_real.backward()\n",
    "\n",
    "        # fake part\n",
    "        dis_c = Variable(torch.FloatTensor(batch_size,10).cuda())\n",
    "        con_c = Variable(torch.FloatTensor(batch_size,2).cuda())\n",
    "        noise = Variable(torch.FloatTensor(batch_size,62).cuda())\n",
    "        z, idx = _noise_sample(dis_c,con_c,noise,batch_size)\n",
    "\n",
    "        fake_x = G(z)\n",
    "        fe_out2 = FE(fake_x.detach())\n",
    "        probs_fake = D(fe_out2)\n",
    "        label.data.fill_(0)\n",
    "        loss_fake = criterionD(probs_fake, label)\n",
    "        loss_fake.backward()\n",
    "\n",
    "        D_loss = loss_real + loss_fake\n",
    "        optimD.step()\n",
    "\n",
    "        # G and Q part\n",
    "        optimG.zero_grad()\n",
    "\n",
    "        fe_out = FE(fake_x)\n",
    "        probs_fake = D(fe_out)\n",
    "        label.data.fill_(1.0)\n",
    "        reconstruct_loss = criterionD(probs_fake, label)\n",
    "\n",
    "        q_logits, q_mu, q_var = Q(fe_out)\n",
    "        class_ = torch.LongTensor(idx).cuda()\n",
    "        target = Variable(class_)\n",
    "        dis_loss = criterionQ_dis(q_logits, target)\n",
    "        con_loss = criterionQ_con(con_c, q_mu, q_var)*0.1\n",
    "\n",
    "        G_loss = reconstruct_loss + dis_loss + con_loss\n",
    "        G_loss.backward()\n",
    "        optimG.step()\n",
    "\n",
    "        if num_iters % 100 == 0:\n",
    "            print('Epoch:{0}, Iter:{1}, Dloss: {2}, Gloss: {3}, Preal: {4}, Pfake: {5}'.format(\n",
    "                epoch, num_iters, D_loss.data.cpu().numpy(),\n",
    "                G_loss.data.cpu().numpy(), probs_real.data.mean(), probs_fake.data.mean())\n",
    "            )\n",
    "\n",
    "            z = Variable(torch.cat([fix_noise, one_hot, c1], 1).view(-1, 74, 1, 1))\n",
    "            x_save = G(z)\n",
    "            title1 = '(C1)'+str(epoch)+'_'+str(num_iters)\n",
    "            save_image(x_save.data, 'tmp/'+title1+'.png', nrow=10)\n",
    "#             title1 = '(C1) ' + str(epoch)+' eopch / '+str(num_iters) + ' iters'\n",
    "            vf.imshow_multi(x_save.data.cpu(), nrow=10, title=title1,factor=1)\n",
    "\n",
    "            z = Variable(torch.cat([fix_noise, one_hot, c2], 1).view(-1, 74, 1, 1))\n",
    "            x_save = G(z)\n",
    "            title2 = '(C2)'+str(epoch)+'_'+str(num_iters)\n",
    "            save_image(x_save.data, 'tmp/'+title2+'.png', nrow=10)\n",
    "#             title2 = '(C2) ' + str(epoch)+' eopch / '+str(num_iters) + ' iters'\n",
    "            vf.imshow_multi(x_save.data.cpu(), nrow=10, title=title2,factor=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
